{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f59bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "import pdal\n",
    "import json\n",
    "import scipy\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import itertools\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e89c9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4063806\n",
      "4063806\n"
     ]
    }
   ],
   "source": [
    "las = laspy.read(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\functional\\Barmasse_2025_AllScans_Raw_Sub2cm_flatten_stripes_10_m\\Barmasse_2025_AllScans_Raw_Sub2cm_flatten_stripe_45.laz\")\n",
    "print(len(las))\n",
    "print(len(set(las.id_point)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d378e2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-79501.86\n"
     ]
    }
   ],
   "source": [
    "las = laspy.read(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test2\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tiles_overlap\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tile_0_0.laz\")\n",
    "print(las.x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925a9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "las = laspy.read(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test2\\Barmasse_2025_AllScans_Raw_Sub2cm_aligned.laz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92bc99b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "No attribute assign_crs in LasHeader",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# las.header.add_crs(CRS.from_epsg(2056))\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mlas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_crs\u001b[49m(CRS\u001b[38;5;241m.\u001b[39mfrom_epsg(\u001b[38;5;241m2056\u001b[39m))\n\u001b[0;32m      4\u001b[0m las\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHubProjects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTerranum_repo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTreeSegmentation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBarmasse\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBarmasse_2023\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBarmasse_2025_AllScans_Raw_Sub2cm_aligned_with_crs.laz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\PDM\\lib\\site-packages\\laspy\\header.py:967\u001b[0m, in \u001b[0;36mLasHeader.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_OLD_LASPY_NAMES[item])\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in LasHeader\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: No attribute assign_crs in LasHeader"
     ]
    }
   ],
   "source": [
    "# header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "las.header.add_crs(CRS.from_epsg(2056))\n",
    "# las.header.assign_crs(CRS.from_epsg(2056))\n",
    "las.write(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test2\\Barmasse_2025_AllScans_Raw_Sub2cm_aligned_with_crs.laz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b681dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "epsg = las.header.parse_crs()\n",
    "print(epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87864ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_original = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test\\Barmasse_2023_AllScans_Raw_Georef.laz\"\n",
    "tile_size = 500\n",
    "overlap = 50\n",
    "grid_size = 10\n",
    "stripe_width = 10\n",
    "ext = 'laz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb1c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_with_id = src_original.split('.laz')[0] + \"_with_id.laz\"\n",
    "src_folder_tiles_wo_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_tiles_no_overlap\")\n",
    "src_folder_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_tiles_overlap\")\n",
    "src_folder_flatten_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_flatten\")\n",
    "src_flatten_file = os.path.join(src_original.split('.laz')[0] + \"_flatten.laz\")\n",
    "src_folder_stripes = os.path.join(os.path.dirname(src_flatten_file), os.path.basename(src_flatten_file).split('.laz')[0] + \"_stripes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d709432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113730157\n"
     ]
    }
   ],
   "source": [
    "# loading\n",
    "start_preprocess_time = time()\n",
    "laz_original = laspy.read(src_original)\n",
    "print(len(laz_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a390afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def tilling(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "    \"\"\"\n",
    "    Tile the input LiDAR file into square tiles using PDAL and store them in the destination folder.\n",
    "\n",
    "    Args:\n",
    "        - verbose (bool): Whether to print verbose status updates.\n",
    "\n",
    "    Returns:\n",
    "        - None: Splits the input file into tiles and saves them in the destination folder.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Start tilling (with overlap = {overlap}m)...\")\n",
    "    os.makedirs(src_target, exist_ok=True)\n",
    "    \n",
    "    # compute the estimate number of tiles\n",
    "    if verbose:\n",
    "        print(\"Computing the estimated number of tiles...\")\n",
    "    original_file = laspy.read(src_input)\n",
    "    x_min = original_file.x.min()\n",
    "    x_max = original_file.x.max()\n",
    "    y_min = original_file.y.min()\n",
    "    y_max = original_file.y.max() \n",
    "    if verbose:\n",
    "        print('Done!')\n",
    "\n",
    "\n",
    "    \n",
    "    # output_pattern = \"tile_{i}_{j}.laz\"\n",
    "    output_pattern = os.path.join(\n",
    "        src_target, \n",
    "        os.path.basename(src_input).split('.')[0] + \"_tile_{i}_{j}.laz\",\n",
    "        )\n",
    "\n",
    "    x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "    y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "    combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "    for _, (i,j) in tqdm(enumerate(combinations), total=len(combinations)):\n",
    "        x0 = x_min + i * tile_size - overlap\n",
    "        x1 = x_min + (i + 1) * tile_size + overlap\n",
    "        y0 = y_min + j * tile_size - overlap\n",
    "        y1 = y_min + (j + 1) * tile_size + overlap\n",
    "\n",
    "        bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "        pipeline_json = {\n",
    "            \"pipeline\": [\n",
    "                # src_input,\n",
    "                {\n",
    "                    \"type\": \"readers.las\",\n",
    "                    \"filename\": src_input,\n",
    "                    \"extra_dims\": \"id_point=uint32\"\n",
    "                },\n",
    "                {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "                {\n",
    "                    \"type\": \"writers.las\",\n",
    "                    \"filename\": output_pattern.format(i=i, j=j),\n",
    "                    \"extra_dims\": \"id_point=uint32\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "        pipeline.execute()\n",
    "\n",
    "\n",
    "def stripes_file(src_input_file, src_output, dims, do_keep_existing=False):\n",
    "    [tile_size_x, tile_size_y] = dims\n",
    "    laz = laspy.read(src_input_file)\n",
    "    # output_folder = os.path.join(src_output, f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripes_{tile_size_x}_{tile_size_y}\")\n",
    "    os.makedirs(src_output, exist_ok=True)\n",
    "\n",
    "    xmin, xmax, ymin, ymax = np.min(laz.x), np.max(laz.x), np.min(laz.y), np.max(laz.y)\n",
    "\n",
    "    x_edges = np.arange(xmin, xmax, tile_size_x)\n",
    "    y_edges = np.arange(ymin, ymax, tile_size_y)\n",
    "\n",
    "\n",
    "    for i, x0 in enumerate(x_edges):\n",
    "        print(f\"Column {i+1} / {len(x_edges)}:\")\n",
    "        for j, y0 in tqdm(enumerate(y_edges), total=len(y_edges)):\n",
    "            output = os.path.join(src_output,f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripe_{i}_{j}.laz\")\n",
    "            if do_keep_existing and os.path.exists(output):\n",
    "                continue\n",
    "            x1 = x0 + tile_size_x\n",
    "            y1 = y0 + tile_size_y\n",
    "            bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "            pipeline_json = {\n",
    "                \"pipeline\": [\n",
    "                    src_input_file,\n",
    "                    {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "                    {\"type\": \"writers.las\", \"filename\": output, 'extra_dims': 'all'}\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "            pipeline.execute()\n",
    "\n",
    "\n",
    "def stripes_file_fast(src_input_file, src_output, dims, do_keep_existing=False):\n",
    "    [tile_size_x, tile_size_y] = dims\n",
    "    laz = laspy.read(src_input_file)\n",
    "    # output_folder = os.path.join(src_output, f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripes_{tile_size_x}_{tile_size_y}\")\n",
    "    os.makedirs(src_output, exist_ok=True)\n",
    "\n",
    "    xmin, xmax, ymin, ymax = np.min(laz.x), np.max(laz.x), np.min(laz.y), np.max(laz.y)\n",
    "\n",
    "    x_edges = np.arange(xmin, xmax, tile_size_x)\n",
    "    y_edges = np.arange(ymin, ymax, tile_size_y)\n",
    "\n",
    "    list_bounds = []\n",
    "    for i, x0 in enumerate(x_edges):\n",
    "        print(f\"Column {i+1} / {len(x_edges)}:\")\n",
    "        for j, y0 in tqdm(enumerate(y_edges), total=len(y_edges)):\n",
    "            output = os.path.join(src_output,f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripe_{i}_{j}.laz\")\n",
    "            if do_keep_existing and os.path.exists(output):\n",
    "                continue\n",
    "            x1 = x0 + tile_size_x\n",
    "            y1 = y0 + tile_size_y\n",
    "            bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "            # str_list_bounds += f\"[{bound[0]},{bound[1]}],\"\n",
    "            \n",
    "            list_bounds.append(bounds)\n",
    "\n",
    "    # str_list_bounds = \"(\"\n",
    "    # for bound in list_bounds:\n",
    "    # str_list_bounds = str_list_bounds[:-1] + \")\"\n",
    "    # print(str_list_bounds)\n",
    "    print(len(list_bounds))\n",
    "    output_pattern = os.path.join(\n",
    "            src_output, \n",
    "            os.path.basename(src_input_file).split('.')[0] + \"_stripe_#.laz\",\n",
    "            )\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": src_input_file,\n",
    "                \"extra_dims\": \"id_point=uint32\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.crop\", \n",
    "                \"bounds\": list_bounds\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.las\", \n",
    "                \"filename\": output_pattern, \n",
    "                'extra_dims': \"id_point=uint32\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "\n",
    "\n",
    "def remove_duplicates(laz_file, decimals=2):\n",
    "    \"\"\"\n",
    "    Removes duplicate points from a LAS/LAZ file based on rounded 3D coordinates.\n",
    "\n",
    "    Args:\n",
    "        - laz_file (laspy.LasData): Input LAS/LAZ file as a laspy object.\n",
    "        - decimals (int, optional): Number of decimals to round the coordinates for duplicate detection. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        - laspy.LasData: A new laspy object with duplicate points removed.\n",
    "    \"\"\"\n",
    "        \n",
    "    coords = np.round(np.vstack((laz_file.x, laz_file.y, laz_file.z)).T, decimals)\n",
    "    _, unique_indices = np.unique(coords, axis=0, return_index=True)\n",
    "    mask = np.zeros(len(coords), dtype=bool)\n",
    "    mask[unique_indices] = True\n",
    "\n",
    "    # Create new LAS object\n",
    "    header = laspy.LasHeader(point_format=laz_file.header.point_format, version=laz_file.header.version)\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "    for dim in laz_file.point_format.dimension_names:\n",
    "        setattr(new_las, dim, getattr(laz_file, dim)[mask])\n",
    "\n",
    "    return new_las\n",
    "\n",
    "def match_pointclouds(laz1, laz2):\n",
    "    \"\"\"Sort laz2 to match the order of laz1 without changing laz1's order.\n",
    "\n",
    "    Args:\n",
    "        laz1: laspy.LasData object (reference order)\n",
    "        laz2: laspy.LasData object (to be sorted)\n",
    "    \n",
    "    Returns:\n",
    "        laz2 sorted to match laz1\n",
    "    \"\"\"\n",
    "    # Retrieve and round coordinates for robust matching\n",
    "    coords_1 = np.round(np.vstack((laz1.x, laz1.y, laz1.z)), 2).T\n",
    "    coords_2 = np.round(np.vstack((laz2.x, laz2.y, laz2.z)), 2).T\n",
    "\n",
    "    # Verify laz2 is of the same size as laz1\n",
    "    assert len(coords_2) == len(coords_1), \"laz2 should be a subset of laz1\"\n",
    "\n",
    "    # Create a dictionary mapping from coordinates to indices\n",
    "    coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(coords_1)}\n",
    "\n",
    "    # Find indices in laz1 that correspond to laz2\n",
    "    matching_indices = []\n",
    "    failed = 0\n",
    "    for coord in coords_2:\n",
    "        try:\n",
    "            matching_indices.append(coord_to_idx[tuple(coord)])\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "\n",
    "    matching_indices = np.array([coord_to_idx[tuple(coord)] for coord in coords_2])\n",
    "\n",
    "    # Sort laz2 to match laz1\n",
    "    sorted_indices = np.argsort(matching_indices)\n",
    "\n",
    "    # Apply sorting to all attributes of laz2\n",
    "    laz2.points = laz2.points[sorted_indices]\n",
    "\n",
    "    return laz2  # Now sorted to match laz1\n",
    "\n",
    "\n",
    "def flattening_tile(tile_src, tile_new_original_src, grid_size=10, do_save_floor=False, do_keep_existing=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Flattens a tile by interpolating the ground surface and subtracting it from the original elevation.\n",
    "\n",
    "    Args:\n",
    "        - tile_src (str): Path to the input tile in LAS/LAZ format.\n",
    "        - tile_new_original_src (str): Path to save the resized original tile after filtering.\n",
    "        - grid_size (int, optional): Size of the grid in meters for local interpolation. Defaults to 10.\n",
    "        - verbose (bool, optional): Whether to display progress and debug information. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        - None: Saves the floor and flattened versions of the tile and updates the original file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(tile_new_original_src) and do_keep_existing:\n",
    "        if verbose:\n",
    "            print(f\"Skipping. {tile_new_original_src} exists already\")\n",
    "        return\n",
    "    # Load file\n",
    "    laz = laspy.read(tile_src)\n",
    "    init_len = len(laz)\n",
    "    if init_len == 0:\n",
    "        return\n",
    "    # laz = remove_duplicates(laz)\n",
    "    if verbose:\n",
    "        print(f\"Removing duplicates: From {init_len} to {len(laz)}\")\n",
    "    # laz.write(tile_new_original_src)\n",
    "    \n",
    "    points = np.vstack((laz.x, laz.y, laz.z)).T\n",
    "    points_flatten = copy.deepcopy(points)\n",
    "    points_interpolated = copy.deepcopy(points)\n",
    "\n",
    "    # Divide into tiles and find local minimums\n",
    "    x_min, y_min = np.min(points[:, :2], axis=0)\n",
    "    x_max, y_max = np.max(points[:, :2], axis=0)\n",
    "\n",
    "    x_bins = np.append(np.arange(x_min, x_max, grid_size), x_max)\n",
    "    y_bins = np.append(np.arange(y_min, y_max, grid_size), y_max)\n",
    "\n",
    "    grid = {i:{j:[] for j in range(y_bins.size - 1)} for i in range(x_bins.size -1)}\n",
    "    for _, (px, py, pz) in tqdm(enumerate(points), total=len(points), desc=\"Creating grid\", disable=verbose==False):\n",
    "        xbin = np.clip(0, (px - x_min) // grid_size, x_bins.size - 2)\n",
    "        ybin = np.clip(0, (py - y_min) // grid_size, y_bins.size - 2)\n",
    "        try:\n",
    "            grid[xbin][ybin].append((px, py, pz))\n",
    "        except Exception as e:\n",
    "            print(xbin)\n",
    "            print(ybin)\n",
    "            print(x_bins)\n",
    "            print(y_bins)\n",
    "            print(grid.keys())\n",
    "            print(grid[0].keys())\n",
    "            raise e\n",
    "\n",
    "\n",
    "    # Create grid_min\n",
    "    grid_used = np.zeros((x_bins.size - 1, y_bins.size - 1))\n",
    "    lst_grid_min = []\n",
    "    lst_grid_min_pos = []\n",
    "    for x in grid.keys():\n",
    "        for y in grid[x].keys():\n",
    "            if np.array(grid[x][y]).shape[0] > 0:\n",
    "                grid_used[x, y] = 1\n",
    "                lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                arg_min = np.argmin(np.array(grid[x][y])[:,2])\n",
    "                lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2])\n",
    "\n",
    "                # test if border\n",
    "                if x == list(grid.keys())[0]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] - [5, 0])\n",
    "                if x == list(grid.keys())[-1]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] + [5, 0])\n",
    "                if y == list(grid[x].keys())[0]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] - [0, 5])\n",
    "                if y == list(grid[x].keys())[-1]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] + [0, 5])\n",
    "            else:\n",
    "                grid_used[x, y] = 0\n",
    "    arr_grid_min_pos = np.vstack(lst_grid_min_pos)\n",
    "    if verbose:\n",
    "        print(\"Resulting grid:\")\n",
    "        print(arr_grid_min_pos.shape)\n",
    "        print(grid_used)\n",
    "\n",
    "    # Interpolate\n",
    "    points_xy = np.array(points)[:,0:2]\n",
    "    interpolated_min_z = scipy.interpolate.griddata(arr_grid_min_pos, np.array(lst_grid_min), points_xy, method=\"cubic\", fill_value=-1)\n",
    "\n",
    "    # # Fill NaNs with nearest neighbor interpolation\n",
    "    # nan_mask = np.isnan(interpolated_min_z)\n",
    "    # x = np.array(points)[:,0]\n",
    "    # y = np.array(points)[:,1]\n",
    "    # z = np.array(points)[:,2]\n",
    "\n",
    "    # if np.any(nan_mask):\n",
    "    #     interpolated_min_z[nan_mask] = scipy.interpolate.griddata(arr_grid_min_pos, np.array(lst_grid_min), (x[nan_mask], y[nan_mask]), method='nearest')\n",
    "\n",
    "    mask_valid = np.array([x != -1 for x in list(interpolated_min_z)])\n",
    "    points_interpolated = points_interpolated[mask_valid]\n",
    "    points_interpolated[:, 2] = interpolated_min_z[mask_valid]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Interpolation:\")\n",
    "        print(f\"Original number of points: {points.shape[0]}\")\n",
    "        print(f\"Interpollated number of points: {points_interpolated.shape[0]} ({int(points_interpolated.shape[0] / points.shape[0]*100)}%)\")\n",
    "\n",
    "    # save floor\n",
    "    filtered_points = {dim: getattr(laz, dim)[mask_valid] for dim in laz.point_format.dimension_names}\n",
    "    header = laspy.LasHeader(point_format=laz.header.point_format, version=laz.header.version)\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "    #   _Assign filtered and modified data\n",
    "    for dim, values in filtered_points.items():\n",
    "        setattr(new_las, dim, values)\n",
    "    setattr(new_las, 'x', points_interpolated[:,0])\n",
    "    setattr(new_las, 'y', points_interpolated[:,1])\n",
    "    setattr(new_las, 'z', points_interpolated[:,2])\n",
    "\n",
    "    #   _Save new file\n",
    "    # floor_dir = os.path.join(os.path.dirname(tile_src), 'floor')\n",
    "    # os.makedirs(floor_dir, exist_ok=True)\n",
    "    # new_las.write(os.path.join(floor_dir, os.path.basename(tile_src).split('.laz')[0] + f\"_floor_{grid_size}m.laz\"))\n",
    "    if do_save_floor:\n",
    "        new_las.write(tile_new_original_src.split('.laz')[0] + \"_floor.laz\")\n",
    "        if verbose:\n",
    "            print(\"Saved file: \", tile_new_original_src.split('.laz')[0] + \"_floor.laz\")\n",
    "            # print(\"Saved file: \", os.path.join(floor_dir, os.path.basename(tile_src).split('.laz')[0] + f\"_floor_{grid_size}m.laz\"))\n",
    "\n",
    "    # Flatten\n",
    "    points_flatten = points_flatten[mask_valid]\n",
    "    points_flatten[:,2] = points_flatten[:,2] - points_interpolated[:,2]\n",
    "\n",
    "    filtered_points = {dim: getattr(laz, dim)[mask_valid] for dim in laz.point_format.dimension_names}\n",
    "    header = laspy.LasHeader(point_format=laz.header.point_format, version=laz.header.version)\n",
    "    header.point_count = 0\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "\n",
    "    #   _Assign filtered and modified data\n",
    "    for dim, values in filtered_points.items():\n",
    "        setattr(new_las, dim, values)\n",
    "\n",
    "    setattr(new_las, 'x', points_flatten[:,0])\n",
    "    setattr(new_las, 'y', points_flatten[:,1])\n",
    "    setattr(new_las, 'z', points_flatten[:,2])\n",
    "\n",
    "    #   _Save new file\n",
    "    # flatten_dir = os.path.join(os.path.dirname(tile_src), 'flatten')\n",
    "    # os.makedirs(flatten_dir, exist_ok=True)\n",
    "    # new_las.write(os.path.join(os.path.dirname(tile_src), os.path.basename(tile_src).split('.laz')[0] + f\"_flatten_{grid_size}m.laz\"))\n",
    "    new_las.write(tile_new_original_src.split('.laz')[0] + \"_flatten.laz\")\n",
    "    if verbose:\n",
    "        print(\"Saved file: \", tile_new_original_src.split('.laz')[0] + \"_flatten.laz\")\n",
    "\n",
    "    # Resize original file\n",
    "    laz.points = laz.points[mask_valid]\n",
    "    laz.write(tile_new_original_src)\n",
    "    if verbose:\n",
    "        print(\"Saved file: \", tile_new_original_src)\n",
    "\n",
    "\n",
    "def flattening(src_tiles, src_new_tiles, grid_size=10, verbose=True, do_keep_existing=False, verbose_full=False):\n",
    "    \"\"\"\n",
    "    Applies the flattening process to all tiles in a directory using grid-based ground surface estimation.\n",
    "\n",
    "    Args:\n",
    "        - src_tiles (str): Path to the directory containing original tiles.\n",
    "        - src_new_tiles (str): Path to the directory where resized tiles will be saved.\n",
    "        - grid_size (int, optional): Size of the grid in meters for interpolation. Defaults to 10.\n",
    "        - verbose (bool, optional): Whether to show a general progress bar. Defaults to True.\n",
    "        - verbose_full (bool, optional): Whether to print detailed info per tile. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        - None: Processes and saves flattened tiles into their respective folders.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting flattening:\")\n",
    "    list_tiles = [x for x in os.listdir(src_tiles) if x.endswith('.laz')]\n",
    "    for _, tile in tqdm(enumerate(list_tiles), total=len(list_tiles), desc=\"Processing\", disable=verbose==False):\n",
    "        if verbose_full:\n",
    "            print(\"Flattening tile: \", tile)\n",
    "        \n",
    "        flattening_tile(\n",
    "            tile_src=os.path.join(src_tiles, tile), \n",
    "            tile_new_original_src=os.path.join(src_new_tiles, tile),\n",
    "            grid_size=grid_size,\n",
    "            do_keep_existing=do_keep_existing,\n",
    "            verbose=verbose_full,\n",
    "            )\n",
    "\n",
    "# # test striping fast\n",
    "# src_file = r\"D:\\GitHubProjects\\PointCloudUtils\\data\\test_big_tiling\\flattening_10m\\test\\SwissSurface_2019_2538_1154_tile_67_with_id.laz\"\n",
    "# src_folder_stripes = os.path.join(os.path.dirname(src_file), os.path.basename(src_file).split('.laz')[0] + \"_stripes\")\n",
    "# laz_original = laspy.read(src_file)\n",
    "# x_span = laz_original.x.max() - laz_original.x.min()\n",
    "# y_span = laz_original.y.max() - laz_original.y.min()\n",
    "# dims = [x_span, y_span]\n",
    "\n",
    "# dims[np.argmin([x_span, y_span])] = 10\n",
    "\n",
    "# print(\"Creation of stripes:\")\n",
    "# stripes_file_fast(src_file, src_folder_stripes, dims, do_keep_existing=True)\n",
    "\n",
    "# print(len(laz_original))\n",
    "# print(len(set(laz_original.id_point)))\n",
    "# print(max(set(laz_original.id_point)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52238\n",
      "52238\n",
      "98134879\n"
     ]
    }
   ],
   "source": [
    "src_file = r\"D:\\GitHubProjects\\PointCloudUtils\\data\\test_big_tiling\\flattening_10m\\Barmasse_2023_AllScans_Raw_Georef_flatten_stripes\\Barmasse_2023_AllScans_Raw_Georef_flatten_stripe_4.laz\"\n",
    "laz_original = laspy.read(src_file)\n",
    "print(len(laz_original))\n",
    "print(len(set(laz_original.id_point)))\n",
    "print(max(set(laz_original.id_point)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3105ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding an id to the points\n",
    "id_point = np.arange(len(laz_original))\n",
    "laz_original.add_extra_dim(laspy.ExtraBytesParams('id_point', type=\"uint32\"))\n",
    "laz_original.id_point = id_point\n",
    "\n",
    "src_with_id = src_original.split(f'.{ext}')[0] + f\"_with_id.{ext}\"\n",
    "laz_original.write(src_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca77be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tilling (with overlap = 50m)...\n",
      "Computing the estimated number of tiles...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [10:38<00:00, 106.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# tiles with overlap\n",
    "src_folder_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split(f'.{ext}')[0] + \"_tiles_overlap\")\n",
    "tilling(src_with_id, src_folder_tiles_w_overlap, tile_size, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c7a28d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tilling (with overlap = 0m)...\n",
      "Computing the estimated number of tiles...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [14:28<00:00, 144.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# tiles without overlap\n",
    "src_folder_tiles_wo_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split(f'.{ext}')[0] + \"_tiles_no_overlap\")\n",
    "tilling(src_with_id, src_folder_tiles_wo_overlap, tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da5fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting flattening:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 6/6 [20:41<00:00, 206.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# Flattening of tiles with overlap\n",
    "src_folder_flatten_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_flatten\")\n",
    "os.makedirs(src_folder_flatten_tiles_w_overlap, exist_ok=True)\n",
    "flattening(\n",
    "    src_tiles=src_folder_tiles_w_overlap,\n",
    "    src_new_tiles=src_folder_flatten_tiles_w_overlap,\n",
    "    grid_size=grid_size,\n",
    "    verbose=True,\n",
    "    do_keep_existing=True,\n",
    "    verbose_full=False,\n",
    ")\n",
    "# flattening(src_tiles, src_new_tiles, grid_size=10, verbose=True, verbose_full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2be27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating flaten tiles without overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barmasse_2023_AllScans_Raw_Georef_with_id_tile_0_0.laz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:02,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barmasse_2023_AllScans_Raw_Georef_with_id_tile_0_1.laz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:03<00:08,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barmasse_2023_AllScans_Raw_Georef_with_id_tile_1_0.laz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:11<00:13,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barmasse_2023_AllScans_Raw_Georef_with_id_tile_1_1.laz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:29<00:19,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barmasse_2023_AllScans_Raw_Georef_with_id_tile_2_0.laz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:31<00:06,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barmasse_2023_AllScans_Raw_Georef_with_id_tile_2_1.laz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:31<00:00,  5.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Creating flatten tiles w/o overlap\n",
    "def hash_coords(x, y, z, rounding=2):\n",
    "    \"\"\"Create a unique integer hash for each rounded coordinate triple.\"\"\"\n",
    "    return np.round(x, rounding) * 1e12 + np.round(y, rounding) * 1e6 + np.round(z, rounding)\n",
    "\n",
    "list_flatten_to_merge = []\n",
    "print(\"Creating flaten tiles without overlap\")\n",
    "list_tiles = [x for x in os.listdir(src_folder_flatten_tiles_w_overlap) if not 'flatten' in x and not 'floor' in x]\n",
    "for _, tile in tqdm(enumerate(list_tiles), total=len(list_tiles)):\n",
    "    assert os.path.exists(os.path.join(src_folder_tiles_wo_overlap, tile))\n",
    "\n",
    "    laz_with_ov = laspy.read(os.path.join(src_folder_flatten_tiles_w_overlap, tile))\n",
    "    laz_without_ov = laspy.read(os.path.join(src_folder_tiles_wo_overlap, tile))\n",
    "    laz_flatten_with_ov = laspy.read(os.path.join(src_folder_flatten_tiles_w_overlap, tile.split('.laz')[0] + \"_flatten.laz\"))\n",
    "\n",
    "    # with_ov_hash = hash_coords(laz_with_ov.x, laz_with_ov.y, laz_with_ov.z)\n",
    "    # without_ov_hash = hash_coords(laz_without_ov.x, laz_without_ov.y, laz_without_ov.z)\n",
    "\n",
    "    mask = np.isin(laz_with_ov.id_point, laz_without_ov.id_point)\n",
    "    # mask = np.isin(with_ov_hash, without_ov_hash)\n",
    "\n",
    "    laz_flatten_wo_ov = laz_flatten_with_ov[mask]\n",
    "    laz_flatten_wo_ov.write(os.path.join(src_folder_flatten_tiles_w_overlap, tile.split('.laz')[0] + \"_flatten_no_ov.laz\"))\n",
    "    list_flatten_to_merge.append(os.path.join(src_folder_flatten_tiles_w_overlap, tile.split('.laz')[0] + \"_flatten_no_ov.laz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "631ac9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all flatten tiles together (might take a few minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "113669835"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging\n",
    "print(\"Merging all flatten tiles together (might take a few minutes)\")\n",
    "src_flatten_file = os.path.join(src_original.split('.laz')[0] + \"_flatten.laz\")\n",
    "pipeline_json = {\n",
    "    \"pipeline\": list_flatten_to_merge + [\n",
    "        {\"type\": \"filters.merge\"},\n",
    "        {\"type\": \"writers.las\", \"filename\": src_flatten_file, 'extra_dims': 'all'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df29522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of stripes:\n",
      "Column 1 / 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:00<00:00, 21218.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate stripes from the merged flatten\n",
    "src_folder_stripes = os.path.join(os.path.dirname(src_flatten_file), os.path.basename(src_flatten_file).split('.laz')[0] + \"_stripes\")\n",
    "x_span = laz_original.x.max() - laz_original.x.min()\n",
    "y_span = laz_original.y.max() - laz_original.y.min()\n",
    "dims = [x_span, y_span]\n",
    "\n",
    "dims[np.argmin([x_span, y_span])] = stripe_width\n",
    "\n",
    "print(\"Creation of stripes:\")\n",
    "stripes_file_fast(src_flatten_file, src_folder_stripes, dims, do_keep_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5617d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of stripes:\n",
      "Column 1 / 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:00<00:00, 9618.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate stripes from the original\n",
    "src_folder_stripes = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_stripes\")\n",
    "x_span = laz_original.x.max() - laz_original.x.min()\n",
    "y_span = laz_original.y.max() - laz_original.y.min()\n",
    "dims = [x_span, y_span]\n",
    "\n",
    "dims[np.argmin([x_span, y_span])] = stripe_width\n",
    "\n",
    "print(\"Creation of stripes:\")\n",
    "stripes_file_fast(src_with_id, src_folder_stripes, dims, do_keep_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63f110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "363ed3e4",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1700dd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barmasse 23:\n",
      "\t num points: 769024\n",
      "\t X in range: [2587220.0, 2587249.75]\n",
      "\t Y in range: [1098717.875, 1098748.75]\n",
      "\t Z in range: [1428.497, 1458.161]\n",
      "\t Bbox size: [29, 30, 29]\n",
      "Barmasse 24:\n",
      "\t num points: 47350\n",
      "\t X in range: [-502.74, -473.49]\n",
      "\t Y in range: [-698.26, -671.0500000000001]\n",
      "\t Z in range: [27.990000000000002, 59.870000000000005]\n",
      "\t Bbox size: [29, 27, 31]\n",
      "Lausanne:\n",
      "\t num points: 162871\n",
      "\t X in range: [2536804.29, 2536904.2800000003]\n",
      "\t Y in range: [1151628.73, 1151728.72]\n",
      "\t Z in range: [377.68, 417.01]\n",
      "\t Bbox size: [99, 99, 39]\n"
     ]
    }
   ],
   "source": [
    "src_barmasse_23 = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\ForestFormer3D\\data\\ForAINetV2\\test_data\\small_test_barmasse_2023.laz\"\n",
    "src_barmasse_24 = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\ForestFormer3D\\data\\ForAINetV2\\test_data\\other_small_test_downsampled_factor_0.1.laz\"\n",
    "src_lausanne = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\ForestFormer3D\\data\\ForAINetV2\\test_data\\SwissSurface_2019_2536_1151_tile_88.laz\"\n",
    "\n",
    "name_place = [\"Barmasse 23\",\"Barmasse 24\",\"Lausanne\"]\n",
    "src_places = [src_barmasse_23, src_barmasse_24, src_lausanne]\n",
    "for i in range(3):\n",
    "    print(f\"{name_place[i]}:\")\n",
    "    laz = laspy.read(src_places[i])\n",
    "    print(f\"\\t num points: {len(laz)}\")\n",
    "    print(f\"\\t X in range: [{min(laz.x)}, {max(laz.x)}]\")\n",
    "    print(f\"\\t Y in range: [{min(laz.y)}, {max(laz.y)}]\")\n",
    "    print(f\"\\t Z in range: [{min(laz.z)}, {max(laz.z)}]\")\n",
    "    deltax = int(max(laz.x) - min(laz.x))\n",
    "    deltay = int(max(laz.y) - min(laz.y))\n",
    "    deltaz = int(max(laz.z) - min(laz.z))\n",
    "    print(f\"\\t Bbox size: [{deltax}, {deltay}, {deltaz}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25ea6c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 371\n",
      "\t Garbage: 36 (9%)\n",
      "\t Multi: 27 (7%)\n",
      "\t Single: 308 (83%)\n"
     ]
    }
   ],
   "source": [
    "garbage = 36\n",
    "multi = 27\n",
    "single = 308\n",
    "\n",
    "total = garbage + multi + single\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"\\t Garbage: {garbage} ({int(garbage / total * 100)}%)\")\n",
    "print(f\"\\t Multi: {multi} ({int(multi / total * 100)}%)\")\n",
    "print(f\"\\t Single: {single} ({int(single / total * 100)}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
