{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fde7713",
   "metadata": {},
   "source": [
    "# Classification training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9983a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "099d319b",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bcb3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import laspy\n",
    "import pdal\n",
    "import shutil\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252da9d",
   "metadata": {},
   "source": [
    "### Extract all samples of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7a53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def instance_extraction(src_tile, src_dest, inst_column='PredInstance', verbose=False):\n",
    "    las = laspy.read(src_tile)\n",
    "    instances = getattr(las, inst_column)\n",
    "    instances_unique = np.unique(instances)\n",
    "    for _, inst_id in tqdm(enumerate(instances_unique), total=len(instances_unique), disable = verbose == False):\n",
    "        if inst_id == 0:\n",
    "            continue\n",
    "        mask = instances == inst_id\n",
    "\n",
    "        sub_points = las.points[mask]\n",
    "\n",
    "        if len(sub_points) == 0:\n",
    "            continue\n",
    "\n",
    "        # Create a new LAS object with the same header\n",
    "        # header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "        # header.scales = las.header.scales\n",
    "        # header.offsets = las.header.offsets\n",
    "        # # header.add_crs(las.header.parse_crs())\n",
    "        sub_las = laspy.LasData(las.header)\n",
    "        sub_las.points = sub_points\n",
    "\n",
    "        # Define output path\n",
    "        out_path = os.path.join(src_dest, f\"{''.join(os.path.basename(src_tile).split('.')[:-1])}_{inst_id}.laz\")\n",
    "\n",
    "        # Save the new instance file\n",
    "        os.makedirs(src_dest, exist_ok=True)\n",
    "        sub_las.write(out_path)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Saved instance {inst_id} with {len(sub_points)} points → {out_path}\")\n",
    "\n",
    "def convert_laz_to_pcd(in_laz, out_pcd, verbose=False):\n",
    "    laz = laspy.read(in_laz)\n",
    "\n",
    "    # Gathering all attributes from laz file\n",
    "    points = np.vstack((laz.x, laz.y, laz.z)).T\n",
    "\n",
    "    attributes = {}\n",
    "    for attribute in laz.point_format.dimensions:\n",
    "        if attribute.name in ['X', 'Y', 'Z']:\n",
    "            continue\n",
    "        attributes[attribute.name] = getattr(laz, attribute.name)\n",
    "    \n",
    "    # Preparing data for pcd\n",
    "    num_points = points.shape[0]\n",
    "    fields = [\"x\", \"y\", \"z\"] + list(attributes.keys())  # All field names\n",
    "    types = [\"F\", \"F\", \"F\"] + [\"F\" for _ in attributes]  # Float32 fields\n",
    "    sizes = [4] * len(fields)  # 4-byte float per field\n",
    "\n",
    "    # Stack all data into a single NumPy array\n",
    "    data = np.column_stack([points] + [attributes[key] for key in attributes])\n",
    "\n",
    "    # Write to a PCD file\n",
    "    with open(out_pcd, \"w\") as f:\n",
    "        # f.write(f\"# .PCD v0.7 - Point Cloud Data file format\\n\")\n",
    "        f.write(f\"VERSION 0.7\\n\")\n",
    "        f.write(f\"FIELDS {' '.join(fields)}\\n\")\n",
    "        f.write(f\"SIZE {' '.join(map(str, sizes))}\\n\")\n",
    "        f.write(f\"TYPE {' '.join(types)}\\n\")\n",
    "        f.write(f\"COUNT {' '.join(['1'] * len(fields))}\\n\")\n",
    "        f.write(f\"WIDTH {num_points}\\n\")\n",
    "        f.write(f\"HEIGHT 1\\n\")\n",
    "        f.write(f\"VIEWPOINT 0 0 0 1 0 0 0\\n\")\n",
    "        f.write(f\"POINTS {num_points}\\n\")\n",
    "        f.write(f\"DATA ascii\\n\")\n",
    "    \n",
    "        # Write data\n",
    "        np.savetxt(f, data, fmt=\" \".join([\"%.6f\"] * len(fields)))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"PCD file saved in {out_pcd}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aaddcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting samples: 100%|██████████| 13/13 [00:01<00:00, 10.81it/s]\n",
      "Converting samples to PCD: 100%|██████████| 60/60 [00:08<00:00,  7.42it/s]\n",
      "Removing LAZ files: 100%|██████████| 60/60 [00:00<00:00, 5127.72it/s]\n"
     ]
    }
   ],
   "source": [
    "src_in = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\lausanne\\train\\temp_instances_treelearn\"\n",
    "src_out = os.path.join(src_in, \"instances\")\n",
    "instance_column = \"treeID\"\n",
    "\n",
    "# Extract all samples:\n",
    "list_files = [x for x in os.listdir(src_in) if x.endswith('laz')]\n",
    "for _, f in tqdm(enumerate(list_files), total=len(list_files), desc='Extracting samples'):\n",
    "    instance_extraction(os.path.join(src_in, f), src_out, instance_column)\n",
    "\n",
    "# Convert samples to pcd\n",
    "list_samples = [x for x in os.listdir(src_out) if x.endswith('laz')]\n",
    "for _, file in tqdm(enumerate(list_samples), total=len(list_samples), desc='Converting samples to PCD'):\n",
    "    file_in = os.path.join(src_out, file)\n",
    "    file_out = file_in.split('.laz')[0] + '.pcd'\n",
    "    convert_laz_to_pcd(file_in, file_out, False)\n",
    "\n",
    "# Remove all laz files\n",
    "for _, file in tqdm(enumerate(list_samples), total=len(list_samples), desc='Removing LAZ files'):\n",
    "    if file.endswith('.laz'):\n",
    "        os.remove(os.path.join(src_out, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161a769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
