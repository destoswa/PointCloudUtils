{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef69897",
   "metadata": {},
   "source": [
    "# Flatten tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285e6bf",
   "metadata": {},
   "source": [
    "# table of content\n",
    "1) [Splitting](#splitting)\n",
    "2) [Flattening of stripes](#flattening-of-stripes)\n",
    "3) [Manual cleaning](#manual-cleaning)\n",
    "4) [Post-process](#post-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54de91",
   "metadata": {},
   "source": [
    "### Dependencies and general utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e637d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "import pdal\n",
    "import json\n",
    "import scipy\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a2fff",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e447d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_original = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\quadric_interp\\Barmasse_2025_AllScans_Raw_Sub2cm.laz\"\n",
    "tile_size = 500\n",
    "overlap = 50\n",
    "grid_size = 5\n",
    "stripe_width = 10\n",
    "method='quadric'\n",
    "do_save_floor=True\n",
    "do_keep_existing_flatten = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce01b04",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fdf98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def tilling_pdal(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "    \"\"\"\n",
    "    Tile the input LiDAR file into square tiles using PDAL and store them in the destination folder.\n",
    "\n",
    "    Args:\n",
    "        - verbose (bool): Whether to print verbose status updates.\n",
    "\n",
    "    Returns:\n",
    "        - None: Splits the input file into tiles and saves them in the destination folder.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Start tilling (with overlap = {overlap}m)...\")\n",
    "    os.makedirs(src_target, exist_ok=True)\n",
    "    \n",
    "    # compute the estimate number of tiles\n",
    "    if verbose:\n",
    "        print(\"Computing the bounds...\")\n",
    "    original_file = laspy.read(src_input)\n",
    "    x_min = original_file.x.min()\n",
    "    x_max = original_file.x.max()\n",
    "    y_min = original_file.y.min()\n",
    "    y_max = original_file.y.max() \n",
    "    if verbose:\n",
    "        print('Done!')\n",
    "\n",
    "    output_pattern = os.path.join(\n",
    "        src_target, \n",
    "        os.path.basename(src_input).split('.')[0] + \"_tile_#.laz\",\n",
    "        )\n",
    "\n",
    "    x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "    y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "    combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "    list_bounds = []\n",
    "    for (i,j) in combinations:\n",
    "        x0 = x_min + i * tile_size - overlap\n",
    "        x1 = x_min + (i + 1) * tile_size + overlap\n",
    "        y0 = y_min + j * tile_size - overlap\n",
    "        y1 = y_min + (j + 1) * tile_size + overlap\n",
    "\n",
    "        bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "        list_bounds.append(bounds)\n",
    "\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": src_input,\n",
    "                \"extra_dims\": \"id_point=uint32\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.crop\", \n",
    "                \"bounds\": list_bounds\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.las\", \n",
    "                \"filename\": output_pattern, \n",
    "                'extra_dims': \"id_point=uint32\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Creation of the tiles (might take a few minutes)\")\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "    if verbose:\n",
    "        print(\"Process done\")\n",
    "\n",
    "\n",
    "def tilling(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "    \"\"\"\n",
    "    Crops a LAS/LAZ file into tiles using laspy directly (preserves id_point and coordinates).\n",
    "    \"\"\"\n",
    "    os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "    las = laspy.read(src_input)\n",
    "    x_min, x_max = las.x.min(), las.x.max()\n",
    "    y_min, y_max = las.y.min(), las.y.max()\n",
    "\n",
    "    x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "    y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "\n",
    "    combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "    \n",
    "    print(\"Creation of tiles:\")\n",
    "    for _, (ix, iy) in tqdm(enumerate(combinations), total=len(combinations)):\n",
    "    # for ix in range(x_steps):\n",
    "    #     for iy in range(y_steps):\n",
    "        x0 = x_min + ix * tile_size - overlap\n",
    "        x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "        y0 = y_min + iy * tile_size - overlap\n",
    "        y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "        mask = (\n",
    "            (las.x >= x0) & (las.x <= x1) &\n",
    "            (las.y >= y0) & (las.y <= y1)\n",
    "        )\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "\n",
    "        # ✅ Create new file with proper header scale/offset\n",
    "        header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "        header.offsets = las.header.offsets\n",
    "        header.scales = las.header.scales\n",
    "\n",
    "        # ✅ Copy CRS if any\n",
    "        if hasattr(las.header, \"epsg\") and las.header.epsg is not None:\n",
    "            header.epsg = las.header.epsg\n",
    "\n",
    "        tile = laspy.LasData(header)\n",
    "        tile.points = las.points[mask]\n",
    "\n",
    "        tile_filename = os.path.join(\n",
    "            src_target,\n",
    "            f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "        )\n",
    "        tile.write(tile_filename)\n",
    "\n",
    "\n",
    "def stripes_file(src_input_file, src_output, dims, do_keep_existing=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Crops a LAS/LAZ file into tiles using laspy directly (preserves id_point and coordinates).\n",
    "    \"\"\"\n",
    "    [tile_size_x, tile_size_y] = dims\n",
    "    las = laspy.read(src_input_file)\n",
    "    os.makedirs(src_output, exist_ok=True)\n",
    "\n",
    "    xmin, xmax, ymin, ymax = np.min(las.x), np.max(las.x), np.min(las.y), np.max(las.y)\n",
    "\n",
    "    x_edges = np.arange(xmin, xmax, tile_size_x)\n",
    "    y_edges = np.arange(ymin, ymax, tile_size_y)\n",
    "\n",
    "    combinations = list(itertools.product(x_edges, y_edges))\n",
    "    # for (x0, y0) in combinations:\n",
    "    num_skipped = 0\n",
    "    for id_stripe, (x0, y0) in tqdm(enumerate(combinations), total=len(combinations)):\n",
    "        x1 = x0 + tile_size_x\n",
    "        y1 = y0 + tile_size_y\n",
    "\n",
    "        mask = (\n",
    "            (las.x >= x0) & (las.x <= x1) &\n",
    "            (las.y >= y0) & (las.y <= y1)\n",
    "        )\n",
    "        if not np.any(mask):\n",
    "            num_skipped += 1\n",
    "            continue\n",
    "\n",
    "        # ✅ Create new file with proper header scale/offset\n",
    "        header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "        header.offsets = las.header.offsets\n",
    "        header.scales = las.header.scales\n",
    "\n",
    "        # ✅ Copy CRS if any\n",
    "        if hasattr(las.header, \"epsg\") and las.header.epsg is not None:\n",
    "            header.epsg = las.header.epsg\n",
    "\n",
    "        tile = laspy.LasData(header)\n",
    "        tile.points = las.points[mask]\n",
    "\n",
    "        tile_filename = os.path.join(\n",
    "            src_output,\n",
    "            f\"{os.path.splitext(os.path.basename(src_input_file))[0]}_stripe_{id_stripe - num_skipped}.laz\"\n",
    "        )\n",
    "        tile.write(tile_filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "    # las = laspy.read(src_input)\n",
    "    # x_min, x_max = las.x.min(), las.x.max()\n",
    "    # y_min, y_max = las.y.min(), las.y.max()\n",
    "\n",
    "    # x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "    # y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "\n",
    "    # combination = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "    \n",
    "    # print(\"Creation of tiles:\")\n",
    "    # for _, (ix, iy) in tqdm(enumerate(combination)):\n",
    "    # # for ix in range(x_steps):\n",
    "    # #     for iy in range(y_steps):\n",
    "    #     x0 = x_min + ix * tile_size - overlap\n",
    "    #     x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "    #     y0 = y_min + iy * tile_size - overlap\n",
    "    #     y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "    #     mask = (\n",
    "    #         (las.x >= x0) & (las.x <= x1) &\n",
    "    #         (las.y >= y0) & (las.y <= y1)\n",
    "    #     )\n",
    "    #     if not np.any(mask):\n",
    "    #         continue\n",
    "\n",
    "    #     # ✅ Create new file with proper header scale/offset\n",
    "    #     header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "    #     header.offsets = las.header.offsets\n",
    "    #     header.scales = las.header.scales\n",
    "\n",
    "    #     # ✅ Copy CRS if any\n",
    "    #     if hasattr(las.header, \"epsg\") and las.header.epsg is not None:\n",
    "    #         header.epsg = las.header.epsg\n",
    "\n",
    "    #     tile = laspy.LasData(header)\n",
    "    #     tile.points = las.points[mask]\n",
    "\n",
    "    #     tile_filename = os.path.join(\n",
    "    #         src_target,\n",
    "    #         f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "    #     )\n",
    "    #     tile.write(tile_filename)\n",
    "\n",
    "\n",
    "def stripes_file_pdal(src_input_file, src_output, dims, do_keep_existing=False, verbose=True):\n",
    "    [tile_size_x, tile_size_y] = dims\n",
    "    laz = laspy.read(src_input_file)\n",
    "    os.makedirs(src_output, exist_ok=True)\n",
    "\n",
    "    xmin, xmax, ymin, ymax = np.min(laz.x), np.max(laz.x), np.min(laz.y), np.max(laz.y)\n",
    "\n",
    "    x_edges = np.arange(xmin, xmax, tile_size_x)\n",
    "    y_edges = np.arange(ymin, ymax, tile_size_y)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computing the bounds...\")\n",
    "\n",
    "    list_bounds = []\n",
    "    combinations = list(itertools.product(x_edges, y_edges))\n",
    "    for (x0, y0) in combinations:\n",
    "        # for j, y0 in tqdm(enumerate(y_edges), total=len(y_edges)):\n",
    "        x1 = x0 + tile_size_x\n",
    "        y1 = y0 + tile_size_y\n",
    "        bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "        \n",
    "        list_bounds.append(bounds)\n",
    "\n",
    "    output_pattern = os.path.join(\n",
    "            src_output, \n",
    "            os.path.basename(src_input_file).split('.')[0] + \"_stripe_#.laz\",\n",
    "            )\n",
    "    \n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": src_input_file,\n",
    "                \"spatialreference\": \"EPSG:2056\",\n",
    "                \"extra_dims\": \"id_point=uint32\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.crop\", \n",
    "                \"bounds\": list_bounds,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.las\", \n",
    "                \"filename\": output_pattern, \n",
    "                'extra_dims': \"id_point=uint32\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    if verbose:\n",
    "        print(\"Creation of the stripes (might take a few minutes)\")\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "    if verbose:\n",
    "        print(\"Process done\")\n",
    "\n",
    "\n",
    "def remove_duplicates(laz_file, decimals=2):\n",
    "    \"\"\"\n",
    "    Removes duplicate points from a LAS/LAZ file based on rounded 3D coordinates.\n",
    "\n",
    "    Args:\n",
    "        - laz_file (laspy.LasData): Input LAS/LAZ file as a laspy object.\n",
    "        - decimals (int, optional): Number of decimals to round the coordinates for duplicate detection. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        - laspy.LasData: A new laspy object with duplicate points removed.\n",
    "    \"\"\"\n",
    "        \n",
    "    coords = np.round(np.vstack((laz_file.x, laz_file.y, laz_file.z)).T, decimals)\n",
    "    _, unique_indices = np.unique(coords, axis=0, return_index=True)\n",
    "    mask = np.zeros(len(coords), dtype=bool)\n",
    "    mask[unique_indices] = True\n",
    "\n",
    "    # Create new LAS object\n",
    "    header = laspy.LasHeader(point_format=laz_file.header.point_format, version=laz_file.header.version)\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "    for dim in laz_file.point_format.dimension_names:\n",
    "        setattr(new_las, dim, getattr(laz_file, dim)[mask])\n",
    "\n",
    "    return new_las\n",
    "\n",
    "\n",
    "def match_pointclouds(laz1, laz2):\n",
    "    \"\"\"Sort laz2 to match the order of laz1 without changing laz1's order.\n",
    "\n",
    "    Args:\n",
    "        laz1: laspy.LasData object (reference order)\n",
    "        laz2: laspy.LasData object (to be sorted)\n",
    "    \n",
    "    Returns:\n",
    "        laz2 sorted to match laz1\n",
    "    \"\"\"\n",
    "    # Retrieve and round coordinates for robust matching\n",
    "    coords_1 = np.round(np.vstack((laz1.x, laz1.y, laz1.z)), 2).T\n",
    "    coords_2 = np.round(np.vstack((laz2.x, laz2.y, laz2.z)), 2).T\n",
    "\n",
    "    # Verify laz2 is of the same size as laz1\n",
    "    assert len(coords_2) == len(coords_1), \"laz2 should be a subset of laz1\"\n",
    "\n",
    "    # Create a dictionary mapping from coordinates to indices\n",
    "    coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(coords_1)}\n",
    "\n",
    "    # Find indices in laz1 that correspond to laz2\n",
    "    matching_indices = []\n",
    "    failed = 0\n",
    "    for coord in coords_2:\n",
    "        try:\n",
    "            matching_indices.append(coord_to_idx[tuple(coord)])\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "\n",
    "    matching_indices = np.array([coord_to_idx[tuple(coord)] for coord in coords_2])\n",
    "\n",
    "    # Sort laz2 to match laz1\n",
    "    sorted_indices = np.argsort(matching_indices)\n",
    "\n",
    "    # Apply sorting to all attributes of laz2\n",
    "    laz2.points = laz2.points[sorted_indices]\n",
    "\n",
    "    return laz2  # Now sorted to match laz1\n",
    "\n",
    "\n",
    "def flattening_tile(tile_src, tile_new_original_src, grid_size=10, method='cubic', do_save_floor=False, do_keep_existing=False, do_extrapolate_outside_hull=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Flattens a tile by interpolating the ground surface and subtracting it from the original elevation.\n",
    "\n",
    "    Args:\n",
    "        - tile_src (str): Path to the input tile in LAS/LAZ format.\n",
    "        - tile_new_original_src (str): Path to save the resized original tile after filtering.\n",
    "        - grid_size (int, optional): Size of the grid in meters for local interpolation. Defaults to 10.\n",
    "        - verbose (bool, optional): Whether to display progress and debug information. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        - None: Saves the floor and flattened versions of the tile and updates the original file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(tile_new_original_src) and do_keep_existing:\n",
    "        if verbose:\n",
    "            print(f\"Skipping. {tile_new_original_src} exists already\")\n",
    "        return\n",
    "    \n",
    "    # Load file\n",
    "    laz = laspy.read(tile_src)\n",
    "    init_len = len(laz)\n",
    "    if init_len == 0:\n",
    "        return\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Removing duplicates: From {init_len} to {len(laz)}\")\n",
    "    \n",
    "    points = np.vstack((laz.x, laz.y, laz.z)).T\n",
    "    points_flatten = copy.deepcopy(points)\n",
    "    points_interpolated = copy.deepcopy(points)\n",
    "\n",
    "    # Divide into tiles and find local minimums\n",
    "    x_min, y_min = np.min(points[:, :2], axis=0)\n",
    "    x_max, y_max = np.max(points[:, :2], axis=0)\n",
    "\n",
    "    x_bins = np.append(np.arange(x_min, x_max, grid_size), x_max)\n",
    "    y_bins = np.append(np.arange(y_min, y_max, grid_size), y_max)\n",
    "\n",
    "    grid = {i:{j:[] for j in range(y_bins.size - 1)} for i in range(x_bins.size -1)}\n",
    "    for _, (px, py, pz) in tqdm(enumerate(points), total=len(points), desc=\"Creating grid\", disable=verbose==False):\n",
    "        xbin = np.clip(0, (px - x_min) // grid_size, x_bins.size - 2)\n",
    "        ybin = np.clip(0, (py - y_min) // grid_size, y_bins.size - 2)\n",
    "        try:\n",
    "            grid[xbin][ybin].append((px, py, pz))\n",
    "        except Exception as e:\n",
    "            print(xbin)\n",
    "            print(ybin)\n",
    "            print(x_bins)\n",
    "            print(y_bins)\n",
    "            print(grid.keys())\n",
    "            print(grid[0].keys())\n",
    "            raise e\n",
    "\n",
    "\n",
    "    # Create grid_min\n",
    "    grid_used = np.zeros((x_bins.size - 1, y_bins.size - 1))\n",
    "    lst_grid_min = []\n",
    "    lst_grid_min_pos = []\n",
    "    for x in grid.keys():\n",
    "        for y in grid[x].keys():\n",
    "            if np.array(grid[x][y]).shape[0] > 0:\n",
    "                grid_used[x, y] = 1\n",
    "                lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                arg_min = np.argmin(np.array(grid[x][y])[:,2])\n",
    "                lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2])\n",
    "\n",
    "                # test if border\n",
    "                if x == list(grid.keys())[0]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] - [5, 0])\n",
    "                if x == list(grid.keys())[-1]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] + [5, 0])\n",
    "                if y == list(grid[x].keys())[0]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] - [0, 5])\n",
    "                if y == list(grid[x].keys())[-1]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] + [0, 5])\n",
    "            else:\n",
    "                grid_used[x, y] = 0\n",
    "    arr_grid_min_pos = np.vstack(lst_grid_min_pos)\n",
    "    if verbose:\n",
    "        print(\"Resulting grid:\")\n",
    "        print(arr_grid_min_pos.shape)\n",
    "        print(grid_used)\n",
    "\n",
    "    # Interpolate\n",
    "    points_xy = np.array(points)[:,0:2]\n",
    "    if method == 'cubic':\n",
    "        interpolated_min_z = scipy.interpolate.griddata(arr_grid_min_pos, np.array(lst_grid_min), points_xy, method=\"cubic\", fill_value=-1)\n",
    "    elif method == 'quadric':\n",
    "        rbf = scipy.interpolate.Rbf(arr_grid_min_pos[:,0], arr_grid_min_pos[:,1], np.array(lst_grid_min), function='multiquadric', smooth=5)\n",
    "        interpolated_min_z = rbf(points_xy[:,0], points_xy[:,1])\n",
    "    else:\n",
    "        raise ValueError(\"Wrong argument for method!\")\n",
    "\n",
    "    # Fill NaNs with nearest neighbor interpolation\n",
    "    if do_extrapolate_outside_hull:\n",
    "        nan_mask = interpolated_min_z == -1\n",
    "        x = np.array(points)[:,0]\n",
    "        y = np.array(points)[:,1]\n",
    "\n",
    "        if np.any(nan_mask):\n",
    "            interpolated_min_z[nan_mask] = scipy.interpolate.griddata(arr_grid_min_pos, np.array(lst_grid_min), (x[nan_mask], y[nan_mask]), method='nearest')\n",
    "\n",
    "    mask_valid = np.array([x != -1 for x in list(interpolated_min_z)])\n",
    "    points_interpolated = points_interpolated[mask_valid]\n",
    "    points_interpolated[:, 2] = interpolated_min_z[mask_valid]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Interpolation:\")\n",
    "        print(f\"Original number of points: {points.shape[0]}\")\n",
    "        print(f\"Interpollated number of points: {points_interpolated.shape[0]} ({int(points_interpolated.shape[0] / points.shape[0]*100)}%)\")\n",
    "\n",
    "    # save floor\n",
    "    filtered_points = {dim: getattr(laz, dim)[mask_valid] for dim in laz.point_format.dimension_names}\n",
    "    header = laspy.LasHeader(point_format=laz.header.point_format, version=laz.header.version)\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "    #   _Assign filtered and modified data\n",
    "    for dim, values in filtered_points.items():\n",
    "        setattr(new_las, dim, values)\n",
    "    setattr(new_las, 'x', points_interpolated[:,0])\n",
    "    setattr(new_las, 'y', points_interpolated[:,1])\n",
    "    setattr(new_las, 'z', points_interpolated[:,2])\n",
    "\n",
    "    #   _Save new file\n",
    "    if do_save_floor:\n",
    "        new_las.write(tile_new_original_src.split('.laz')[0] + \"_floor.laz\")\n",
    "        if verbose:\n",
    "            print(\"Saved file: \", tile_new_original_src.split('.laz')[0] + \"_floor.laz\")\n",
    "\n",
    "    # Flatten\n",
    "    points_flatten = points_flatten[mask_valid]\n",
    "    points_flatten[:,2] = points_flatten[:,2] - points_interpolated[:,2]\n",
    "\n",
    "    filtered_points = {dim: getattr(laz, dim)[mask_valid] for dim in laz.point_format.dimension_names}\n",
    "    header = laspy.LasHeader(point_format=laz.header.point_format, version=laz.header.version)\n",
    "    header.point_count = 0\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "\n",
    "    #   _Assign filtered and modified data\n",
    "    for dim, values in filtered_points.items():\n",
    "        setattr(new_las, dim, values)\n",
    "\n",
    "    setattr(new_las, 'x', points_flatten[:,0])\n",
    "    setattr(new_las, 'y', points_flatten[:,1])\n",
    "    setattr(new_las, 'z', points_flatten[:,2])\n",
    "\n",
    "    #   _Save new file\n",
    "    new_las.write(tile_new_original_src.split('.laz')[0] + \"_flatten.laz\")\n",
    "    if verbose:\n",
    "        print(\"Saved file: \", tile_new_original_src.split('.laz')[0] + \"_flatten.laz\")\n",
    "\n",
    "    # Resize original file\n",
    "    laz.points = laz.points[mask_valid]\n",
    "    laz.write(tile_new_original_src)\n",
    "    if verbose:\n",
    "        print(\"Saved file: \", tile_new_original_src)\n",
    "\n",
    "\n",
    "def flattening(src_tiles, src_new_tiles, grid_size=10, verbose=True, method='cubic', do_save_floor=True, do_keep_existing=False, verbose_full=False):\n",
    "    \"\"\"\n",
    "    Applies the flattening process to all tiles in a directory using grid-based ground surface estimation.\n",
    "\n",
    "    Args:\n",
    "        - src_tiles (str): Path to the directory containing original tiles.\n",
    "        - src_new_tiles (str): Path to the directory where resized tiles will be saved.\n",
    "        - grid_size (int, optional): Size of the grid in meters for interpolation. Defaults to 10.\n",
    "        - verbose (bool, optional): Whether to show a general progress bar. Defaults to True.\n",
    "        - verbose_full (bool, optional): Whether to print detailed info per tile. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        - None: Processes and saves flattened tiles into their respective folders.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting flattening:\")\n",
    "    list_tiles = [x for x in os.listdir(src_tiles) if x.endswith('.laz')]\n",
    "    for _, tile in tqdm(enumerate(list_tiles), total=len(list_tiles), desc=\"Processing\", disable=verbose==False):\n",
    "        if verbose_full:\n",
    "            print(\"Flattening tile: \", tile)\n",
    "        if do_keep_existing and os.path.exists(os.path.join(src_new_tiles, tile).split('.laz')[0] + \"_flatten.laz\"):\n",
    "            continue\n",
    "\n",
    "        flattening_tile(\n",
    "            tile_src=os.path.join(src_tiles, tile), \n",
    "            tile_new_original_src=os.path.join(src_new_tiles, tile),\n",
    "            grid_size=grid_size,\n",
    "            method=method, \n",
    "            do_save_floor=do_save_floor,\n",
    "            do_keep_existing=do_keep_existing,\n",
    "            verbose=verbose_full,\n",
    "            )\n",
    "        \n",
    "\n",
    "def merge_laz(list_files, output_file):\n",
    "    \"\"\"\n",
    "    Merge multiple LAS/LAZ files into one using laspy.\n",
    "    Preserves all dimensions including extra_dims like 'id_point'.\n",
    "    \"\"\"\n",
    "    # Read the header from the first file\n",
    "    first_las = laspy.read(list_files[0])\n",
    "    header = laspy.LasHeader(point_format=first_las.header.point_format,\n",
    "                              version=first_las.header.version)\n",
    "\n",
    "    all_arrays = []\n",
    "    for _, f in tqdm(enumerate(list_files), total=len(list_files)):\n",
    "        las = laspy.read(f)\n",
    "        all_arrays.append(las.points.array)  # extract structured array\n",
    "        # print(f\"Read {f} ({len(las)} pts)\")\n",
    "\n",
    "    # Concatenate structured arrays\n",
    "    merged_array = np.concatenate(all_arrays)\n",
    "\n",
    "    # Create new LasData with header\n",
    "    out = laspy.LasData(header)\n",
    "\n",
    "    # Wrap the concatenated array as ScaleAwarePointRecord and assign\n",
    "    out.points = laspy.ScaleAwarePointRecord(\n",
    "        merged_array,\n",
    "        point_format=header.point_format,\n",
    "        scales=header.scales,\n",
    "        offsets=header.offsets\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    out.write(output_file)\n",
    "    print(f\"Merged file saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27d5acd",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34ef616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "start_preprocess_time = time()\n",
    "laz_original = laspy.read(src_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f0780c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_with_id = src_original.split('.laz')[0] + \"_with_id.laz\"\n",
    "src_folder_tiles_wo_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_tiles_no_overlap\")\n",
    "src_folder_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_tiles_overlap\")\n",
    "src_folder_flatten_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_flatten\")\n",
    "src_flatten_file = os.path.join(src_original.split('.laz')[0] + \"_flatten.laz\")\n",
    "src_folder_stripes = os.path.join(os.path.dirname(src_flatten_file), os.path.basename(src_flatten_file).split('.laz')[0] + f\"_stripes_{stripe_width}_m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2994d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding an id to the points\n",
    "id_point = np.arange(len(laz_original))\n",
    "laz_original.add_extra_dim(laspy.ExtraBytesParams('id_point', type=\"uint32\"))\n",
    "laz_original.id_point = id_point\n",
    "\n",
    "src_with_id = src_original.split('.laz')[0] + \"_with_id.laz\"\n",
    "laz_original.write(src_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e2ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of tiles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [03:29<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# tiles without overlap\n",
    "src_folder_tiles_wo_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_tiles_no_overlap\")\n",
    "tilling(src_with_id, src_folder_tiles_wo_overlap, tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a49f8783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of tiles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [03:46<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# tiles with overlap\n",
    "src_folder_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_tiles_overlap\")\n",
    "tilling(src_with_id, src_folder_tiles_w_overlap, tile_size, overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbc474",
   "metadata": {},
   "source": [
    "### Flattening of stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8d9a17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting flattening:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|███▉      | 29/73 [26:39<40:26, 55.14s/it]   \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 45.4 GiB for an array with shape (7453854, 818) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m src_folder_flatten_tiles_w_overlap \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(src_with_id), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src_with_id)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.laz\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flatten\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(src_folder_flatten_tiles_w_overlap, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mflattening\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_tiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_folder_tiles_w_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_new_tiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_folder_flatten_tiles_w_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_save_floor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_save_floor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_keep_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_keep_existing_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_full\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 529\u001b[0m, in \u001b[0;36mflattening\u001b[1;34m(src_tiles, src_new_tiles, grid_size, verbose, method, do_save_floor, do_keep_existing, verbose_full)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_keep_existing \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(src_new_tiles, tile)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.laz\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flatten.laz\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m \u001b[43mflattening_tile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtile_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtile_new_original_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_new_tiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_save_floor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_save_floor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_keep_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_keep_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 436\u001b[0m, in \u001b[0;36mflattening_tile\u001b[1;34m(tile_src, tile_new_original_src, grid_size, method, do_save_floor, do_keep_existing, do_extrapolate_outside_hull, verbose)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquadric\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    435\u001b[0m     rbf \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39minterpolate\u001b[38;5;241m.\u001b[39mRbf(arr_grid_min_pos[:,\u001b[38;5;241m0\u001b[39m], arr_grid_min_pos[:,\u001b[38;5;241m1\u001b[39m], np\u001b[38;5;241m.\u001b[39marray(lst_grid_min), function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiquadric\u001b[39m\u001b[38;5;124m'\u001b[39m, smooth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m--> 436\u001b[0m     interpolated_min_z \u001b[38;5;241m=\u001b[39m \u001b[43mrbf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints_xy\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints_xy\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong argument for method!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\PDM\\lib\\site-packages\\scipy\\interpolate\\_rbf.py:290\u001b[0m, in \u001b[0;36mRbf.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    288\u001b[0m xa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([a\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    289\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_norm(xa, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxi)\n\u001b[1;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes)\u001b[38;5;241m.\u001b[39mreshape(shp)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\PDM\\lib\\site-packages\\scipy\\interpolate\\_rbf.py:152\u001b[0m, in \u001b[0;36mRbf._h_multiquadric\u001b[1;34m(self, r)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_h_multiquadric\u001b[39m(\u001b[38;5;28mself\u001b[39m, r):\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 45.4 GiB for an array with shape (7453854, 818) and data type float64"
     ]
    }
   ],
   "source": [
    "# Flattening of tiles with overlap\n",
    "src_folder_flatten_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_flatten\")\n",
    "os.makedirs(src_folder_flatten_tiles_w_overlap, exist_ok=True)\n",
    "flattening(\n",
    "    src_tiles=src_folder_tiles_w_overlap,\n",
    "    src_new_tiles=src_folder_flatten_tiles_w_overlap,\n",
    "    grid_size=grid_size,\n",
    "    method=method,\n",
    "    do_save_floor=do_save_floor,\n",
    "    do_keep_existing=do_keep_existing_flatten,\n",
    "    verbose=True,\n",
    "    verbose_full=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b55b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating flaten tiles without overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [01:03<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all flatten tiles together (might take a few minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:08<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\quadric_interp\\Barmasse_2025_AllScans_Raw_Sub2cm_flatten.laz\n"
     ]
    }
   ],
   "source": [
    "# Creating flatten tiles w/o overlap and merging\n",
    "def hash_coords(x, y, z, rounding=2):\n",
    "    \"\"\"Create a unique integer hash for each rounded coordinate triple.\"\"\"\n",
    "    return np.round(x, rounding) * 1e12 + np.round(y, rounding) * 1e6 + np.round(z, rounding)\n",
    "\n",
    "list_flatten_to_merge = []\n",
    "print(\"Creating flaten tiles without overlap\")\n",
    "# list_tiles = [x for x in os.listdir(src_folder_flatten_tiles_w_overlap) if not x.endswith('_flatten.laz')]\n",
    "list_tiles = [x for x in os.listdir(src_folder_tiles_wo_overlap) if x.endswith('.laz')]\n",
    "for _, tile in tqdm(enumerate(list_tiles), total=len(list_tiles)):\n",
    "    # print(tile)\n",
    "    assert os.path.exists(os.path.join(src_folder_tiles_wo_overlap, tile))\n",
    "\n",
    "    laz_with_ov = laspy.read(os.path.join(src_folder_flatten_tiles_w_overlap, tile))\n",
    "    laz_without_ov = laspy.read(os.path.join(src_folder_tiles_wo_overlap, tile))\n",
    "    laz_flatten_with_ov = laspy.read(os.path.join(src_folder_flatten_tiles_w_overlap, tile.split('.laz')[0] + \"_flatten.laz\"))\n",
    "\n",
    "    mask = np.isin(laz_with_ov.id_point, laz_without_ov.id_point)\n",
    "\n",
    "    laz_flatten_wo_ov = laz_flatten_with_ov[mask]\n",
    "    laz_flatten_wo_ov.write(os.path.join(src_folder_flatten_tiles_w_overlap, tile.split('.laz')[0] + \"_flatten_no_ov.laz\"))\n",
    "    list_flatten_to_merge.append(os.path.join(src_folder_flatten_tiles_w_overlap, tile.split('.laz')[0] + \"_flatten_no_ov.laz\"))\n",
    "\n",
    "# Merging\n",
    "print(\"Merging all flatten tiles together (might take a few minutes)\")\n",
    "src_flatten_file = os.path.join(src_original.split('.laz')[0] + \"_flatten.laz\")\n",
    "merge_laz(list_flatten_to_merge, src_flatten_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad12268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of stripes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158/158 [03:19<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate stripes from the merged flatten\n",
    "src_folder_stripes = os.path.join(os.path.dirname(src_flatten_file), os.path.basename(src_flatten_file).split('.laz')[0] + f\"_stripes_{stripe_width}_m\")\n",
    "x_span = laz_original.x.max() - laz_original.x.min()\n",
    "y_span = laz_original.y.max() - laz_original.y.min()\n",
    "dims = [stripe_width, y_span]\n",
    "\n",
    "print(\"Creation of stripes:\")\n",
    "stripes_file(src_flatten_file, src_folder_stripes, dims, do_keep_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bcca8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Preprocess done in 1:6:29 ====\n"
     ]
    }
   ],
   "source": [
    "delta_time_loop = time() - start_preprocess_time\n",
    "hours = int(delta_time_loop // 3600)\n",
    "min = int((delta_time_loop - 3600 * hours) // 60)\n",
    "sec = int(delta_time_loop - 3600 * hours - 60 * min)\n",
    "print(f\"==== Preprocess done in {hours}:{min}:{sec} ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e802b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLICK ON \"Execute above cells\" ON THIS CELL TO RUN THE PRE-PROCESS. OVER THERE (triangle with top arrow) --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba4da0",
   "metadata": {},
   "source": [
    "### Manual cleaning\n",
    "Once the flattening step done, you will find the resulting stripes in the folder <>.\n",
    "\n",
    "Please clean them and save them with the same name in the folder <> alredy created.\n",
    "\n",
    "To be aware of regarding the cleaning:\n",
    " - a\n",
    " - b\n",
    " - c\n",
    "\n",
    "Afterward, you can run the post-processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb6d9b",
   "metadata": {},
   "source": [
    "### Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63e3b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_clean_stripes = None\n",
    "\n",
    "if src_clean_stripes == None:\n",
    "    src_clean_stripes = src_folder_stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb7c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all flatten tiles together (might take a few minutes)\n",
      "D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\functional\\Barmasse_2025_AllScans_Raw_Sub2cm_flatten_stripes_10_m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:06<00:00, 17.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\functional\\Barmasse_2025_AllScans_Raw_Sub2cm_flatten_clean.laz\n"
     ]
    }
   ],
   "source": [
    "# Merging\n",
    "print(\"Merging all flatten tiles together (might take a few minutes)\")\n",
    "src_flatten_clean_file = os.path.join(src_original.split('.laz')[0] + \"_flatten_clean.laz\")\n",
    "print(src_clean_stripes)\n",
    "list_clean_flatten_to_merge = [os.path.join(src_clean_stripes, x) for x in os.listdir(src_clean_stripes) if x.endswith('laz')]\n",
    "\n",
    "merge_laz(list_clean_flatten_to_merge, src_flatten_clean_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d54e1235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85660348\n",
      "101685240\n",
      "(101685240,)\n",
      "83926944\n"
     ]
    }
   ],
   "source": [
    "# remapping to original\n",
    "# src = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test_flatten_clean.laz\"\n",
    "laz_flatten_clean = laspy.read(src_flatten_clean_file)\n",
    "# print(src_flatten_file)\n",
    "# print(list(laz_flatten_clean.point_format.dimension_names))\n",
    "mask = np.isin(laz_original.id_point, laz_flatten_clean.id_point)\n",
    "print(len(laz_flatten_clean))\n",
    "print(len(laz_original))\n",
    "print(mask.shape)\n",
    "print(np.sum(mask))\n",
    "src_final_result = src_original.split('.laz')[0] + \"_FINAL.laz\"\n",
    "laz_original[mask].write(src_final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5819fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76849043\n",
      "76849043\n"
     ]
    }
   ],
   "source": [
    "las = laspy.read(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test\\Barmasse_2023_AllScans_Raw_Georef_with_id_tiles_overlap\\Barmasse_2023_AllScans_Raw_Georef_with_id_tile_4.laz\")\n",
    "print(len(las))\n",
    "print(len(set(las.id_point)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d098a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "from pyproj import CRS\n",
    "from laspy.vlrs.known import WktCoordinateSystemVlr\n",
    "\n",
    "\n",
    "def ensure_crs_in_las(src_input, epsg=2056):\n",
    "    las = laspy.read(src_input)\n",
    "    if las.header.parse_crs() is None:\n",
    "        crs = CRS.from_epsg(epsg)\n",
    "        wkt = crs.to_wkt()\n",
    "        vlr = WktCoordinateSystemVlr(wkt)\n",
    "        las.header.vlrs.append(vlr)\n",
    "        tmp_path = src_input.replace(\".laz\", \"_crs.laz\")\n",
    "        las.write(tmp_path)\n",
    "        print(f\"✅ Added CRS {epsg} to {src_input}\")\n",
    "        return tmp_path\n",
    "    else:\n",
    "        return src_input\n",
    "\n",
    "\n",
    "def tilling_slow(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "    \"\"\"\n",
    "    Tile the input LiDAR file into square tiles using PDAL and store them in the destination folder.\n",
    "\n",
    "    Args:\n",
    "        - verbose (bool): Whether to print verbose status updates.\n",
    "\n",
    "    Returns:\n",
    "        - None: Splits the input file into tiles and saves them in the destination folder.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Start tilling (with overlap = {overlap}m)...\")\n",
    "    os.makedirs(src_target, exist_ok=True)\n",
    "    \n",
    "    src_input = ensure_crs_in_las(src_input, epsg=2056)\n",
    "    \n",
    "    # compute the estimate number of tiles\n",
    "    if verbose:\n",
    "        print(\"Computing the estimated number of tiles...\")\n",
    "    original_file = laspy.read(src_input)\n",
    "    x_min = original_file.x.min()\n",
    "    x_max = original_file.x.max()\n",
    "    y_min = original_file.y.min()\n",
    "    y_max = original_file.y.max() \n",
    "    if verbose:\n",
    "        print('Done!')\n",
    "\n",
    "\n",
    "    \n",
    "    # output_pattern = \"tile_{i}_{j}.laz\"\n",
    "    output_pattern = os.path.join(\n",
    "        src_target, \n",
    "        os.path.basename(src_input).split('.')[0] + \"_tile_{i}_{j}.laz\",\n",
    "        )\n",
    "\n",
    "    x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "    y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "    combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "    for _, (i,j) in tqdm(enumerate(combinations), total=len(combinations)):\n",
    "        x0 = x_min + i * tile_size - overlap\n",
    "        x1 = x_min + (i + 1) * tile_size + overlap\n",
    "        y0 = y_min + j * tile_size - overlap\n",
    "        y1 = y_min + (j + 1) * tile_size + overlap\n",
    "\n",
    "        bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "        pipeline_json = {\n",
    "            \"pipeline\": [\n",
    "                # src_input,\n",
    "                {\n",
    "                    \"type\": \"readers.las\",\n",
    "                    \"filename\": src_input,\n",
    "                    \"extra_dims\": \"id_point=uint32\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"filters.reprojection\",\n",
    "                    \"in_srs\": \"EPSG:2056\",\n",
    "                    \"out_srs\": \"EPSG:2056\"\n",
    "                },\n",
    "                {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "                {\n",
    "                    \"type\": \"writers.las\",\n",
    "                    \"filename\": output_pattern.format(i=i, j=j),\n",
    "                    \"extra_dims\": \"id_point=uint32\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "        pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e27b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_las_files_inmemory(list_files, output_file):\n",
    "#     \"\"\"\n",
    "#     Merge multiple LAS/LAZ files into one using laspy.\n",
    "#     Preserves all dimensions including extra_dims like 'id_point'.\n",
    "#     \"\"\"\n",
    "#     # Read the header from the first file\n",
    "#     first_las = laspy.read(list_files[0])\n",
    "#     header = laspy.LasHeader(point_format=first_las.header.point_format,\n",
    "#                               version=first_las.header.version)\n",
    "\n",
    "#     all_arrays = []\n",
    "#     for f in list_files:\n",
    "#         las = laspy.read(f)\n",
    "#         all_arrays.append(las.points.array)  # extract structured array\n",
    "#         print(f\"Read {f} ({len(las)} pts)\")\n",
    "\n",
    "#     # Concatenate structured arrays\n",
    "#     merged_array = np.concatenate(all_arrays)\n",
    "\n",
    "#     # Create new LasData with header\n",
    "#     out = laspy.LasData(header)\n",
    "\n",
    "#     # Wrap the concatenated array as ScaleAwarePointRecord and assign\n",
    "#     out.points = laspy.ScaleAwarePointRecord(\n",
    "#         merged_array,\n",
    "#         point_format=header.point_format,\n",
    "#         scales=header.scales,\n",
    "#         offsets=header.offsets\n",
    "#     )\n",
    "\n",
    "#     # Save\n",
    "#     out.write(output_file)\n",
    "#     print(f\"Merged file saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afa3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_las_files(list_files, output_path, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Merge multiple LAS/LAZ files into a single output file using laspy.\n",
    "\n",
    "#     Args:\n",
    "#         list_files (list[str]): Paths to input LAS/LAZ files.\n",
    "#         output_path (str): Path to output merged LAS/LAZ file.\n",
    "#         verbose (bool): Whether to print progress info.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     if len(list_files) == 0:\n",
    "#         raise ValueError(\"No input files provided for merging.\")\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"🧩 Merging {len(list_files)} LAS/LAZ files...\")\n",
    "\n",
    "#     # --- Read the first file to set the reference header ---\n",
    "#     ref_las = laspy.read(list_files[0])\n",
    "#     header = laspy.LasHeader(point_format=ref_las.header.point_format, version=ref_las.header.version)\n",
    "#     header.offsets = ref_las.header.offsets\n",
    "#     header.scales = ref_las.header.scales\n",
    "\n",
    "#     # Copy CRS if available\n",
    "#     if hasattr(ref_las.header, \"epsg\") and ref_las.header.epsg is not None:\n",
    "#         header.epsg = ref_las.header.epsg\n",
    "\n",
    "#     merged = laspy.LasData(header)\n",
    "#     all_points = []\n",
    "\n",
    "#     # --- Collect points from each file ---\n",
    "#     for f in list_files:\n",
    "#         las = laspy.read(f)\n",
    "#         if verbose:\n",
    "#             print(f\"   → Reading {f} ({len(las)} points)\")\n",
    "\n",
    "#         # Ensure same point format\n",
    "#         if las.header.point_format != ref_las.header.point_format:\n",
    "#             raise ValueError(f\"File {f} has different point format.\")\n",
    "\n",
    "#         # Convert to numpy structured array for fast stacking\n",
    "#         all_points.append(las.points.array)\n",
    "\n",
    "#     # --- Concatenate all points ---\n",
    "#     merged.points = laspy.ScaleAwarePointRecord.merge(all_points, header=header)\n",
    "\n",
    "#     # --- Save merged file ---\n",
    "#     merged.write(output_path)\n",
    "#     if verbose:\n",
    "#         print(f\"✅ Merged file saved to {output_path} ({len(merged)} points)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import laspy\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# def merge_las_files_inmemory(list_files, output_path, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Merge multiple LAS/LAZ files into one using laspy, preserving extra dimensions and CRS.\n",
    "#     \"\"\"\n",
    "#     if len(list_files) == 0:\n",
    "#         raise ValueError(\"No input files provided for merging.\")\n",
    "\n",
    "#     # --- Read reference file ---\n",
    "#     ref = laspy.read(list_files[0])\n",
    "#     header = laspy.LasHeader(point_format=ref.header.point_format, version=ref.header.version)\n",
    "#     header.scales = ref.header.scales\n",
    "#     header.offsets = ref.header.offsets\n",
    "\n",
    "#     # Copy CRS/VLRs if available\n",
    "#     header.vlrs = list(ref.header.vlrs)\n",
    "#     try:\n",
    "#         crs = ref.header.parse_crs()\n",
    "#         if crs is not None:\n",
    "#             header.parse_crs(crs)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     # --- Prepare dimension names ---\n",
    "#     dim_names = ref.point_format.dimension_names\n",
    "#     if verbose:\n",
    "#         print(f\"Detected dimensions: {dim_names}\")\n",
    "\n",
    "#     # --- Concatenate all points ---\n",
    "#     all_points = []\n",
    "#     total_points = 0\n",
    "#     for f in list_files:\n",
    "#         las = laspy.read(f)\n",
    "#         if las.point_format != ref.point_format:\n",
    "#             raise ValueError(f\"Point format mismatch in {f}\")\n",
    "#         all_points.append(las.points)\n",
    "#         total_points += len(las)\n",
    "#         if verbose:\n",
    "#             print(f\"Read {f} ({len(las)} pts)\")\n",
    "\n",
    "#     # Extract structured arrays and concatenate\n",
    "#     arrays_to_merge = [p.array for p in all_points]\n",
    "#     merged_array = np.concatenate(arrays_to_merge)\n",
    "\n",
    "#     # Wrap back as ScaleAwarePointRecord\n",
    "#     # merged_points = laspy.ScaleAwarePointRecord(merged_array, header=header)\n",
    "\n",
    "#     # Assign to LasData\n",
    "#     out = laspy.LasData(header)\n",
    "#     out.points = merged_array\n",
    "\n",
    "#     # # Stack ScaleAwarePointRecords\n",
    "#     # merged_points = np.concatenate(all_points)\n",
    "\n",
    "#     # # --- Create LasData and assign points ---\n",
    "#     # out = laspy.LasData(header)\n",
    "#     # out.points = laspy.ScaleAwarePointRecord(merged_points, header=header)\n",
    "\n",
    "#     # --- Write output ---\n",
    "#     out.write(output_path)\n",
    "#     if verbose:\n",
    "#         print(f\"✅ Merged {len(list_files)} files ({total_points} pts) -> {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd496ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import traceback\n",
    "# import laspy\n",
    "# import os\n",
    "# try:\n",
    "#     print('Running Merge LAS')\n",
    "\n",
    "#     #This is the las file to append to.  DO NOT STORE THIS FILE IN THE SAME DIRECTORY AS BELOW...\n",
    "#     out_las = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\functional\\test_merge\\merged_file.laz \"  \n",
    "#     #this is a directory of las files\n",
    "#     inDir = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\functional\\test_merge\\source\"  \n",
    "\n",
    "#     def append_to_las(in_laz, out_las):\n",
    "#         with laspy.open(out_las, mode='a') as outlas:\n",
    "#             with laspy.open(in_las) as inlas:\n",
    "#                 for points in inlas.chunk_iterator(2_000_000):\n",
    "#                     outlas.append_points(points)\n",
    "\n",
    "#     # print(list(os.listdir(inDir)))\n",
    "#     for (dirpath, dirnames, filenames) in os.walk(inDir):\n",
    "#         # print(filenames)\n",
    "#         for inFile in filenames:\n",
    "#             if inFile.endswith('.laz'):\n",
    "#                 in_las = os.path.join(dirpath, inFile)\n",
    "#                 print(in_las)\n",
    "#                 append_to_las(in_las, out_las)\n",
    "        \n",
    "#     print('Finished without errors - merge_LAS.py')\n",
    "# except:\n",
    "#     tb = sys.exc_info()[2]\n",
    "#     tbinfo = traceback.format_tb(tb)[0]\n",
    "#     print('Error in append las')\n",
    "#     print (\"PYTHON ERRORS:\\nTraceback info:\\n\" + tbinfo + \"\\nError     Info:\\n\" + str(sys.exc_info()[1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import laspy\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# def crop_laspy(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Crops a LAS/LAZ file into tiles using laspy directly (preserves id_point perfectly).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "#     las = laspy.read(src_input)\n",
    "#     x_min, x_max = las.x.min(), las.x.max()\n",
    "#     y_min, y_max = las.y.min(), las.y.max()\n",
    "\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "\n",
    "#     for ix in range(x_steps):\n",
    "#         for iy in range(y_steps):\n",
    "#             x0 = x_min + ix * tile_size - overlap\n",
    "#             x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "#             y0 = y_min + iy * tile_size - overlap\n",
    "#             y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "#             mask = (\n",
    "#                 (las.x >= x0) & (las.x <= x1) &\n",
    "#                 (las.y >= y0) & (las.y <= y1)\n",
    "#             )\n",
    "#             if not np.any(mask):\n",
    "#                 continue\n",
    "\n",
    "#             tile = laspy.create(point_format=las.header.point_format, file_version=las.header.version)\n",
    "#             tile.points = las.points[mask]\n",
    "\n",
    "#             tile_filename = os.path.join(\n",
    "#                 src_target,\n",
    "#                 f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "#             )\n",
    "#             tile.write(tile_filename)\n",
    "#             if verbose:\n",
    "#                 print(f\"✅ Wrote {tile_filename} ({mask.sum()} points)\")\n",
    "\n",
    "\n",
    "\n",
    "# def crop_laspy(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Crops a LAS/LAZ file into tiles using laspy directly (preserves id_point and coordinates).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "#     las = laspy.read(src_input)\n",
    "#     x_min, x_max = las.x.min(), las.x.max()\n",
    "#     y_min, y_max = las.y.min(), las.y.max()\n",
    "\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "\n",
    "#     for ix in range(x_steps):\n",
    "#         for iy in range(y_steps):\n",
    "#             x0 = x_min + ix * tile_size - overlap\n",
    "#             x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "#             y0 = y_min + iy * tile_size - overlap\n",
    "#             y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "#             mask = (\n",
    "#                 (las.x >= x0) & (las.x <= x1) &\n",
    "#                 (las.y >= y0) & (las.y <= y1)\n",
    "#             )\n",
    "#             if not np.any(mask):\n",
    "#                 continue\n",
    "\n",
    "#             # ✅ Create new file with proper header scale/offset\n",
    "#             header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "#             header.offsets = las.header.offsets\n",
    "#             header.scales = las.header.scales\n",
    "\n",
    "#             # ✅ Copy CRS if any\n",
    "#             if hasattr(las.header, \"epsg\") and las.header.epsg is not None:\n",
    "#                 header.epsg = las.header.epsg\n",
    "\n",
    "#             tile = laspy.LasData(header)\n",
    "#             tile.points = las.points[mask]\n",
    "\n",
    "#             tile_filename = os.path.join(\n",
    "#                 src_target,\n",
    "#                 f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "#             )\n",
    "#             tile.write(tile_filename)\n",
    "\n",
    "#             if verbose:\n",
    "#                 print(f\"✅ Wrote {tile_filename} ({mask.sum()} points)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "263eab1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test2\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tiles_overlap\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tile_0_0.laz (11986946 points)\n",
      "✅ Wrote D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test2\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tiles_overlap\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tile_0_1.laz (5450402 points)\n",
      "✅ Wrote D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test2\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tiles_overlap\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tile_1_0.laz (57045997 points)\n",
      "✅ Wrote D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test2\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tiles_overlap\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tile_1_1.laz (8776736 points)\n",
      "✅ Wrote D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test2\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tiles_overlap\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tile_2_0.laz (18356127 points)\n",
      "✅ Wrote D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test2\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tiles_overlap\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tile_2_1.laz (69099 points)\n",
      "✅ Wrote D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2023\\test2\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tiles_overlap\\Barmasse_2025_AllScans_Raw_Sub2cm_with_id_tile_3_0.laz (16 points)\n"
     ]
    }
   ],
   "source": [
    "# tilling_slow(src_with_id, src_folder_tiles_w_overlap, tile_size, overlap)\n",
    "crop_laspy(src_with_id, src_folder_tiles_w_overlap, tile_size, overlap=0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b1f41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # utils\n",
    "# def tilling(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Tile the input LiDAR file into square tiles using PDAL and store them in the destination folder.\n",
    "\n",
    "#     Args:\n",
    "#         - verbose (bool): Whether to print verbose status updates.\n",
    "\n",
    "#     Returns:\n",
    "#         - None: Splits the input file into tiles and saves them in the destination folder.\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"Start tilling (with overlap = {overlap}m)...\")\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "    \n",
    "#     # compute the estimate number of tiles\n",
    "#     if verbose:\n",
    "#         print(\"Computing the estimated number of tiles...\")\n",
    "#     original_file = laspy.read(src_input)\n",
    "#     x_min = original_file.x.min()\n",
    "#     x_max = original_file.x.max()\n",
    "#     y_min = original_file.y.min()\n",
    "#     y_max = original_file.y.max() \n",
    "#     if verbose:\n",
    "#         print('Done!')\n",
    "\n",
    "\n",
    "    \n",
    "#     # output_pattern = \"tile_{i}_{j}.laz\"\n",
    "#     output_pattern = os.path.join(\n",
    "#         src_target, \n",
    "#         os.path.basename(src_input).split('.')[0] + \"_tile_{i}_{j}.laz\",\n",
    "#         )\n",
    "\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "#     combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "#     for _, (i,j) in tqdm(enumerate(combinations), total=len(combinations)):\n",
    "#         x0 = x_min + i * tile_size - overlap\n",
    "#         x1 = x_min + (i + 1) * tile_size + overlap\n",
    "#         y0 = y_min + j * tile_size - overlap\n",
    "#         y1 = y_min + (j + 1) * tile_size + overlap\n",
    "\n",
    "#         bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "#         pipeline_json = {\n",
    "#             \"pipeline\": [\n",
    "#                 # src_input,\n",
    "#                 {\n",
    "#                     \"type\": \"readers.las\",\n",
    "#                     \"filename\": src_input,\n",
    "#                     \"extra_dims\": \"id_point=uint32\"\n",
    "#                 },\n",
    "#                 {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "#                 {\n",
    "#                     \"type\": \"writers.las\",\n",
    "#                     \"filename\": output_pattern.format(i=i, j=j),\n",
    "#                     \"extra_dims\": \"id_point=uint32\"\n",
    "#                 }\n",
    "#             ]\n",
    "#         }\n",
    "\n",
    "#         pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#         pipeline.execute()\n",
    "\n",
    "\n",
    "# def stripes_file(src_input_file, src_output, dims, do_keep_existant=False):\n",
    "#     [tile_size_x, tile_size_y] = dims\n",
    "#     laz = laspy.read(src_input_file)\n",
    "#     # output_folder = os.path.join(src_output, f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripes_{tile_size_x}_{tile_size_y}\")\n",
    "#     os.makedirs(src_output, exist_ok=True)\n",
    "\n",
    "#     xmin, xmax, ymin, ymax = np.min(laz.x), np.max(laz.x), np.min(laz.y), np.max(laz.y)\n",
    "\n",
    "#     x_edges = np.arange(xmin, xmax, tile_size_x)\n",
    "#     y_edges = np.arange(ymin, ymax, tile_size_y)\n",
    "\n",
    "\n",
    "#     for i, x0 in enumerate(x_edges):\n",
    "#         print(f\"Column {i+1} / {len(x_edges)}:\")\n",
    "#         for j, y0 in tqdm(enumerate(y_edges), total=len(y_edges)):\n",
    "#             output = os.path.join(src_output,f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripe_{i}_{j}.laz\")\n",
    "#             if do_keep_existant and os.path.exists(output):\n",
    "#                 continue\n",
    "#             x1 = x0 + tile_size_x\n",
    "#             y1 = y0 + tile_size_y\n",
    "#             bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "#             pipeline_json = {\n",
    "#                 \"pipeline\": [\n",
    "#                     src_input_file,\n",
    "#                     {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "#                     {\"type\": \"writers.las\", \"filename\": output, 'extra_dims': 'all'}\n",
    "#                 ]\n",
    "#             }\n",
    "\n",
    "#             pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#             pipeline.execute()\n",
    "\n",
    "\n",
    "# def stripes_file(src_input_file, src_output, dims, do_keep_existant=False):\n",
    "#     [tile_size_x, tile_size_y] = dims\n",
    "#     laz = laspy.read(src_input_file)\n",
    "#     # output_folder = os.path.join(src_output, f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripes_{tile_size_x}_{tile_size_y}\")\n",
    "#     os.makedirs(src_output, exist_ok=True)\n",
    "\n",
    "#     xmin, xmax, ymin, ymax = np.min(laz.x), np.max(laz.x), np.min(laz.y), np.max(laz.y)\n",
    "\n",
    "#     x_edges = np.arange(xmin, xmax, tile_size_x)\n",
    "#     y_edges = np.arange(ymin, ymax, tile_size_y)\n",
    "\n",
    "\n",
    "#     for i, x0 in enumerate(x_edges):\n",
    "#         print(f\"Column {i+1} / {len(x_edges)}:\")\n",
    "#         for j, y0 in tqdm(enumerate(y_edges), total=len(y_edges)):\n",
    "#             output = os.path.join(src_output,f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripe_{i}_{j}.laz\")\n",
    "#             if do_keep_existant and os.path.exists(output):\n",
    "#                 continue\n",
    "#             x1 = x0 + tile_size_x\n",
    "#             y1 = y0 + tile_size_y\n",
    "#             bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "#             pipeline_json = {\n",
    "#                 \"pipeline\": [\n",
    "#                     src_input_file,\n",
    "#                     {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "#                     {\"type\": \"writers.las\", \"filename\": output, 'extra_dims': 'all'}\n",
    "#                 ]\n",
    "#             }\n",
    "\n",
    "#             pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#             pipeline.execute()\n",
    "\n",
    "\n",
    "\n",
    "# def remove_duplicates(laz_file, decimals=2):\n",
    "#     \"\"\"\n",
    "#     Removes duplicate points from a LAS/LAZ file based on rounded 3D coordinates.\n",
    "\n",
    "#     Args:\n",
    "#         - laz_file (laspy.LasData): Input LAS/LAZ file as a laspy object.\n",
    "#         - decimals (int, optional): Number of decimals to round the coordinates for duplicate detection. Defaults to 2.\n",
    "\n",
    "#     Returns:\n",
    "#         - laspy.LasData: A new laspy object with duplicate points removed.\n",
    "#     \"\"\"\n",
    "        \n",
    "#     coords = np.round(np.vstack((laz_file.x, laz_file.y, laz_file.z)).T, decimals)\n",
    "#     _, unique_indices = np.unique(coords, axis=0, return_index=True)\n",
    "#     mask = np.zeros(len(coords), dtype=bool)\n",
    "#     mask[unique_indices] = True\n",
    "\n",
    "#     # Create new LAS object\n",
    "#     header = laspy.LasHeader(point_format=laz_file.header.point_format, version=laz_file.header.version)\n",
    "#     new_las = laspy.LasData(header)\n",
    "\n",
    "#     for dim in laz_file.point_format.dimension_names:\n",
    "#         setattr(new_las, dim, getattr(laz_file, dim)[mask])\n",
    "\n",
    "#     return new_las\n",
    "\n",
    "# def match_pointclouds(laz1, laz2):\n",
    "#     \"\"\"Sort laz2 to match the order of laz1 without changing laz1's order.\n",
    "\n",
    "#     Args:\n",
    "#         laz1: laspy.LasData object (reference order)\n",
    "#         laz2: laspy.LasData object (to be sorted)\n",
    "    \n",
    "#     Returns:\n",
    "#         laz2 sorted to match laz1\n",
    "#     \"\"\"\n",
    "#     # Retrieve and round coordinates for robust matching\n",
    "#     coords_1 = np.round(np.vstack((laz1.x, laz1.y, laz1.z)), 2).T\n",
    "#     coords_2 = np.round(np.vstack((laz2.x, laz2.y, laz2.z)), 2).T\n",
    "\n",
    "#     # Verify laz2 is of the same size as laz1\n",
    "#     assert len(coords_2) == len(coords_1), \"laz2 should be a subset of laz1\"\n",
    "\n",
    "#     # Create a dictionary mapping from coordinates to indices\n",
    "#     coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(coords_1)}\n",
    "\n",
    "#     # Find indices in laz1 that correspond to laz2\n",
    "#     matching_indices = []\n",
    "#     failed = 0\n",
    "#     for coord in coords_2:\n",
    "#         try:\n",
    "#             matching_indices.append(coord_to_idx[tuple(coord)])\n",
    "#         except Exception as e:\n",
    "#             failed += 1\n",
    "\n",
    "#     matching_indices = np.array([coord_to_idx[tuple(coord)] for coord in coords_2])\n",
    "\n",
    "#     # Sort laz2 to match laz1\n",
    "#     sorted_indices = np.argsort(matching_indices)\n",
    "\n",
    "#     # Apply sorting to all attributes of laz2\n",
    "#     laz2.points = laz2.points[sorted_indices]\n",
    "\n",
    "#     return laz2  # Now sorted to match laz1\n",
    "\n",
    "\n",
    "# def flattening_tile(tile_src, tile_new_original_src, grid_size=10, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Flattens a tile by interpolating the ground surface and subtracting it from the original elevation.\n",
    "\n",
    "#     Args:\n",
    "#         - tile_src (str): Path to the input tile in LAS/LAZ format.\n",
    "#         - tile_new_original_src (str): Path to save the resized original tile after filtering.\n",
    "#         - grid_size (int, optional): Size of the grid in meters for local interpolation. Defaults to 10.\n",
    "#         - verbose (bool, optional): Whether to display progress and debug information. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         - None: Saves the floor and flattened versions of the tile and updates the original file.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Load file\n",
    "#     laz = laspy.read(tile_src)\n",
    "#     init_len = len(laz)\n",
    "#     if init_len == 0:\n",
    "#         return\n",
    "#     # laz = remove_duplicates(laz)\n",
    "#     if verbose:\n",
    "#         print(f\"Removing duplicates: From {init_len} to {len(laz)}\")\n",
    "#     # laz.write(tile_new_original_src)\n",
    "    \n",
    "#     points = np.vstack((laz.x, laz.y, laz.z)).T\n",
    "#     points_flatten = copy.deepcopy(points)\n",
    "#     points_interpolated = copy.deepcopy(points)\n",
    "\n",
    "#     # Divide into tiles and find local minimums\n",
    "#     x_min, y_min = np.min(points[:, :2], axis=0)\n",
    "#     x_max, y_max = np.max(points[:, :2], axis=0)\n",
    "\n",
    "#     x_bins = np.append(np.arange(x_min, x_max, grid_size), x_max)\n",
    "#     y_bins = np.append(np.arange(y_min, y_max, grid_size), y_max)\n",
    "\n",
    "#     grid = {i:{j:[] for j in range(y_bins.size - 1)} for i in range(x_bins.size -1)}\n",
    "#     for _, (px, py, pz) in tqdm(enumerate(points), total=len(points), desc=\"Creating grid\", disable=verbose==False):\n",
    "#         xbin = np.clip(0, (px - x_min) // grid_size, x_bins.size - 2)\n",
    "#         ybin = np.clip(0, (py - y_min) // grid_size, y_bins.size - 2)\n",
    "#         try:\n",
    "#             grid[xbin][ybin].append((px, py, pz))\n",
    "#         except Exception as e:\n",
    "#             print(xbin)\n",
    "#             print(ybin)\n",
    "#             print(x_bins)\n",
    "#             print(y_bins)\n",
    "#             print(grid.keys())\n",
    "#             print(grid[0].keys())\n",
    "#             raise e\n",
    "\n",
    "\n",
    "#     # Create grid_min\n",
    "#     grid_used = np.zeros((x_bins.size - 1, y_bins.size - 1))\n",
    "#     lst_grid_min = []\n",
    "#     lst_grid_min_pos = []\n",
    "#     for x in grid.keys():\n",
    "#         for y in grid[x].keys():\n",
    "#             if np.array(grid[x][y]).shape[0] > 0:\n",
    "#                 grid_used[x, y] = 1\n",
    "#                 lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "#                 arg_min = np.argmin(np.array(grid[x][y])[:,2])\n",
    "#                 lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2])\n",
    "\n",
    "#                 # test if border\n",
    "#                 if x == list(grid.keys())[0]:\n",
    "#                     lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "#                     lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] - [5, 0])\n",
    "#                 if x == list(grid.keys())[-1]:\n",
    "#                     lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "#                     lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] + [5, 0])\n",
    "#                 if y == list(grid[x].keys())[0]:\n",
    "#                     lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "#                     lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] - [0, 5])\n",
    "#                 if y == list(grid[x].keys())[-1]:\n",
    "#                     lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "#                     lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] + [0, 5])\n",
    "#             else:\n",
    "#                 grid_used[x, y] = 0\n",
    "#     arr_grid_min_pos = np.vstack(lst_grid_min_pos)\n",
    "#     if verbose:\n",
    "#         print(\"Resulting grid:\")\n",
    "#         print(arr_grid_min_pos.shape)\n",
    "#         print(grid_used)\n",
    "\n",
    "#     # Interpolate\n",
    "#     points_xy = np.array(points)[:,0:2]\n",
    "#     interpolated_min_z = scipy.interpolate.griddata(arr_grid_min_pos, np.array(lst_grid_min), points_xy, method=\"cubic\", fill_value=-1)\n",
    "\n",
    "#     # # Fill NaNs with nearest neighbor interpolation\n",
    "#     # nan_mask = np.isnan(interpolated_min_z)\n",
    "#     # x = np.array(points)[:,0]\n",
    "#     # y = np.array(points)[:,1]\n",
    "#     # z = np.array(points)[:,2]\n",
    "\n",
    "#     # if np.any(nan_mask):\n",
    "#     #     interpolated_min_z[nan_mask] = scipy.interpolate.griddata(arr_grid_min_pos, np.array(lst_grid_min), (x[nan_mask], y[nan_mask]), method='nearest')\n",
    "\n",
    "#     mask_valid = np.array([x != -1 for x in list(interpolated_min_z)])\n",
    "#     points_interpolated = points_interpolated[mask_valid]\n",
    "#     points_interpolated[:, 2] = interpolated_min_z[mask_valid]\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"Interpolation:\")\n",
    "#         print(f\"Original number of points: {points.shape[0]}\")\n",
    "#         print(f\"Interpollated number of points: {points_interpolated.shape[0]} ({int(points_interpolated.shape[0] / points.shape[0]*100)}%)\")\n",
    "\n",
    "#     # save floor\n",
    "#     filtered_points = {dim: getattr(laz, dim)[mask_valid] for dim in laz.point_format.dimension_names}\n",
    "#     header = laspy.LasHeader(point_format=laz.header.point_format, version=laz.header.version)\n",
    "#     new_las = laspy.LasData(header)\n",
    "\n",
    "#     #   _Assign filtered and modified data\n",
    "#     for dim, values in filtered_points.items():\n",
    "#         setattr(new_las, dim, values)\n",
    "#     setattr(new_las, 'x', points_interpolated[:,0])\n",
    "#     setattr(new_las, 'y', points_interpolated[:,1])\n",
    "#     setattr(new_las, 'z', points_interpolated[:,2])\n",
    "\n",
    "#     #   _Save new file\n",
    "#     # floor_dir = os.path.join(os.path.dirname(tile_src), 'floor')\n",
    "#     # os.makedirs(floor_dir, exist_ok=True)\n",
    "#     # new_las.write(os.path.join(floor_dir, os.path.basename(tile_src).split('.laz')[0] + f\"_floor_{grid_size}m.laz\"))\n",
    "#     # if verbose:\n",
    "#     #     print(\"Saved file: \", os.path.join(floor_dir, os.path.basename(tile_src).split('.laz')[0] + f\"_floor_{grid_size}m.laz\"))\n",
    "\n",
    "#     # Flatten\n",
    "#     points_flatten = points_flatten[mask_valid]\n",
    "#     points_flatten[:,2] = points_flatten[:,2] - points_interpolated[:,2]\n",
    "\n",
    "#     filtered_points = {dim: getattr(laz, dim)[mask_valid] for dim in laz.point_format.dimension_names}\n",
    "#     header = laspy.LasHeader(point_format=laz.header.point_format, version=laz.header.version)\n",
    "#     header.point_count = 0\n",
    "#     new_las = laspy.LasData(header)\n",
    "\n",
    "\n",
    "#     #   _Assign filtered and modified data\n",
    "#     for dim, values in filtered_points.items():\n",
    "#         setattr(new_las, dim, values)\n",
    "\n",
    "#     setattr(new_las, 'x', points_flatten[:,0])\n",
    "#     setattr(new_las, 'y', points_flatten[:,1])\n",
    "#     setattr(new_las, 'z', points_flatten[:,2])\n",
    "\n",
    "#     #   _Save new file\n",
    "#     # flatten_dir = os.path.join(os.path.dirname(tile_src), 'flatten')\n",
    "#     # os.makedirs(flatten_dir, exist_ok=True)\n",
    "#     # new_las.write(os.path.join(os.path.dirname(tile_src), os.path.basename(tile_src).split('.laz')[0] + f\"_flatten_{grid_size}m.laz\"))\n",
    "#     new_las.write(tile_new_original_src.split('.laz')[0] + \"_flatten.laz\")\n",
    "#     if verbose:\n",
    "#         print(\"Saved file: \", tile_new_original_src.split('.laz')[0] + \"_flatten.laz\")\n",
    "\n",
    "#     # Resize original file\n",
    "#     laz.points = laz.points[mask_valid]\n",
    "#     laz.write(tile_new_original_src)\n",
    "#     if verbose:\n",
    "#         print(\"Saved file: \", tile_new_original_src)\n",
    "\n",
    "\n",
    "# def flattening(src_tiles, src_new_tiles, grid_size=10, verbose=True, verbose_full=False):\n",
    "#     \"\"\"\n",
    "#     Applies the flattening process to all tiles in a directory using grid-based ground surface estimation.\n",
    "\n",
    "#     Args:\n",
    "#         - src_tiles (str): Path to the directory containing original tiles.\n",
    "#         - src_new_tiles (str): Path to the directory where resized tiles will be saved.\n",
    "#         - grid_size (int, optional): Size of the grid in meters for interpolation. Defaults to 10.\n",
    "#         - verbose (bool, optional): Whether to show a general progress bar. Defaults to True.\n",
    "#         - verbose_full (bool, optional): Whether to print detailed info per tile. Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#         - None: Processes and saves flattened tiles into their respective folders.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(\"Starting flattening:\")\n",
    "#     list_tiles = [x for x in os.listdir(src_tiles) if x.endswith('.laz')]\n",
    "#     for _, tile in tqdm(enumerate(list_tiles), total=len(list_tiles), desc=\"Processing\", disable=verbose==False):\n",
    "#         if verbose_full:\n",
    "#             print(\"Flattening tile: \", tile)\n",
    "#         flattening_tile(\n",
    "#             tile_src=os.path.join(src_tiles, tile), \n",
    "#             tile_new_original_src=os.path.join(src_new_tiles, tile),\n",
    "#             grid_size=grid_size,\n",
    "#             verbose=verbose_full,\n",
    "#             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc76dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tilling(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Tile the input LiDAR file into square tiles using PDAL and store them in the destination folder.\n",
    "\n",
    "#     Args:\n",
    "#         - verbose (bool): Whether to print verbose status updates.\n",
    "\n",
    "#     Returns:\n",
    "#         - None: Splits the input file into tiles and saves them in the destination folder.\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"Start tilling (with overlap = {overlap}m)...\")\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "#     # Compute the bounds from the input file\n",
    "#     if verbose:\n",
    "#         print(\"Computing the bounds...\")\n",
    "#     original_file = laspy.read(src_input)\n",
    "#     x_min = original_file.x.min()\n",
    "#     x_max = original_file.x.max()\n",
    "#     y_min = original_file.y.min()\n",
    "#     y_max = original_file.y.max()\n",
    "#     if verbose:\n",
    "#         print(f\"Done! Bounding box: x=[{x_min:.2f}, {x_max:.2f}], y=[{y_min:.2f}, {y_max:.2f}]\")\n",
    "\n",
    "#     # Compute tile steps\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "#     combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"Creating {len(combinations)} tiles...\")\n",
    "\n",
    "#     # Loop over each tile (avoid list of bounds)\n",
    "#     for i, (ix, iy) in enumerate(tqdm(combinations, total=len(combinations))):\n",
    "#         x0 = x_min + ix * tile_size - overlap\n",
    "#         x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "#         y0 = y_min + iy * tile_size - overlap\n",
    "#         y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "#         bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "\n",
    "#         tile_filename = os.path.join(\n",
    "#             src_target,\n",
    "#             f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "#         )\n",
    "\n",
    "#         pipeline_json = {\n",
    "#             \"pipeline\": [\n",
    "#                 {\n",
    "#                     \"type\": \"readers.las\",\n",
    "#                     \"filename\": src_input,\n",
    "#                     \"extra_dims\": \"id_point=uint32\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"filters.crop\",\n",
    "#                     \"bounds\": bounds\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"writers.las\",\n",
    "#                     \"filename\": tile_filename,\n",
    "#                     \"extra_dims\": \"id_point=uint32\"\n",
    "#                 }\n",
    "#             ]\n",
    "#         }\n",
    "\n",
    "#         pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#         pipeline.execute()\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"✅ All tiles successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import json\n",
    "# # import laspy\n",
    "# # import os\n",
    "# # import itertools\n",
    "# # import pdal\n",
    "\n",
    "# def tilling_batch(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Tile a LAS/LAZ file into multiple tiles in one PDAL pipeline execution,\n",
    "#     avoiding reloading the dataset multiple times.\n",
    "#     \"\"\"\n",
    "\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "#     original_file = laspy.read(src_input)\n",
    "#     x_min, x_max = original_file.x.min(), original_file.x.max()\n",
    "#     y_min, y_max = original_file.y.min(), original_file.y.max()\n",
    "\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "\n",
    "#     combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "\n",
    "#     # we’ll accumulate crop/writer pairs (no repeated readers)\n",
    "#     branches = []\n",
    "#     for ix, iy in combinations:\n",
    "#         x0 = x_min + ix * tile_size - overlap\n",
    "#         x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "#         y0 = y_min + iy * tile_size - overlap\n",
    "#         y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "#         bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "#         tile_filename = os.path.join(\n",
    "#             src_target,\n",
    "#             f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "#         )\n",
    "\n",
    "#         branches.extend([  # ✅ use extend to flatten directly\n",
    "#             {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "#             {\"type\": \"writers.las\", \"filename\": tile_filename, \"extra_dims\": \"id_point=uint32\"}\n",
    "#         ])\n",
    "\n",
    "#     # ✅ Only one reader at the top, then all crops/writers\n",
    "#     pipeline_json = {\n",
    "#         \"pipeline\": [\n",
    "#             {\"type\": \"readers.las\", \"filename\": src_input, \"extra_dims\": \"id_point=uint32\"},\n",
    "#             *branches  # expands list to flat sequence\n",
    "#         ]\n",
    "#     }\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"Running PDAL with {len(combinations)} tile branches...\")\n",
    "\n",
    "#     pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#     pipeline.execute()\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"✅ Tiling complete.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
