{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef69897",
   "metadata": {},
   "source": [
    "# Flatten tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285e6bf",
   "metadata": {},
   "source": [
    "# table of content\n",
    "1) [Splitting](#splitting)\n",
    "2) [Flattening of stripes](#flattening-of-stripes)\n",
    "3) [Manual cleaning](#manual-cleaning)\n",
    "4) [Post-process](#post-process)\n",
    "5) [Test shift](#test-shift)\n",
    "6) [Test duplicates](#test-duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54de91",
   "metadata": {},
   "source": [
    "### Dependencies and general utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e637d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "# import pdal\n",
    "import json\n",
    "import scipy\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a2fff",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3e447d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_original = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_without_id_point\\Barmasse_2025_AllScans_Raw_Sub2cm_aligned.laz\"\n",
    "tile_size = 100\n",
    "overlap = 20\n",
    "grid_size = 10\n",
    "stripe_width = 20\n",
    "shift=0\n",
    "method='quadric'\n",
    "do_save_floor=True\n",
    "do_keep_existing_flatten = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f43c04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_with_id = src_original.split('.laz')[0] + \"_with_id.laz\"\n",
    "src_folder_tiles_wo_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_tiles_no_overlap\")\n",
    "src_folder_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_tiles_overlap\")\n",
    "src_folder_flatten_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_flatten\")\n",
    "src_flatten_file = os.path.join(src_original.split('.laz')[0] + \"_flatten.laz\")\n",
    "src_folder_stripes_flatten = os.path.join(os.path.dirname(src_flatten_file), os.path.basename(src_flatten_file).split('.laz')[0] + f\"_stripes{stripe_width}m_flatten\")\n",
    "src_folder_stripes = os.path.join(os.path.dirname(src_flatten_file), os.path.basename(src_flatten_file).split('.laz')[0] + f\"_stripes_{stripe_width}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce01b04",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def tilling_pdal(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "    \"\"\"\n",
    "    Tile the input LiDAR file into square tiles using PDAL and store them in the destination folder.\n",
    "\n",
    "    Args:\n",
    "        - verbose (bool): Whether to print verbose status updates.\n",
    "\n",
    "    Returns:\n",
    "        - None: Splits the input file into tiles and saves them in the destination folder.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Start tilling (with overlap = {overlap}m)...\")\n",
    "    os.makedirs(src_target, exist_ok=True)\n",
    "    \n",
    "    # compute the estimate number of tiles\n",
    "    if verbose:\n",
    "        print(\"Computing the bounds...\")\n",
    "    original_file = laspy.read(src_input)\n",
    "    x_min = original_file.x.min()\n",
    "    x_max = original_file.x.max()\n",
    "    y_min = original_file.y.min()\n",
    "    y_max = original_file.y.max() \n",
    "    if verbose:\n",
    "        print('Done!')\n",
    "\n",
    "    output_pattern = os.path.join(\n",
    "        src_target, \n",
    "        os.path.basename(src_input).split('.')[0] + \"_tile_#.laz\",\n",
    "        )\n",
    "\n",
    "    x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "    y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "    combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "    list_bounds = []\n",
    "    for (i,j) in combinations:\n",
    "        x0 = x_min + i * tile_size - overlap\n",
    "        x1 = x_min + (i + 1) * tile_size + overlap\n",
    "        y0 = y_min + j * tile_size - overlap\n",
    "        y1 = y_min + (j + 1) * tile_size + overlap\n",
    "\n",
    "        bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "        list_bounds.append(bounds)\n",
    "\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": src_input,\n",
    "                \"extra_dims\": \"id_point=uint32\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.crop\", \n",
    "                \"bounds\": list_bounds\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.las\", \n",
    "                \"filename\": output_pattern, \n",
    "                'extra_dims': \"id_point=uint32\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Creation of the tiles (might take a few minutes)\")\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "    if verbose:\n",
    "        print(\"Process done\")\n",
    "\n",
    "\n",
    "def tilling(src_input, src_target, tile_size, overlap=0, shift=0, verbose=True):\n",
    "    \"\"\"\n",
    "    Crops a LAS/LAZ file into tiles using laspy directly (preserves id_point and coordinates).\n",
    "    \"\"\"\n",
    "    os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "    las = laspy.read(src_input)\n",
    "    x_min, x_max = las.x.min() - shift, las.x.max()\n",
    "    y_min, y_max = las.y.min() - shift, las.y.max()\n",
    "\n",
    "    x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "    y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "\n",
    "    combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "    \n",
    "    print(\"Creation of tiles:\")\n",
    "    for _, (ix, iy) in tqdm(enumerate(combinations), total=len(combinations)):\n",
    "    # for ix in range(x_steps):\n",
    "    #     for iy in range(y_steps):\n",
    "        x0 = x_min + ix * tile_size\n",
    "        x1 = x_min + (ix + 1) * tile_size\n",
    "        y0 = y_min + iy * tile_size\n",
    "        y1 = y_min + (iy + 1) * tile_size\n",
    "\n",
    "        mask_wo_overlap = (\n",
    "            (las.x >= x0) & (las.x <= x1) &\n",
    "            (las.y >= y0) & (las.y <= y1)\n",
    "        )\n",
    "        x0 -= overlap\n",
    "        y0 -= overlap\n",
    "        x1 += overlap\n",
    "        y1 += overlap\n",
    "\n",
    "        mask_with_overlap = (\n",
    "            (las.x >= x0) & (las.x <= x1) &\n",
    "            (las.y >= y0) & (las.y <= y1)\n",
    "        )\n",
    "        if not np.any(mask_wo_overlap):\n",
    "            continue\n",
    "\n",
    "        # ✅ Create new file with proper header scale/offset\n",
    "        header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "        header.offsets = las.header.offsets\n",
    "        header.scales = las.header.scales\n",
    "\n",
    "        # ✅ Copy CRS if any\n",
    "        if hasattr(las.header, \"epsg\") and las.header.epsg is not None:\n",
    "            header.epsg = las.header.epsg\n",
    "\n",
    "        tile = laspy.LasData(header)\n",
    "        tile.points = las.points[mask_with_overlap]\n",
    "\n",
    "        tile_filename = os.path.join(\n",
    "            src_target,\n",
    "            f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "        )\n",
    "        tile.write(tile_filename)\n",
    "\n",
    "\n",
    "def stripes_file(src_input_file, src_output, dims, do_keep_existing=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Crops a LAS/LAZ file into tiles using laspy directly (preserves id_point and coordinates).\n",
    "    \"\"\"\n",
    "    [tile_size_x, tile_size_y] = dims\n",
    "    las = laspy.read(src_input_file)\n",
    "    os.makedirs(src_output, exist_ok=True)\n",
    "\n",
    "    xmin, xmax, ymin, ymax = np.min(las.x), np.max(las.x), np.min(las.y), np.max(las.y)\n",
    "\n",
    "    x_edges = np.arange(xmin, xmax, tile_size_x)\n",
    "    y_edges = np.arange(ymin, ymax, tile_size_y)\n",
    "\n",
    "    combinations = list(itertools.product(x_edges, y_edges))\n",
    "    # for (x0, y0) in combinations:\n",
    "    num_skipped = 0\n",
    "    for id_stripe, (x0, y0) in tqdm(enumerate(combinations), total=len(combinations)):\n",
    "        x1 = x0 + tile_size_x\n",
    "        y1 = y0 + tile_size_y\n",
    "\n",
    "        mask = (\n",
    "            (las.x >= x0) & (las.x <= x1) &\n",
    "            (las.y >= y0) & (las.y <= y1)\n",
    "        )\n",
    "        if not np.any(mask):\n",
    "            num_skipped += 1\n",
    "            continue\n",
    "\n",
    "        # Create new file with proper header scale/offset\n",
    "        header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "        header.offsets = las.header.offsets\n",
    "        header.scales = las.header.scales\n",
    "\n",
    "        # Copy CRS if any\n",
    "        if hasattr(las.header, \"epsg\") and las.header.epsg is not None:\n",
    "            header.epsg = las.header.epsg\n",
    "\n",
    "        tile = laspy.LasData(header)\n",
    "        tile.points = las.points[mask]\n",
    "\n",
    "        tile_filename = os.path.join(\n",
    "            src_output,\n",
    "            f\"{os.path.splitext(os.path.basename(src_input_file))[0]}_stripe_{id_stripe - num_skipped}.laz\"\n",
    "        )\n",
    "        tile.write(tile_filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "    # las = laspy.read(src_input)\n",
    "    # x_min, x_max = las.x.min(), las.x.max()\n",
    "    # y_min, y_max = las.y.min(), las.y.max()\n",
    "\n",
    "    # x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "    # y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "\n",
    "    # combination = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "    \n",
    "    # print(\"Creation of tiles:\")\n",
    "    # for _, (ix, iy) in tqdm(enumerate(combination)):\n",
    "    # # for ix in range(x_steps):\n",
    "    # #     for iy in range(y_steps):\n",
    "    #     x0 = x_min + ix * tile_size - overlap\n",
    "    #     x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "    #     y0 = y_min + iy * tile_size - overlap\n",
    "    #     y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "    #     mask = (\n",
    "    #         (las.x >= x0) & (las.x <= x1) &\n",
    "    #         (las.y >= y0) & (las.y <= y1)\n",
    "    #     )\n",
    "    #     if not np.any(mask):\n",
    "    #         continue\n",
    "\n",
    "    #     # ✅ Create new file with proper header scale/offset\n",
    "    #     header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "    #     header.offsets = las.header.offsets\n",
    "    #     header.scales = las.header.scales\n",
    "\n",
    "    #     # ✅ Copy CRS if any\n",
    "    #     if hasattr(las.header, \"epsg\") and las.header.epsg is not None:\n",
    "    #         header.epsg = las.header.epsg\n",
    "\n",
    "    #     tile = laspy.LasData(header)\n",
    "    #     tile.points = las.points[mask]\n",
    "\n",
    "    #     tile_filename = os.path.join(\n",
    "    #         src_target,\n",
    "    #         f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "    #     )\n",
    "    #     tile.write(tile_filename)\n",
    "\n",
    "\n",
    "def stripes_file_pdal(src_input_file, src_output, dims, do_keep_existing=False, verbose=True):\n",
    "    [tile_size_x, tile_size_y] = dims\n",
    "    laz = laspy.read(src_input_file)\n",
    "    os.makedirs(src_output, exist_ok=True)\n",
    "\n",
    "    xmin, xmax, ymin, ymax = np.min(laz.x), np.max(laz.x), np.min(laz.y), np.max(laz.y)\n",
    "\n",
    "    x_edges = np.arange(xmin, xmax, tile_size_x)\n",
    "    y_edges = np.arange(ymin, ymax, tile_size_y)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computing the bounds...\")\n",
    "\n",
    "    list_bounds = []\n",
    "    combinations = list(itertools.product(x_edges, y_edges))\n",
    "    for (x0, y0) in combinations:\n",
    "        # for j, y0 in tqdm(enumerate(y_edges), total=len(y_edges)):\n",
    "        x1 = x0 + tile_size_x\n",
    "        y1 = y0 + tile_size_y\n",
    "        bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "        \n",
    "        list_bounds.append(bounds)\n",
    "\n",
    "    output_pattern = os.path.join(\n",
    "            src_output, \n",
    "            os.path.basename(src_input_file).split('.')[0] + \"_stripe_#.laz\",\n",
    "            )\n",
    "    \n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": src_input_file,\n",
    "                \"spatialreference\": \"EPSG:2056\",\n",
    "                \"extra_dims\": \"id_point=uint32\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.crop\", \n",
    "                \"bounds\": list_bounds,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.las\", \n",
    "                \"filename\": output_pattern, \n",
    "                'extra_dims': \"id_point=uint32\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    if verbose:\n",
    "        print(\"Creation of the stripes (might take a few minutes)\")\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "    if verbose:\n",
    "        print(\"Process done\")\n",
    "\n",
    "\n",
    "def remove_duplicates(laz_file, decimals=2):\n",
    "    \"\"\"\n",
    "    Removes duplicate points from a LAS/LAZ file based on rounded 3D coordinates.\n",
    "\n",
    "    Args:\n",
    "        - laz_file (laspy.LasData): Input LAS/LAZ file as a laspy object.\n",
    "        - decimals (int, optional): Number of decimals to round the coordinates for duplicate detection. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        - laspy.LasData: A new laspy object with duplicate points removed.\n",
    "    \"\"\"\n",
    "        \n",
    "    coords = np.round(np.vstack((laz_file.x, laz_file.y, laz_file.z)).T, decimals)\n",
    "    _, unique_indices = np.unique(coords, axis=0, return_index=True)\n",
    "    mask = np.zeros(len(coords), dtype=bool)\n",
    "    mask[unique_indices] = True\n",
    "\n",
    "    # Create new LAS object\n",
    "    header = laspy.LasHeader(point_format=laz_file.header.point_format, version=laz_file.header.version)\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "    for dim in laz_file.point_format.dimension_names:\n",
    "        setattr(new_las, dim, getattr(laz_file, dim)[mask])\n",
    "\n",
    "    return new_las\n",
    "\n",
    "\n",
    "def match_pointclouds(laz1, laz2):\n",
    "    \"\"\"Sort laz2 to match the order of laz1 without changing laz1's order.\n",
    "\n",
    "    Args:\n",
    "        laz1: laspy.LasData object (reference order)\n",
    "        laz2: laspy.LasData object (to be sorted)\n",
    "    \n",
    "    Returns:\n",
    "        laz2 sorted to match laz1\n",
    "    \"\"\"\n",
    "    # Retrieve and round coordinates for robust matching\n",
    "    coords_1 = np.round(np.vstack((laz1.x, laz1.y, laz1.z)), 2).T\n",
    "    coords_2 = np.round(np.vstack((laz2.x, laz2.y, laz2.z)), 2).T\n",
    "\n",
    "    # Verify laz2 is of the same size as laz1\n",
    "    assert len(coords_2) == len(coords_1), \"laz2 should be a subset of laz1\"\n",
    "\n",
    "    # Create a dictionary mapping from coordinates to indices\n",
    "    coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(coords_1)}\n",
    "\n",
    "    # Find indices in laz1 that correspond to laz2\n",
    "    matching_indices = []\n",
    "    failed = 0\n",
    "    for coord in coords_2:\n",
    "        try:\n",
    "            matching_indices.append(coord_to_idx[tuple(coord)])\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "\n",
    "    matching_indices = np.array([coord_to_idx[tuple(coord)] for coord in coords_2])\n",
    "\n",
    "    # Sort laz2 to match laz1\n",
    "    sorted_indices = np.argsort(matching_indices)\n",
    "\n",
    "    # Apply sorting to all attributes of laz2\n",
    "    laz2.points = laz2.points[sorted_indices]\n",
    "\n",
    "    return laz2  # Now sorted to match laz1\n",
    "\n",
    "\n",
    "def flattening_tile(tile_src, tile_new_original_src, grid_size=10, method='cubic', do_save_floor=False, do_keep_existing=False, do_extrapolate_outside_hull=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Flattens a tile by interpolating the ground surface and subtracting it from the original elevation.\n",
    "\n",
    "    Args:\n",
    "        - tile_src (str): Path to the input tile in LAS/LAZ format.\n",
    "        - tile_new_original_src (str): Path to save the resized original tile after filtering.\n",
    "        - grid_size (int, optional): Size of the grid in meters for local interpolation. Defaults to 10.\n",
    "        - verbose (bool, optional): Whether to display progress and debug information. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        - None: Saves the floor and flattened versions of the tile and updates the original file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(tile_new_original_src) and do_keep_existing:\n",
    "        if verbose:\n",
    "            print(f\"Skipping. {tile_new_original_src} exists already\")\n",
    "        return\n",
    "    \n",
    "    # Load file\n",
    "    laz = laspy.read(tile_src)\n",
    "    init_len = len(laz)\n",
    "    if init_len == 0:\n",
    "        return\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Removing duplicates: From {init_len} to {len(laz)}\")\n",
    "    \n",
    "    points = np.vstack((laz.x, laz.y, laz.z)).T\n",
    "    points_flatten = copy.deepcopy(points)\n",
    "    points_interpolated = copy.deepcopy(points)\n",
    "\n",
    "    # Divide into tiles and find local minimums\n",
    "    x_min, y_min = np.min(points[:, :2], axis=0)\n",
    "    x_max, y_max = np.max(points[:, :2], axis=0)\n",
    "\n",
    "    x_bins = np.append(np.arange(x_min, x_max, grid_size), x_max)\n",
    "    y_bins = np.append(np.arange(y_min, y_max, grid_size), y_max)\n",
    "\n",
    "    grid = {i:{j:[] for j in range(y_bins.size - 1)} for i in range(x_bins.size -1)}\n",
    "    for _, (px, py, pz) in tqdm(enumerate(points), total=len(points), desc=\"Creating grid\", disable=verbose==False):\n",
    "        xbin = np.clip(0, (px - x_min) // grid_size, x_bins.size - 2)\n",
    "        ybin = np.clip(0, (py - y_min) // grid_size, y_bins.size - 2)\n",
    "        try:\n",
    "            grid[xbin][ybin].append((px, py, pz))\n",
    "        except Exception as e:\n",
    "            print(\"Problem with: \", tile_src)\n",
    "            print(xbin)\n",
    "            print(ybin)\n",
    "            print(x_bins)\n",
    "            print(y_bins)\n",
    "            print(grid.keys())\n",
    "            print(grid[0].keys())\n",
    "            raise e\n",
    "\n",
    "\n",
    "    # Create grid_min\n",
    "    grid_used = np.zeros((x_bins.size - 1, y_bins.size - 1))\n",
    "    lst_grid_min = []\n",
    "    lst_grid_min_pos = []\n",
    "    for x in grid.keys():\n",
    "        for y in grid[x].keys():\n",
    "            if np.array(grid[x][y]).shape[0] > 0:\n",
    "                grid_used[x, y] = 1\n",
    "                lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                arg_min = np.argmin(np.array(grid[x][y])[:,2])\n",
    "                lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2])\n",
    "\n",
    "                # test if border\n",
    "                if x == list(grid.keys())[0]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] - [5, 0])\n",
    "                if x == list(grid.keys())[-1]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] + [5, 0])\n",
    "                if y == list(grid[x].keys())[0]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] - [0, 5])\n",
    "                if y == list(grid[x].keys())[-1]:\n",
    "                    lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "                    lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] + [0, 5])\n",
    "            else:\n",
    "                grid_used[x, y] = 0\n",
    "    arr_grid_min_pos = np.vstack(lst_grid_min_pos)\n",
    "    if verbose:\n",
    "        print(\"Resulting grid:\")\n",
    "        print(arr_grid_min_pos.shape)\n",
    "        print(grid_used)\n",
    "\n",
    "    # Interpolate\n",
    "    points_xy = np.array(points)[:,0:2]\n",
    "    if method == 'cubic':\n",
    "        interpolated_min_z = scipy.interpolate.griddata(arr_grid_min_pos, np.array(lst_grid_min), points_xy, method=\"cubic\", fill_value=-1)\n",
    "    elif method == 'quadric':\n",
    "        rbf = scipy.interpolate.Rbf(arr_grid_min_pos[:,0], arr_grid_min_pos[:,1], np.array(lst_grid_min), function='multiquadric', smooth=5)\n",
    "        interpolated_min_z = rbf(points_xy[:,0], points_xy[:,1])\n",
    "    else:\n",
    "        raise ValueError(\"Wrong argument for method!\")\n",
    "\n",
    "    # Fill NaNs with nearest neighbor interpolation\n",
    "    if do_extrapolate_outside_hull:\n",
    "        nan_mask = interpolated_min_z == -1\n",
    "        x = np.array(points)[:,0]\n",
    "        y = np.array(points)[:,1]\n",
    "\n",
    "        if np.any(nan_mask):\n",
    "            interpolated_min_z[nan_mask] = scipy.interpolate.griddata(arr_grid_min_pos, np.array(lst_grid_min), (x[nan_mask], y[nan_mask]), method='nearest')\n",
    "\n",
    "    mask_valid = np.array([x != -1 for x in list(interpolated_min_z)])\n",
    "    points_interpolated = points_interpolated[mask_valid]\n",
    "    points_interpolated[:, 2] = interpolated_min_z[mask_valid]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Interpolation:\")\n",
    "        print(f\"Original number of points: {points.shape[0]}\")\n",
    "        print(f\"Interpollated number of points: {points_interpolated.shape[0]} ({int(points_interpolated.shape[0] / points.shape[0]*100)}%)\")\n",
    "\n",
    "    # save floor\n",
    "    filtered_points = {dim: getattr(laz, dim)[mask_valid] for dim in laz.point_format.dimension_names}\n",
    "    header = laspy.LasHeader(point_format=laz.header.point_format, version=laz.header.version)\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "    #   _Assign filtered and modified data\n",
    "    for dim, values in filtered_points.items():\n",
    "        setattr(new_las, dim, values)\n",
    "    setattr(new_las, 'x', points_interpolated[:,0])\n",
    "    setattr(new_las, 'y', points_interpolated[:,1])\n",
    "    setattr(new_las, 'z', points_interpolated[:,2])\n",
    "\n",
    "    #   _Save new file\n",
    "    if do_save_floor:\n",
    "        new_las.write(tile_new_original_src.split('.laz')[0] + \"_floor.laz\")\n",
    "        if verbose:\n",
    "            print(\"Saved file: \", tile_new_original_src.split('.laz')[0] + \"_floor.laz\")\n",
    "\n",
    "    # Flatten\n",
    "    points_flatten = points_flatten[mask_valid]\n",
    "    points_flatten[:,2] = points_flatten[:,2] - points_interpolated[:,2]\n",
    "\n",
    "    filtered_points = {dim: getattr(laz, dim)[mask_valid] for dim in laz.point_format.dimension_names}\n",
    "    header = laspy.LasHeader(point_format=laz.header.point_format, version=laz.header.version)\n",
    "    header.point_count = 0\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "\n",
    "    #   _Assign filtered and modified data\n",
    "    for dim, values in filtered_points.items():\n",
    "        setattr(new_las, dim, values)\n",
    "\n",
    "    setattr(new_las, 'x', points_flatten[:,0])\n",
    "    setattr(new_las, 'y', points_flatten[:,1])\n",
    "    setattr(new_las, 'z', points_flatten[:,2])\n",
    "\n",
    "    #   _Save new file\n",
    "    new_las.write(tile_new_original_src.split('.laz')[0] + \"_flatten.laz\")\n",
    "    if verbose:\n",
    "        print(\"Saved file: \", tile_new_original_src.split('.laz')[0] + \"_flatten.laz\")\n",
    "\n",
    "    # Resize original file\n",
    "    laz.points = laz.points[mask_valid]\n",
    "    laz.write(tile_new_original_src)\n",
    "    if verbose:\n",
    "        print(\"Saved file: \", tile_new_original_src)\n",
    "\n",
    "\n",
    "def flattening(src_tiles, src_new_tiles, grid_size=10, verbose=True, method='cubic', do_save_floor=True, do_keep_existing=False, verbose_full=False):\n",
    "    \"\"\"\n",
    "    Applies the flattening process to all tiles in a directory using grid-based ground surface estimation.\n",
    "\n",
    "    Args:\n",
    "        - src_tiles (str): Path to the directory containing original tiles.\n",
    "        - src_new_tiles (str): Path to the directory where resized tiles will be saved.\n",
    "        - grid_size (int, optional): Size of the grid in meters for interpolation. Defaults to 10.\n",
    "        - verbose (bool, optional): Whether to show a general progress bar. Defaults to True.\n",
    "        - verbose_full (bool, optional): Whether to print detailed info per tile. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        - None: Processes and saves flattened tiles into their respective folders.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting flattening:\")\n",
    "    list_tiles = [x for x in os.listdir(src_tiles) if x.endswith('.laz')]\n",
    "    for _, tile in tqdm(enumerate(list_tiles), total=len(list_tiles), desc=\"Processing\", disable=verbose==False):\n",
    "        if verbose_full:\n",
    "            print(\"Flattening tile: \", tile)\n",
    "        if do_keep_existing and os.path.exists(os.path.join(src_new_tiles, tile).split('.laz')[0] + \"_flatten.laz\"):\n",
    "            continue\n",
    "\n",
    "        flattening_tile(\n",
    "            tile_src=os.path.join(src_tiles, tile), \n",
    "            tile_new_original_src=os.path.join(src_new_tiles, tile),\n",
    "            grid_size=grid_size,\n",
    "            method=method, \n",
    "            do_save_floor=do_save_floor,\n",
    "            do_keep_existing=do_keep_existing,\n",
    "            verbose=verbose_full,\n",
    "            )\n",
    "        \n",
    "\n",
    "def merge_laz(list_files, output_file):\n",
    "    \"\"\"\n",
    "    Merge multiple LAS/LAZ files into one using laspy.\n",
    "    Preserves all dimensions including extra_dims like 'id_point'.\n",
    "    \"\"\"\n",
    "    # Read the header from the first file\n",
    "    first_las = laspy.read(list_files[0])\n",
    "    header = laspy.LasHeader(point_format=first_las.header.point_format,\n",
    "                              version=first_las.header.version)\n",
    "\n",
    "    all_arrays = []\n",
    "    for _, f in tqdm(enumerate(list_files), total=len(list_files)):\n",
    "        las = laspy.read(f)\n",
    "        all_arrays.append(las.points.array)  # extract structured array\n",
    "        # print(f\"Read {f} ({len(las)} pts)\")\n",
    "\n",
    "    # Concatenate structured arrays\n",
    "    merged_array = np.concatenate(all_arrays)\n",
    "\n",
    "    # Create new LasData with header\n",
    "    out = laspy.LasData(header)\n",
    "\n",
    "    # Wrap the concatenated array as ScaleAwarePointRecord and assign\n",
    "    out.points = laspy.ScaleAwarePointRecord(\n",
    "        merged_array,\n",
    "        point_format=header.point_format,\n",
    "        scales=header.scales,\n",
    "        offsets=header.offsets\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    out.write(output_file)\n",
    "    print(f\"Merged file saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f34ef616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "start_preprocess_time = time()\n",
    "laz_original = laspy.read(src_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d2994d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding an id to the points\n",
    "# id_point = np.arange(len(laz_original))\n",
    "# laz_original.add_extra_dim(laspy.ExtraBytesParams('id_point', type=\"uint32\"))\n",
    "# laz_original.id_point = id_point\n",
    "\n",
    "# src_with_id = src_original.split('.laz')[0] + \"_with_id.laz\"\n",
    "# laz_original.write(src_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a82e2ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of tiles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:25<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# tiles without overlap\n",
    "src_folder_tiles_wo_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_tiles_no_overlap\")\n",
    "tilling(src_original, src_folder_tiles_wo_overlap, tile_size, overlap=0, shift=shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a49f8783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of tiles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:45<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# tiles with overlap\n",
    "src_folder_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_tiles_overlap\")\n",
    "tilling(src_original, src_folder_tiles_w_overlap, tile_size, overlap=overlap, shift=shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbc474",
   "metadata": {},
   "source": [
    "### Flattening of stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f8d9a17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting flattening:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████  | 63/78 [43:41<10:24, 41.61s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem with:  D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_without_id_point\\Barmasse_2025_AllScans_Raw_Sub2cm_aligned_with_id_tiles_overlap\\Barmasse_2025_AllScans_Raw_Sub2cm_aligned_tile_8_13.laz\n",
      "-1.0\n",
      "-1.0\n",
      "[2587548.1609]\n",
      "[1099410.0897]\n",
      "dict_keys([])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 386\u001b[0m, in \u001b[0;36mflattening_tile\u001b[1;34m(tile_src, tile_new_original_src, grid_size, method, do_save_floor, do_keep_existing, do_extrapolate_outside_hull, verbose)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[43mgrid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxbin\u001b[49m\u001b[43m]\u001b[49m[ybin]\u001b[38;5;241m.\u001b[39mappend((px, py, pz))\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: -1.0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m src_folder_flatten_tiles_w_overlap \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(src_with_id), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src_with_id)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.laz\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flatten\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(src_folder_flatten_tiles_w_overlap, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mflattening\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_tiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_folder_tiles_w_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_new_tiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_folder_flatten_tiles_w_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_save_floor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_save_floor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_keep_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_keep_existing_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_full\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[84], line 530\u001b[0m, in \u001b[0;36mflattening\u001b[1;34m(src_tiles, src_new_tiles, grid_size, verbose, method, do_save_floor, do_keep_existing, verbose_full)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_keep_existing \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(src_new_tiles, tile)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.laz\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flatten.laz\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m \u001b[43mflattening_tile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtile_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtile_new_original_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_new_tiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_save_floor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_save_floor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_keep_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_keep_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[84], line 394\u001b[0m, in \u001b[0;36mflattening_tile\u001b[1;34m(tile_src, tile_new_original_src, grid_size, method, do_save_floor, do_keep_existing, do_extrapolate_outside_hull, verbose)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28mprint\u001b[39m(y_bins)\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mprint\u001b[39m(grid\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m--> 394\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mgrid\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# Create grid_min\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Flattening of tiles with overlap\n",
    "src_folder_flatten_tiles_w_overlap = os.path.join(os.path.dirname(src_with_id), os.path.basename(src_with_id).split('.laz')[0] + \"_flatten\")\n",
    "os.makedirs(src_folder_flatten_tiles_w_overlap, exist_ok=True)\n",
    "flattening(\n",
    "    src_tiles=src_folder_tiles_w_overlap,\n",
    "    src_new_tiles=src_folder_flatten_tiles_w_overlap,\n",
    "    grid_size=grid_size,\n",
    "    method=method,\n",
    "    do_save_floor=do_save_floor,\n",
    "    do_keep_existing=do_keep_existing_flatten,\n",
    "    verbose=True,\n",
    "    verbose_full=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b55b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating flaten tiles without overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [01:00<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all flatten tiles together (might take a few minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:07<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_flatten.laz\n"
     ]
    }
   ],
   "source": [
    "# Creating flatten tiles w/o overlap and merging\n",
    "def hash_coords(x, y, z, rounding=2):\n",
    "    \"\"\"Create a unique integer hash for each rounded coordinate triple.\"\"\"\n",
    "    return np.round(x, rounding) * 1e12 + np.round(y, rounding) * 1e6 + np.round(z, rounding)\n",
    "\n",
    "list_flatten_to_merge = []\n",
    "print(\"Creating flaten tiles without overlap\")\n",
    "# list_tiles = [x for x in os.listdir(src_folder_flatten_tiles_w_overlap) if not x.endswith('_flatten.laz')]\n",
    "list_tiles = [x for x in os.listdir(src_folder_tiles_wo_overlap) if x.endswith('.laz')]\n",
    "for _, tile in tqdm(enumerate(list_tiles), total=len(list_tiles)):\n",
    "    # print(tile)\n",
    "    assert os.path.exists(os.path.join(src_folder_tiles_wo_overlap, tile))\n",
    "    try:\n",
    "        laz_with_ov = laspy.read(os.path.join(src_folder_flatten_tiles_w_overlap, tile))\n",
    "    except:\n",
    "        print(\"Error with : \", os.path.join(src_folder_flatten_tiles_w_overlap, tile))\n",
    "        continue\n",
    "    laz_without_ov = laspy.read(os.path.join(src_folder_tiles_wo_overlap, tile))\n",
    "    laz_flatten_with_ov = laspy.read(os.path.join(src_folder_flatten_tiles_w_overlap, tile.split('.laz')[0] + \"_flatten.laz\"))\n",
    "\n",
    "    mask = np.isin(laz_with_ov.id_point, laz_without_ov.id_point)\n",
    "\n",
    "    laz_flatten_wo_ov = laz_flatten_with_ov[mask]\n",
    "    laz_flatten_wo_ov.write(os.path.join(src_folder_flatten_tiles_w_overlap, tile.split('.laz')[0] + \"_flatten_no_ov.laz\"))\n",
    "    list_flatten_to_merge.append(os.path.join(src_folder_flatten_tiles_w_overlap, tile.split('.laz')[0] + \"_flatten_no_ov.laz\"))\n",
    "\n",
    "# Merging\n",
    "print(\"Merging all flatten tiles together (might take a few minutes)\")\n",
    "src_flatten_file = os.path.join(src_original.split('.laz')[0] + \"_flatten.laz\")\n",
    "merge_laz(list_flatten_to_merge, src_flatten_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad12268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of stripes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [01:24<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate stripes from the merged flatten\n",
    "# src_folder_stripes = os.path.join(os.path.dirname(src_flatten_file), os.path.basename(src_flatten_file).split('.laz')[0] + f\"_stripes_{stripe_width}_m\")\n",
    "x_span = laz_original.x.max() - laz_original.x.min()\n",
    "y_span = laz_original.y.max() - laz_original.y.min()\n",
    "dims = [stripe_width, y_span]\n",
    "\n",
    "print(\"Creation of stripes:\")\n",
    "stripes_file(src_flatten_file, src_folder_stripes_flatten, dims, do_keep_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07ac0b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Preprocess done in 0:53:48 ====\n"
     ]
    }
   ],
   "source": [
    "delta_time_loop = time() - start_preprocess_time\n",
    "hours = int(delta_time_loop // 3600)\n",
    "min = int((delta_time_loop - 3600 * hours) // 60)\n",
    "sec = int(delta_time_loop - 3600 * hours - 60 * min)\n",
    "print(f\"==== Preprocess done in {hours}:{min}:{sec} ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e802b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLICK ON \"Execute above cells\" ON THIS CELL TO RUN THE PRE-PROCESS. OVER THERE (triangle with top arrow) --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba4da0",
   "metadata": {},
   "source": [
    "### Manual cleaning\n",
    "Once the flattening step done, you will find the resulting stripes in the folder <>.\n",
    "\n",
    "Please clean them and save them with the same name in the folder <> alredy created.\n",
    "\n",
    "To be aware of regarding the cleaning:\n",
    " - a\n",
    " - b\n",
    " - c\n",
    "\n",
    "Afterward, you can run the post-processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885d71c",
   "metadata": {},
   "source": [
    "### Prepare second pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e92a5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_clean_stripes_flatten = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\quadric_interp\\flatten_and_clean\"\n",
    "\n",
    "if src_clean_stripes_flatten == None:\n",
    "    src_clean_stripes_flatten = src_folder_stripes_flatten\n",
    "\n",
    "# check for errors\n",
    "for file in os.listdir(src_clean_stripes_flatten):\n",
    "    if not file.endswith('laz'):\n",
    "        print(f\"Warning! The following file is not laz:\\n\\t{os.path.join(src_clean_stripes_flatten, file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8968d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:01<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001660,)\n",
      "1203082\n"
     ]
    }
   ],
   "source": [
    "laz = laspy.read(src_with_id)\n",
    "\n",
    "list_clean_flatten_to_merge = [os.path.join(src_clean_stripes_flatten, x) for x in os.listdir(src_clean_stripes_flatten) if x.endswith('laz')]\n",
    "\n",
    "id_points = np.array([])\n",
    "for _, clean_stripe_path in tqdm(enumerate(list_clean_flatten_to_merge), total=len(list_clean_flatten_to_merge)):\n",
    "    laz_stripe_flatten =laspy.read(clean_stripe_path)\n",
    "    id_points = np.concatenate([id_points, laz_stripe_flatten.id_point])\n",
    "print(id_points.shape)\n",
    "print(len(set(id_points)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36bca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10a106dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len laz 1: 867462\n",
      "len laz 2: 3366076\n",
      "set id_point laz 1: 867460\n",
      "set id_point laz 2: 3366069\n",
      "random point - laz 1: 36466327\n",
      "random point - laz 2: 36465845\n",
      "max id_point laz 2: 59188718\n",
      "max id_point laz 2: 59405457\n",
      "len concatenate version: 4233538\n",
      "set id_point concatenate version: 3366069\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "src_1 = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_flatten_stripe_40_cropped_with_id_points.laz\"\n",
    "src_2 = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_flatten_stripes_20m_flatten\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_flatten_stripe_40.laz\"\n",
    "\n",
    "laz_1 = laspy.read(src_1)\n",
    "laz_2 = laspy.read(src_2)\n",
    "\n",
    "rnd_point = 301\n",
    "\n",
    "# class_mask = laz_2.classification == 0\n",
    "# print(np.sum(class_mask))\n",
    "# laz_2.points = laz_2.points[class_mask]\n",
    "\n",
    "print(f\"len laz 1: {len(laz_1)}\")\n",
    "print(f\"len laz 2: {len(laz_2)}\")\n",
    "print(f\"set id_point laz 1: {len(set(laz_1.id_point))}\")\n",
    "print(f\"set id_point laz 2: {len(set(laz_2.id_point))}\")\n",
    "\n",
    "print(f\"random point - laz 1: {laz_1.id_point[rnd_point]}\")\n",
    "print(f\"random point - laz 2: {laz_2.id_point[rnd_point]}\")\n",
    "\n",
    "print(f\"max id_point laz 2: {max(set(laz_1.id_point))}\")\n",
    "print(f\"max id_point laz 2: {max(set(laz_2.id_point))}\")\n",
    "\n",
    "conc = np.concatenate([laz_1.id_point, laz_2.id_point])\n",
    "print(f\"len concatenate version: {len(conc)}\")\n",
    "print(f\"set id_point concatenate version: {len(set(conc))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fcc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_point = 301\n",
    "\n",
    "3189445\n",
    "\n",
    "\n",
    "id_points = list(laz_1.id_point)\n",
    "print(len(id_points))\n",
    "\n",
    "different_vals = set(laz_2.id_point)\n",
    "for counter, i in enumerate(different_vals):\n",
    "    mask = laz_2.id_point == i\n",
    "    print(f\"{i}: {np.sum(mask)}\")\n",
    "    # print(f\"random point - laz 1: {laz_1.id_point[rnd_point + a]}\")\n",
    "    # print(f\"random point - laz 2: {laz_2.id_point[rnd_point] + a}\")\n",
    "    # print('---')\n",
    "    if counter == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3cf4465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3366076,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(laz_2.id_point).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "12e9bc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca9eb0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'Y', 'Z', 'intensity', 'return_number', 'number_of_returns', 'scan_direction_flag', 'edge_of_flight_line', 'classification', 'synthetic', 'key_point', 'withheld', 'scan_angle_rank', 'user_data', 'point_source_id', 'gps_time', 'Amplitude', 'Reflectance', 'Deviation', 'id_point']\n",
      "(3366076, 5)\n",
      "3366076\n"
     ]
    }
   ],
   "source": [
    "def convert_laz_to_pcd(in_laz, out_pcd, verbose=True):\n",
    "    laz = laspy.read(in_laz)\n",
    "\n",
    "    # Gathering all attributes from laz file\n",
    "    points = np.vstack((laz.x, laz.y, laz.z)).T\n",
    "\n",
    "    attributes = {}\n",
    "    for attribute in laz.point_format.dimensions:\n",
    "        if attribute.name in ['X', 'Y', 'Z']:\n",
    "            continue\n",
    "        attributes[attribute.name] = getattr(laz, attribute.name)\n",
    "    \n",
    "    # Preparing data for pcd\n",
    "    num_points = points.shape[0]\n",
    "    fields = [\"x\", \"y\", \"z\"] + list(attributes.keys())  # All field names\n",
    "    types = [\"F\", \"F\", \"F\"] + [\"F\" for _ in attributes]  # Float32 fields\n",
    "    sizes = [4] * len(fields)  # 4-byte float per field\n",
    "\n",
    "    # Stack all data into a single NumPy array\n",
    "    data = np.column_stack([points] + [attributes[key] for key in attributes])\n",
    "\n",
    "    # Write to a PCD file\n",
    "    with open(out_pcd, \"w\") as f:\n",
    "        # f.write(f\"# .PCD v0.7 - Point Cloud Data file format\\n\")\n",
    "        f.write(f\"VERSION 0.7\\n\")\n",
    "        f.write(f\"FIELDS {' '.join(fields)}\\n\")\n",
    "        f.write(f\"SIZE {' '.join(map(str, sizes))}\\n\")\n",
    "        f.write(f\"TYPE {' '.join(types)}\\n\")\n",
    "        f.write(f\"COUNT {' '.join(['1'] * len(fields))}\\n\")\n",
    "        f.write(f\"WIDTH {num_points}\\n\")\n",
    "        f.write(f\"HEIGHT 1\\n\")\n",
    "        f.write(f\"VIEWPOINT 0 0 0 1 0 0 0\\n\")\n",
    "        f.write(f\"POINTS {num_points}\\n\")\n",
    "        f.write(f\"DATA ascii\\n\")\n",
    "    \n",
    "        # Write data\n",
    "        np.savetxt(f, data, fmt=\" \".join([\"%.6f\"] * len(fields)))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"PCD file saved in {out_pcd}\")\n",
    "\n",
    "\n",
    "def convert_laz_to_txt(in_laz, out_pcd, verbose=True):\n",
    "    laz = laspy.read(in_laz)\n",
    "    print(list(laz.point_format.dimension_names))\n",
    "    list_dims = ['X', 'Y', 'Z','intensity', 'id_point']\n",
    "    points = np.concatenate([np.array(getattr(laz, x)).reshape(-1,1) for x in list_dims], axis=1)\n",
    "    print(points.shape)\n",
    "    print(len(laz))\n",
    "    points[:,3] = points[:,3] / 65535 * 255\n",
    "\n",
    "    # print(\" \".join([\"%.0f\"] * len(list_dims)))\n",
    "    # return\n",
    "    # Write to a txt file\n",
    "    with open(out_pcd, \"w\") as f:\n",
    "        # np.savetxt(f, points, fmt=\" \".join([\"%.0f\"] * len(list_dims)))\n",
    "        np.savetxt(f, points, fmt=\"%.3f %.3f %.3f %d %d\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def convert_txt_to_laz(in_txt, out_laz, verbose=True):\n",
    "    # Load the TXT (columns: X,Y,Z,intensity,id_point)\n",
    "    print('derp')\n",
    "    data = np.loadtxt(in_txt, delimiter=',')\n",
    "    print(data.shape)\n",
    "    # return\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(1, -1)\n",
    "\n",
    "    X = data[:, 0]\n",
    "    Y = data[:, 1]\n",
    "    Z = data[:, 2]\n",
    "    # intensity = data[:, 3].astype(np.uint16)\n",
    "    # id_point = data[:, 4].astype(np.uint32)\n",
    "\n",
    "    # Create a LAS file with a point format that supports intensity + extra dims\n",
    "    header = laspy.LasHeader(point_format=3, version=\"1.4\")\n",
    "\n",
    "    las = laspy.LasData(header)\n",
    "\n",
    "    # Assign coordinates (laspy expects scaled ints internally)\n",
    "    las.X = X\n",
    "    las.Y = Y\n",
    "    las.Z = Z\n",
    "\n",
    "    # # Built-in dimension\n",
    "    # las.intensity = intensity\n",
    "\n",
    "    # # Create the extra dimension \"id_point\"\n",
    "    # if \"id_point\" not in las.point_format.dimension_names:\n",
    "    #     las.add_extra_dim(\n",
    "    #         laspy.ExtraBytesParams(\n",
    "    #             name=\"id_point\",\n",
    "    #             type=np.uint32,\n",
    "    #             description=\"Original point ID\"\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    # las.id_point = id_point\n",
    "\n",
    "    # Save the result\n",
    "    las.write(out_laz)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Saved LAZ with {len(las)} points to: {out_laz}\")\n",
    "        print(\"Dimensions:\", las.point_format.dimension_names)\n",
    "\n",
    "in_laz = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_flatten_stripe_40.laz\"\n",
    "out_txt = in_laz.split('.laz')[0] + '.txt'\n",
    "convert_laz_to_txt(in_laz, out_txt)\n",
    "# in_txt = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_flatten_stripe_40_cropped.txt\"\n",
    "# out_laz = in_txt.split('.txt')[0] + '.laz'\n",
    "# convert_txt_to_laz(in_txt, out_laz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcca57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_coords(x, y, z, rounding=2):\n",
    "    \"\"\"Create a unique integer hash for each rounded coordinate triple.\"\"\"\n",
    "    return np.round(x, rounding) * 1e12 + np.round(y, rounding) * 1e6 + np.round(z, rounding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f52c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_original = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_flatten_stripe_40.laz\"\n",
    "src_cropped = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_flatten_stripe_40_cropped.laz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aabab795",
   "metadata": {},
   "outputs": [],
   "source": [
    "laz_original = laspy.read(src_original)\n",
    "laz_cropped = laspy.read(src_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "172ac91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3366076,)\n",
      "(867117,)\n"
     ]
    }
   ],
   "source": [
    "hash_original = hash_coords(laz_original.x, laz_original.y, laz_original.z, 3)\n",
    "hash_cropped = hash_coords(laz_cropped.x, laz_cropped.y, laz_cropped.z, 3)\n",
    "print(hash_original.shape)\n",
    "print(hash_cropped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32efa982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867462\n",
      "(3366076,)\n"
     ]
    }
   ],
   "source": [
    "mask = np.isin(hash_original, hash_cropped)\n",
    "print(np.sum(mask))\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2229648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "laz_original.points = laz_original.points[mask]\n",
    "laz_original.write(src_original.split('.laz')[0] + \"_cropped_with_id_points.laz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc80c817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all flatten tiles together (might take a few minutes)\n",
      "D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\quadric_interp\\flatten_and_clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 27.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_flatten_clean.laz\n"
     ]
    }
   ],
   "source": [
    "# Merging\n",
    "print(\"Merging all flatten tiles together (might take a few minutes)\")\n",
    "src_flatten_clean_file = os.path.join(src_original.split('.laz')[0] + \"_flatten_clean.laz\")\n",
    "print(src_clean_stripes_flatten)\n",
    "list_clean_flatten_to_merge = [os.path.join(src_clean_stripes_flatten, x) for x in os.listdir(src_clean_stripes_flatten) if x.endswith('laz')]\n",
    "\n",
    "merge_laz(list_clean_flatten_to_merge, src_flatten_clean_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ce2103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remapping to original\n",
    "laz_flatten_clean = laspy.read(src_flatten_clean_file)\n",
    "\n",
    "mask = np.isin(laz_original.id_point, laz_flatten_clean.id_point)\n",
    "\n",
    "src_first_path_result = src_original.split('.laz')[0] + \"_first_path.laz\"\n",
    "laz_original[mask].write(src_first_path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7484d7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of stripes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 33.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# striping original\n",
    "x_span = laz_original.x.max() - laz_original.x.min()\n",
    "y_span = laz_original.y.max() - laz_original.y.min()\n",
    "dims = [stripe_width, y_span]\n",
    "\n",
    "print(\"Creation of stripes:\")\n",
    "stripes_file(src_first_path_result, src_folder_stripes, dims, do_keep_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72315f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f5c8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correspond number to path\n",
    "list_files_original = [os.path.join(src_folder_stripes, x) for x in os.listdir(src_folder_stripes) if x.endswith('.laz')]\n",
    "list_files_flatten = [os.path.join(src_clean_stripes_flatten, x) for x in os.listdir(src_clean_stripes_flatten) if x.endswith('.laz')]\n",
    "\n",
    "dict_files_original = {int(x.split('_')[-1].split('.')[0]): x for x in list_files_original}\n",
    "\n",
    "dict_files_flatten = {int(x.split('_')[-1].split('.')[0]): x for x in list_files_flatten}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7efee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52026\n",
      "160\n",
      "0\n",
      "---\n",
      "202867\n",
      "13820\n",
      "77\n",
      "---\n",
      "1695840\n",
      "597662\n",
      "75765\n",
      "---\n",
      "2118880\n",
      "604055\n",
      "76334\n",
      "---\n",
      "2187766\n",
      "582333\n",
      "73501\n",
      "---\n",
      "2641702\n",
      "940345\n",
      "118559\n",
      "---\n",
      "1955111\n",
      "1258451\n",
      "181\n",
      "---\n",
      "2018221\n",
      "1107544\n",
      "63\n",
      "---\n",
      "1850382\n",
      "937504\n",
      "133\n",
      "---\n",
      "1503084\n",
      "934909\n",
      "168\n",
      "---\n",
      "2372085\n",
      "639394\n",
      "177\n",
      "---\n",
      "2423121\n",
      "814411\n",
      "247\n",
      "---\n",
      "362158\n",
      "26301\n",
      "5\n",
      "---\n",
      "2128482\n",
      "782347\n",
      "0\n",
      "---\n",
      "492350\n",
      "81395\n",
      "145\n",
      "---\n",
      "282678\n",
      "99721\n",
      "14\n",
      "---\n",
      "307949\n",
      "123662\n",
      "79\n",
      "---\n",
      "361463\n",
      "108384\n",
      "30\n",
      "---\n",
      "401036\n",
      "85759\n",
      "0\n",
      "---\n",
      "325504\n",
      "90151\n",
      "50\n",
      "---\n",
      "630274\n",
      "173352\n",
      "123\n",
      "---\n",
      "Warning! The following 34/55 files did not find corresponding clean versions:\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_21.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_22.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_23.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_24.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_25.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_26.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_27.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_28.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_29.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_30.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_31.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_32.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_33.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_34.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_35.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_36.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_37.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_38.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_39.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_40.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_41.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_42.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_43.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_44.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_45.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_46.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_47.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_48.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_49.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_50.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_51.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_52.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_53.laz\n",
      "\tBarmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_stripe_54.laz\n"
     ]
    }
   ],
   "source": [
    "# mask the originals with the clean stripess\n",
    "list_files = [x for x in os.listdir(src_folder_stripes) if x.endswith('.laz')]\n",
    "list_failed = []\n",
    "for key, path_original in dict_files_original.items():\n",
    "    # num_stripe = int(file.split('_')[-1].split('.')[0])\n",
    "    # if num_stripe not in [int(x.split('_')[-1].split('.')[0]) for x in os.listdir(src_clean_stripes_flatten)]:\n",
    "    #     list_failed.append(file)\n",
    "    #     continue\n",
    "    if key not in list(dict_files_flatten.keys()):\n",
    "        list_failed.append(os.path.basename(path_original))\n",
    "        continue\n",
    "    # flatten_file_src = os.path.join(src_clean_stripes_flatten, file)\n",
    "    laz_flatten_stripe = laspy.read(dict_files_flatten[key])\n",
    "    laz_original_stripe = laspy.read(path_original)\n",
    "\n",
    "    mask = np.isin(laz_original_stripe.id_point, laz_flatten_stripe.id_point)\n",
    "    print(len(laz_original_stripe))\n",
    "    print(len(laz_flatten_stripe))\n",
    "    print(np.sum(mask))\n",
    "    print('---')\n",
    "    laz_original_stripe.points = laz_original_stripe.points[mask]\n",
    "\n",
    "    laz_original_stripe.write(path_original)\n",
    "\n",
    "if len(list_failed) > 0:\n",
    "    print(f\"Warning! The following {len(list_failed)}/{len(os.listdir(src_folder_stripes))} files did not find corresponding clean versions:\")\n",
    "    for x in list_failed:\n",
    "        print(f\"\\t{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb6d9b",
   "metadata": {},
   "source": [
    "### Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63e3b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_clean_stripes = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\quadric_interp\\flatten_and_clean\"\n",
    "\n",
    "if src_clean_stripes == None:\n",
    "    src_clean_stripes = src_folder_stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7bb7c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all flatten tiles together (might take a few minutes)\n",
      "D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\quadric_interp\\flatten_and_clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 27.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_flatten_clean.laz\n"
     ]
    }
   ],
   "source": [
    "# Merging\n",
    "print(\"Merging all flatten tiles together (might take a few minutes)\")\n",
    "src_flatten_clean_file = os.path.join(src_original.split('.laz')[0] + \"_flatten_clean.laz\")\n",
    "print(src_clean_stripes)\n",
    "list_clean_flatten_to_merge = [os.path.join(src_clean_stripes, x) for x in os.listdir(src_clean_stripes) if x.endswith('laz')]\n",
    "\n",
    "merge_laz(list_clean_flatten_to_merge, src_flatten_clean_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e1235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001660\n",
      "101685063\n",
      "(101685063,)\n",
      "1203082\n"
     ]
    }
   ],
   "source": [
    "# remapping to original\n",
    "laz_flatten_clean = laspy.read(src_flatten_clean_file)\n",
    "\n",
    "mask = np.isin(laz_original.id_point, laz_flatten_clean.id_point)\n",
    "\n",
    "src_final_result = src_original.split('.laz')[0] + \"_FINAL.laz\"\n",
    "laz_original[mask].write(src_final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2091fa42",
   "metadata": {},
   "source": [
    "### Test shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05dce3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_loc_1 = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_mean_with_shift\\no_shift\"\n",
    "src_loc_2 = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_mean_with_shift\\shift_50\"\n",
    "src_results = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_mean_with_shift\\res_shifted\"\n",
    "src_final = os.path.join(src_results, \"merged_final.laz\")\n",
    "\n",
    "os.makedirs(src_results, exist_ok=True)\n",
    "\n",
    "locs = [src_loc_1, src_loc_2]\n",
    "\n",
    "lst_merged = []\n",
    "for loc in locs:\n",
    "    src_merged = os.path.join(src_results, \"merged_\" + os.path.basename(loc) + \".laz\")\n",
    "    lst_merged.append(src_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610fb06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging \"no_shift\" :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:06<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_mean_with_shift\\res_shifted\\merged_no_shift.laz\n",
      "Merging \"shift_50\" :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:06<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_mean_with_shift\\res_shifted\\merged_shift_50.laz\n"
     ]
    }
   ],
   "source": [
    "# merge sampels\n",
    "lst_merged = []\n",
    "for loc in locs:\n",
    "    src_merged = os.path.join(src_results, \"merged_\" + os.path.basename(loc) + \".laz\")\n",
    "    lst_merged.append(src_merged)\n",
    "    print(f'Merging \"{os.path.basename(loc)}\" :')\n",
    "\n",
    "    lst_files_to_merge = [os.path.join(loc, 'Barmasse_2025_AllScans_Raw_Sub2cm_with_id_flatten', x) for x in os.listdir(os.path.join(loc, 'Barmasse_2025_AllScans_Raw_Sub2cm_with_id_flatten')) if x.endswith('flatten_no_ov.laz')] \n",
    "    merge_laz(lst_files_to_merge, src_merged)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101685552\n",
      "101685240\n",
      "101685556\n",
      "101685223\n"
     ]
    }
   ],
   "source": [
    "# combining results\n",
    "lst_inputs = []\n",
    "for src_merged in lst_merged:\n",
    "    laz = laspy.read(src_merged)\n",
    "    lst_inputs.append(laz.points)\n",
    "    print(len(laz))\n",
    "    print(len(set(laz.id_point)))\n",
    "    # print(np.asarray(laz.points).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8b4a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "las1 = laspy.read(lst_merged[0])\n",
    "las2 = laspy.read(lst_merged[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8a895be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101685535\n"
     ]
    }
   ],
   "source": [
    "mask_is_in = np.isin(las1.id_point, las2.id_point)\n",
    "print(np.sum(mask_is_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7422ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Index array for las2\n",
    "# idx_map = {pid: i for i, pid in enumerate(las2.id_point)}\n",
    "\n",
    "# reorder_idx = np.array([idx_map[pid] for pid in las1.id_point])\n",
    "# las2.id_point[reorder_idx] == las1.id_point\n",
    "\n",
    "# z2_reordered = las2.z[reorder_idx]\n",
    "# z1 = las1.z\n",
    "\n",
    "# mean_z1 = np.mean(z1)\n",
    "# mean_z2 = np.mean(z2_reordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbcfcf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101685552\n",
      "101685552\n",
      "(101685552,)\n",
      "(101685552,)\n",
      "(101685552,)\n",
      "(101685552,)\n"
     ]
    }
   ],
   "source": [
    "reorder_idx = np.argsort(las2.id_point)[np.searchsorted(\n",
    "    np.sort(las2.id_point),\n",
    "    las1.id_point\n",
    ")]\n",
    "\n",
    "# z2_reordered = las2.z[reorder_idx]\n",
    "\n",
    "z2_reordered = las2.z[reorder_idx]\n",
    "z1 = las1.z\n",
    "\n",
    "print(len(z2_reordered))\n",
    "print(len(z1))\n",
    "\n",
    "print(z1.shape)\n",
    "print(z2_reordered.shape)\n",
    "z_min = np.min([z1, z2_reordered], axis=0)\n",
    "\n",
    "print(z_min.shape)\n",
    "print(z1.shape)\n",
    "setattr(las1, 'z', z_min)\n",
    "\n",
    "las1.write(src_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09b86cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 5 3 3 2 4]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,5,6,3,2,4])\n",
    "b = np.array([2,5,6,3,9,8,5])\n",
    "\n",
    "c = np.min([a,b], axis=0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c84dab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\GitHubProjects\\\\Terranum_repo\\\\TreeSegmentation\\\\data\\\\Barmasse\\\\Barmasse_2025\\\\test_mean_with_shift\\\\res_shifted\\\\merged_final.laz', 'D:\\\\GitHubProjects\\\\Terranum_repo\\\\TreeSegmentation\\\\data\\\\Barmasse\\\\Barmasse_2025\\\\test_mean_with_shift\\\\res_shifted\\\\merged_no_shift.laz', 'D:\\\\GitHubProjects\\\\Terranum_repo\\\\TreeSegmentation\\\\data\\\\Barmasse\\\\Barmasse_2025\\\\test_mean_with_shift\\\\res_shifted\\\\merged_shift_50.laz']\n",
      "Creation of stripes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:51<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# stripes all\n",
    "lst_files = [os.path.join(src_results, x) for x in os.listdir(src_results) if x.endswith('laz')]\n",
    "print(lst_files)\n",
    "for src_laz in lst_files:\n",
    "    laz = laspy.read(src_laz)\n",
    "\n",
    "    # Generate stripes from the merged flatten\n",
    "    src_folder_stripes = os.path.join(os.path.dirname(src_laz), os.path.basename(src_laz).split('.laz')[0] + f\"_stripes_{stripe_width}_m\")\n",
    "    if os.path.exists(src_folder_stripes):\n",
    "        continue\n",
    "    x_span = laz.x.max() - laz.x.min()\n",
    "    y_span = laz.y.max() - laz.y.min()\n",
    "    dims = [stripe_width, y_span]\n",
    "\n",
    "    print(\"Creation of stripes:\")\n",
    "    stripes_file(src_laz, src_folder_stripes, dims, do_keep_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e51f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be0607ad",
   "metadata": {},
   "source": [
    "### test unflattening based on positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cb41ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "laz_original = laspy.read(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\test_unflatten_based_on_pos\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_tile_5_4.laz\")\n",
    "laz_flatten = laspy.read(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\test_unflatten_based_on_pos\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_tile_5_4_flatten.laz\")\n",
    "laz_floor = laspy.read(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\test_unflatten_based_on_pos\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_tile_5_4_floor.laz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18864d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8551996\n",
      "8551996\n",
      "8551996\n"
     ]
    }
   ],
   "source": [
    "print(len(laz_original))\n",
    "print(len(laz_flatten))\n",
    "print(len(laz_floor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8ffd404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8551996\n",
      "2883456\n"
     ]
    }
   ],
   "source": [
    "laz_flatten_clean = laspy.read(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\test_unflatten_based_on_pos\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_tile_5_4_flatten.las\")\n",
    "print(len(laz_flatten_clean))\n",
    "print(len(set(laz_flatten_clean.id_point)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004167e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4025521, 2)\n",
      "(8551996, 2)\n",
      "[[(-890.42, 219.8 )]\n",
      " [(-890.31, 219.91)]\n",
      " [(-890.42, 219.85)]\n",
      " ...\n",
      " [(-864.91, 341.53)]\n",
      " [(-864.85, 341.51)]\n",
      " [(-865.18, 341.92)]]\n",
      "[ True  True  True ...  True  True  True]\n",
      "17103992\n",
      "17103992\n"
     ]
    }
   ],
   "source": [
    "# add floor to corresponding flatten\n",
    "x_y_flatten = np.concatenate([np.array(laz_flatten_clean.x).reshape(-1,1), np.array(laz_flatten_clean.y).reshape(-1,1)], axis=1)\n",
    "x_y_floor = np.concatenate([np.array(laz_floor.x).reshape(-1,1), np.array(laz_floor.y).reshape(-1,1)], axis=1)\n",
    "print(x_y_flatten.shape)\n",
    "print(x_y_floor.shape)\n",
    "x_y_flatten_view = x_y_flatten.view([('x', x_y_flatten.dtype), ('y', x_y_flatten.dtype)])\n",
    "print(x_y_flatten_view)\n",
    "x_y_floor_view = x_y_floor.view([('x', x_y_floor.dtype), ('y', x_y_floor.dtype)])\n",
    "# mask = np.isin(x_y_floor, x_y_flatten)\n",
    "mask = np.isin(x_y_floor, x_y_flatten)\n",
    "mask = mask.reshape(-1)\n",
    "print(mask)\n",
    "print(len(mask))\n",
    "print(np.sum(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f8f6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ... False False False]\n",
      "(8551996,)\n",
      "4107071\n"
     ]
    }
   ],
   "source": [
    "def row_isin_bool(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return boolean mask of shape (N,) where mask[i] is True if A[i] is a row in B.\n",
    "    A: (N, K), B: (M, K)\n",
    "    \"\"\"\n",
    "    if A.ndim != 2 or B.ndim != 2:\n",
    "        raise ValueError(\"A and B must be 2-D arrays\")\n",
    "    if A.shape[1] != B.shape[1]:\n",
    "        raise ValueError(\"A and B must have same number of columns\")\n",
    "\n",
    "    # view each row as a single void item\n",
    "    row_dtype = np.dtype((np.void, A.dtype.itemsize * A.shape[1]))\n",
    "    A_view = np.ascontiguousarray(A).view(row_dtype).ravel()\n",
    "    B_view = np.ascontiguousarray(B).view(row_dtype).ravel()\n",
    "\n",
    "    mask = np.isin(A_view, B_view)\n",
    "    return mask  # shape (N,)\n",
    "\n",
    "# Example\n",
    "A = np.array([[1,2],[3,4],[5,6],[7,8]])\n",
    "B = np.array([[3,4],[7,8]])\n",
    "mask = row_isin_bool(x_y_floor, x_y_flatten)\n",
    "print(mask)          # [False  True False  True]\n",
    "print(mask.shape)    # (4,)\n",
    "print(np.sum(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4107071, 2)\n",
      "4107071\n",
      "3495786\n"
     ]
    }
   ],
   "source": [
    "x_y_floor_clean = x_y_floor[mask]\n",
    "print(x_y_floor_clean.shape)\n",
    "laz_floor.points = laz_floor.points[mask]\n",
    "print(len(laz_floor))\n",
    "laz_floor = remove_duplicates(laz_floor)\n",
    "print(len(laz_floor))\n",
    "laz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c084a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4025521 [00:00<?, ?it/s]C:\\Users\\swann\\AppData\\Local\\Temp\\ipykernel_17072\\3514537216.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  xyz_flatten[i,2] += corresponding_z(xy_floor, xy_point, z_floor)\n",
      "  0%|          | 4514/4025521 [04:18<64:03:17, 17.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m xy_point \u001b[38;5;241m=\u001b[39m xyz_flatten[i,:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(xy_point.shape)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m xyz_flatten[i,\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcorresponding_z\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy_floor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxy_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_floor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 18\u001b[0m, in \u001b[0;36mcorresponding_z\u001b[1;34m(A, B, z_values)\u001b[0m\n\u001b[0;32m     16\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misin(A_view, B_view)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(mask) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mz_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def corresponding_z(A: np.ndarray, B: np.ndarray, z_values) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return boolean mask of shape (N,) where mask[i] is True if A[i] is a row in B.\n",
    "    A: (N, K), B: (M, K)\n",
    "    \"\"\"\n",
    "    if A.ndim != 2 or B.ndim != 2:\n",
    "        raise ValueError(\"A and B must be 2-D arrays\")\n",
    "    if A.shape[1] != B.shape[1]:\n",
    "        raise ValueError(\"A and B must have same number of columns\")\n",
    "\n",
    "    # view each row as a single void item\n",
    "    row_dtype = np.dtype((np.void, A.dtype.itemsize * A.shape[1]))\n",
    "    A_view = np.ascontiguousarray(A).view(row_dtype).ravel()\n",
    "    B_view = np.ascontiguousarray(B).view(row_dtype).ravel()\n",
    "\n",
    "    mask = np.isin(A_view, B_view)\n",
    "    if np.sum(mask) > 0:\n",
    "        return z_values[mask][0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "xy_floor = np.concatenate([np.array(laz_floor.x).reshape(-1,1), np.array(laz_floor.y).reshape(-1,1)], axis=1)\n",
    "xyz_flatten = np.concatenate([np.array(laz_flatten_clean.x).reshape(-1,1), np.array(laz_flatten_clean.y).reshape(-1,1), np.array(laz_flatten_clean.z).reshape(-1,1)], axis=1)\n",
    "z_floor = np.array(laz_floor.z).reshape(-1,1)\n",
    "for i in tqdm(range(xyz_flatten.shape[0]), total=xyz_flatten.shape[0]):\n",
    "    xy_point = xyz_flatten[i,:2].reshape(1,-1)\n",
    "    xyz_flatten[i,2] += corresponding_z(xy_floor, xy_point, z_floor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "355fbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_matching_z(xy_floor, z_floor, xyz_flatten):\n",
    "    \"\"\"\n",
    "    xy_floor:    (M, 2)\n",
    "    z_floor:     (M,)\n",
    "    xyz_flatten: (N, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert xy_floor rows to tuples → hashable\n",
    "    xy_tuples = [tuple(row) for row in xy_floor]\n",
    "\n",
    "    # Build dictionary: (x, y) → z\n",
    "    mapping = dict(zip(xy_tuples, z_floor))\n",
    "\n",
    "    # For the points to check (xyz_flatten)\n",
    "    xy_flatten_tuples = [tuple(row) for row in xyz_flatten[:, :2]]\n",
    "\n",
    "    # Vectorized lookup (dictionary lookups are very fast)\n",
    "    matched_z = np.array([mapping.get(t, 0) for t in xy_flatten_tuples])\n",
    "\n",
    "    # Add to z column\n",
    "    xyz_flatten[:, 2] += matched_z\n",
    "\n",
    "    return xyz_flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6df61730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_matching_z_fast(xy_floor, z_floor, xyz_flatten):\n",
    "    \"\"\"\n",
    "    Vectorized addition of z_floor to xyz_flatten where xy matches.\n",
    "    xy_floor: (M,2)\n",
    "    z_floor: (M,)\n",
    "    xyz_flatten: (N,3)\n",
    "    \"\"\"\n",
    "    # --- View each row as a single void element ---\n",
    "    row_dtype = np.dtype((np.void, xy_floor.dtype.itemsize * xy_floor.shape[1]))\n",
    "    xy_floor_view = np.ascontiguousarray(xy_floor).view(row_dtype).ravel()\n",
    "    xy_flatten_view = np.ascontiguousarray(xyz_flatten[:, :2]).view(row_dtype).ravel()\n",
    "\n",
    "    # --- Use np.isin for mask (vectorized) ---\n",
    "    mask = np.isin(xy_flatten_view, xy_floor_view)\n",
    "\n",
    "    # --- Create mapping from xy_floor_view -> z_floor ---\n",
    "    # Only select matching floor points\n",
    "    xy_floor_match = xy_floor_view[np.isin(xy_floor_view, xy_flatten_view)]\n",
    "    z_floor_match = z_floor[np.isin(xy_floor_view, xy_flatten_view)]\n",
    "\n",
    "    # --- Build mapping dict (only for matches, fast) ---\n",
    "    mapping = dict(zip(xy_floor_match, z_floor_match))\n",
    "\n",
    "    # --- Map z values to flatten array ---\n",
    "    matched_z = np.array([mapping.get(v, 0) for v in xy_flatten_view], dtype=z_floor.dtype)\n",
    "\n",
    "    # --- Add to xyz_flatten z ---\n",
    "    xyz_flatten[:, 2] += matched_z\n",
    "\n",
    "    return xyz_flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eb6fc006",
   "metadata": {},
   "outputs": [],
   "source": [
    "laz_flatten_clean = laspy.read(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\test_unflatten_based_on_pos\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_tile_5_4_flatten_clean.laz\")\n",
    "laz_floor = laspy.read(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\test_unflatten_based_on_pos\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_tile_5_4_floor.laz\")\n",
    "xy_floor = np.column_stack([laz_floor.x, laz_floor.y])\n",
    "z_floor  = np.array(laz_floor.z)\n",
    "\n",
    "xyz_flatten = np.column_stack([laz_flatten_clean.x,\n",
    "                               laz_flatten_clean.y,\n",
    "                               laz_flatten_clean.z])\n",
    "\n",
    "xyz_flatten = add_matching_z(xy_floor, z_floor, xyz_flatten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a0fbbc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(laz_flatten_clean, 'z', xyz_flatten[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dddbb837",
   "metadata": {},
   "outputs": [],
   "source": [
    "laz_flatten_clean.write(r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\test_from_python_script\\test\\test_unflatten_based_on_pos\\Barmasse_2025_AllScans_Raw_Sub2cm_clean_with_id_tile_5_4_clean.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7aaa3e",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2954ec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear formula: y = 1.606137285578297e-06 * x + 2.436066175792884\n",
      "Total time would be 165.756136059822\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "points = np.array([\n",
    "    [2393853, 7.12],\n",
    "    [2235262, 8.03],\n",
    "    [1603158, 6.48],\n",
    "    [692378, 2.02],\n",
    "    [1877736, 7.04],\n",
    "    [1934395, 6.04],\n",
    "    [2237376, 9.35],\n",
    "    [1829152, 6.10],\n",
    "    [2642786, 4.39],\n",
    "    [2188907, 2.41],\n",
    "    [2119903, 3.15],\n",
    "    [1696805, 4.04],\n",
    "    [362739, 3.57],\n",
    "    [327552, 2.37],\n",
    "    [373182, 3.38],\n",
    "    [300376, 4.25],\n",
    "    [366145, 4.05],\n",
    "    [417841, 3.29],\n",
    "    [359032, 3.49],\n",
    "    [159698, 1.49],\n",
    "    [26818,  1.09],\n",
    "])\n",
    "\n",
    "\n",
    "def linear_interpolation_formula(points: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the linear interpolation (linear regression) formula y = ax + b\n",
    "    from a 2D array of shape (N, 2).\n",
    "\n",
    "    points : np.ndarray (N, 2)   Array of [x, y] points\n",
    "    Returns : (a, b)             Slope and intercept of the fitted line\n",
    "    \"\"\"\n",
    "    x = points[:, 0]\n",
    "    y = points[:, 1]\n",
    "\n",
    "    # Least squares solution for ax + b\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    a, b = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "\n",
    "    return a, b\n",
    "\n",
    "\n",
    "a, b = linear_interpolation_formula(points)\n",
    "print(f\"Linear formula: y = {a} * x + {b}\")\n",
    "\n",
    "num_points = 101685000\n",
    "print(f\"Total time would be {a*num_points + b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ea037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import laspy\n",
    "# from pyproj import CRS\n",
    "# from laspy.vlrs.known import WktCoordinateSystemVlr\n",
    "\n",
    "\n",
    "# def ensure_crs_in_las(src_input, epsg=2056):\n",
    "#     las = laspy.read(src_input)\n",
    "#     if las.header.parse_crs() is None:\n",
    "#         crs = CRS.from_epsg(epsg)\n",
    "#         wkt = crs.to_wkt()\n",
    "#         vlr = WktCoordinateSystemVlr(wkt)\n",
    "#         las.header.vlrs.append(vlr)\n",
    "#         tmp_path = src_input.replace(\".laz\", \"_crs.laz\")\n",
    "#         las.write(tmp_path)\n",
    "#         print(f\"✅ Added CRS {epsg} to {src_input}\")\n",
    "#         return tmp_path\n",
    "#     else:\n",
    "#         return src_input\n",
    "\n",
    "\n",
    "# def tilling_slow(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Tile the input LiDAR file into square tiles using PDAL and store them in the destination folder.\n",
    "\n",
    "#     Args:\n",
    "#         - verbose (bool): Whether to print verbose status updates.\n",
    "\n",
    "#     Returns:\n",
    "#         - None: Splits the input file into tiles and saves them in the destination folder.\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"Start tilling (with overlap = {overlap}m)...\")\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "    \n",
    "#     src_input = ensure_crs_in_las(src_input, epsg=2056)\n",
    "    \n",
    "#     # compute the estimate number of tiles\n",
    "#     if verbose:\n",
    "#         print(\"Computing the estimated number of tiles...\")\n",
    "#     original_file = laspy.read(src_input)\n",
    "#     x_min = original_file.x.min()\n",
    "#     x_max = original_file.x.max()\n",
    "#     y_min = original_file.y.min()\n",
    "#     y_max = original_file.y.max() \n",
    "#     if verbose:\n",
    "#         print('Done!')\n",
    "\n",
    "\n",
    "    \n",
    "#     # output_pattern = \"tile_{i}_{j}.laz\"\n",
    "#     output_pattern = os.path.join(\n",
    "#         src_target, \n",
    "#         os.path.basename(src_input).split('.')[0] + \"_tile_{i}_{j}.laz\",\n",
    "#         )\n",
    "\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "#     combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "#     for _, (i,j) in tqdm(enumerate(combinations), total=len(combinations)):\n",
    "#         x0 = x_min + i * tile_size - overlap\n",
    "#         x1 = x_min + (i + 1) * tile_size + overlap\n",
    "#         y0 = y_min + j * tile_size - overlap\n",
    "#         y1 = y_min + (j + 1) * tile_size + overlap\n",
    "\n",
    "#         bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "#         pipeline_json = {\n",
    "#             \"pipeline\": [\n",
    "#                 # src_input,\n",
    "#                 {\n",
    "#                     \"type\": \"readers.las\",\n",
    "#                     \"filename\": src_input,\n",
    "#                     \"extra_dims\": \"id_point=uint32\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"filters.reprojection\",\n",
    "#                     \"in_srs\": \"EPSG:2056\",\n",
    "#                     \"out_srs\": \"EPSG:2056\"\n",
    "#                 },\n",
    "#                 {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "#                 {\n",
    "#                     \"type\": \"writers.las\",\n",
    "#                     \"filename\": output_pattern.format(i=i, j=j),\n",
    "#                     \"extra_dims\": \"id_point=uint32\"\n",
    "#                 }\n",
    "#             ]\n",
    "#         }\n",
    "#         pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#         pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e27b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_las_files_inmemory(list_files, output_file):\n",
    "#     \"\"\"\n",
    "#     Merge multiple LAS/LAZ files into one using laspy.\n",
    "#     Preserves all dimensions including extra_dims like 'id_point'.\n",
    "#     \"\"\"\n",
    "#     # Read the header from the first file\n",
    "#     first_las = laspy.read(list_files[0])\n",
    "#     header = laspy.LasHeader(point_format=first_las.header.point_format,\n",
    "#                               version=first_las.header.version)\n",
    "\n",
    "#     all_arrays = []\n",
    "#     for f in list_files:\n",
    "#         las = laspy.read(f)\n",
    "#         all_arrays.append(las.points.array)  # extract structured array\n",
    "#         print(f\"Read {f} ({len(las)} pts)\")\n",
    "\n",
    "#     # Concatenate structured arrays\n",
    "#     merged_array = np.concatenate(all_arrays)\n",
    "\n",
    "#     # Create new LasData with header\n",
    "#     out = laspy.LasData(header)\n",
    "\n",
    "#     # Wrap the concatenated array as ScaleAwarePointRecord and assign\n",
    "#     out.points = laspy.ScaleAwarePointRecord(\n",
    "#         merged_array,\n",
    "#         point_format=header.point_format,\n",
    "#         scales=header.scales,\n",
    "#         offsets=header.offsets\n",
    "#     )\n",
    "\n",
    "#     # Save\n",
    "#     out.write(output_file)\n",
    "#     print(f\"Merged file saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afa3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_las_files(list_files, output_path, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Merge multiple LAS/LAZ files into a single output file using laspy.\n",
    "\n",
    "#     Args:\n",
    "#         list_files (list[str]): Paths to input LAS/LAZ files.\n",
    "#         output_path (str): Path to output merged LAS/LAZ file.\n",
    "#         verbose (bool): Whether to print progress info.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     if len(list_files) == 0:\n",
    "#         raise ValueError(\"No input files provided for merging.\")\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"🧩 Merging {len(list_files)} LAS/LAZ files...\")\n",
    "\n",
    "#     # --- Read the first file to set the reference header ---\n",
    "#     ref_las = laspy.read(list_files[0])\n",
    "#     header = laspy.LasHeader(point_format=ref_las.header.point_format, version=ref_las.header.version)\n",
    "#     header.offsets = ref_las.header.offsets\n",
    "#     header.scales = ref_las.header.scales\n",
    "\n",
    "#     # Copy CRS if available\n",
    "#     if hasattr(ref_las.header, \"epsg\") and ref_las.header.epsg is not None:\n",
    "#         header.epsg = ref_las.header.epsg\n",
    "\n",
    "#     merged = laspy.LasData(header)\n",
    "#     all_points = []\n",
    "\n",
    "#     # --- Collect points from each file ---\n",
    "#     for f in list_files:\n",
    "#         las = laspy.read(f)\n",
    "#         if verbose:\n",
    "#             print(f\"   → Reading {f} ({len(las)} points)\")\n",
    "\n",
    "#         # Ensure same point format\n",
    "#         if las.header.point_format != ref_las.header.point_format:\n",
    "#             raise ValueError(f\"File {f} has different point format.\")\n",
    "\n",
    "#         # Convert to numpy structured array for fast stacking\n",
    "#         all_points.append(las.points.array)\n",
    "\n",
    "#     # --- Concatenate all points ---\n",
    "#     merged.points = laspy.ScaleAwarePointRecord.merge(all_points, header=header)\n",
    "\n",
    "#     # --- Save merged file ---\n",
    "#     merged.write(output_path)\n",
    "#     if verbose:\n",
    "#         print(f\"✅ Merged file saved to {output_path} ({len(merged)} points)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import laspy\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# def merge_las_files_inmemory(list_files, output_path, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Merge multiple LAS/LAZ files into one using laspy, preserving extra dimensions and CRS.\n",
    "#     \"\"\"\n",
    "#     if len(list_files) == 0:\n",
    "#         raise ValueError(\"No input files provided for merging.\")\n",
    "\n",
    "#     # --- Read reference file ---\n",
    "#     ref = laspy.read(list_files[0])\n",
    "#     header = laspy.LasHeader(point_format=ref.header.point_format, version=ref.header.version)\n",
    "#     header.scales = ref.header.scales\n",
    "#     header.offsets = ref.header.offsets\n",
    "\n",
    "#     # Copy CRS/VLRs if available\n",
    "#     header.vlrs = list(ref.header.vlrs)\n",
    "#     try:\n",
    "#         crs = ref.header.parse_crs()\n",
    "#         if crs is not None:\n",
    "#             header.parse_crs(crs)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     # --- Prepare dimension names ---\n",
    "#     dim_names = ref.point_format.dimension_names\n",
    "#     if verbose:\n",
    "#         print(f\"Detected dimensions: {dim_names}\")\n",
    "\n",
    "#     # --- Concatenate all points ---\n",
    "#     all_points = []\n",
    "#     total_points = 0\n",
    "#     for f in list_files:\n",
    "#         las = laspy.read(f)\n",
    "#         if las.point_format != ref.point_format:\n",
    "#             raise ValueError(f\"Point format mismatch in {f}\")\n",
    "#         all_points.append(las.points)\n",
    "#         total_points += len(las)\n",
    "#         if verbose:\n",
    "#             print(f\"Read {f} ({len(las)} pts)\")\n",
    "\n",
    "#     # Extract structured arrays and concatenate\n",
    "#     arrays_to_merge = [p.array for p in all_points]\n",
    "#     merged_array = np.concatenate(arrays_to_merge)\n",
    "\n",
    "#     # Wrap back as ScaleAwarePointRecord\n",
    "#     # merged_points = laspy.ScaleAwarePointRecord(merged_array, header=header)\n",
    "\n",
    "#     # Assign to LasData\n",
    "#     out = laspy.LasData(header)\n",
    "#     out.points = merged_array\n",
    "\n",
    "#     # # Stack ScaleAwarePointRecords\n",
    "#     # merged_points = np.concatenate(all_points)\n",
    "\n",
    "#     # # --- Create LasData and assign points ---\n",
    "#     # out = laspy.LasData(header)\n",
    "#     # out.points = laspy.ScaleAwarePointRecord(merged_points, header=header)\n",
    "\n",
    "#     # --- Write output ---\n",
    "#     out.write(output_path)\n",
    "#     if verbose:\n",
    "#         print(f\"✅ Merged {len(list_files)} files ({total_points} pts) -> {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd496ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import traceback\n",
    "# import laspy\n",
    "# import os\n",
    "# try:\n",
    "#     print('Running Merge LAS')\n",
    "\n",
    "#     #This is the las file to append to.  DO NOT STORE THIS FILE IN THE SAME DIRECTORY AS BELOW...\n",
    "#     out_las = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\functional\\test_merge\\merged_file.laz \"  \n",
    "#     #this is a directory of las files\n",
    "#     inDir = r\"D:\\GitHubProjects\\Terranum_repo\\TreeSegmentation\\data\\Barmasse\\Barmasse_2025\\functional\\test_merge\\source\"  \n",
    "\n",
    "#     def append_to_las(in_laz, out_las):\n",
    "#         with laspy.open(out_las, mode='a') as outlas:\n",
    "#             with laspy.open(in_las) as inlas:\n",
    "#                 for points in inlas.chunk_iterator(2_000_000):\n",
    "#                     outlas.append_points(points)\n",
    "\n",
    "#     # print(list(os.listdir(inDir)))\n",
    "#     for (dirpath, dirnames, filenames) in os.walk(inDir):\n",
    "#         # print(filenames)\n",
    "#         for inFile in filenames:\n",
    "#             if inFile.endswith('.laz'):\n",
    "#                 in_las = os.path.join(dirpath, inFile)\n",
    "#                 print(in_las)\n",
    "#                 append_to_las(in_las, out_las)\n",
    "        \n",
    "#     print('Finished without errors - merge_LAS.py')\n",
    "# except:\n",
    "#     tb = sys.exc_info()[2]\n",
    "#     tbinfo = traceback.format_tb(tb)[0]\n",
    "#     print('Error in append las')\n",
    "#     print (\"PYTHON ERRORS:\\nTraceback info:\\n\" + tbinfo + \"\\nError     Info:\\n\" + str(sys.exc_info()[1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import laspy\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# def crop_laspy(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Crops a LAS/LAZ file into tiles using laspy directly (preserves id_point perfectly).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "#     las = laspy.read(src_input)\n",
    "#     x_min, x_max = las.x.min(), las.x.max()\n",
    "#     y_min, y_max = las.y.min(), las.y.max()\n",
    "\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "\n",
    "#     for ix in range(x_steps):\n",
    "#         for iy in range(y_steps):\n",
    "#             x0 = x_min + ix * tile_size - overlap\n",
    "#             x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "#             y0 = y_min + iy * tile_size - overlap\n",
    "#             y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "#             mask = (\n",
    "#                 (las.x >= x0) & (las.x <= x1) &\n",
    "#                 (las.y >= y0) & (las.y <= y1)\n",
    "#             )\n",
    "#             if not np.any(mask):\n",
    "#                 continue\n",
    "\n",
    "#             tile = laspy.create(point_format=las.header.point_format, file_version=las.header.version)\n",
    "#             tile.points = las.points[mask]\n",
    "\n",
    "#             tile_filename = os.path.join(\n",
    "#                 src_target,\n",
    "#                 f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "#             )\n",
    "#             tile.write(tile_filename)\n",
    "#             if verbose:\n",
    "#                 print(f\"✅ Wrote {tile_filename} ({mask.sum()} points)\")\n",
    "\n",
    "\n",
    "\n",
    "# def crop_laspy(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Crops a LAS/LAZ file into tiles using laspy directly (preserves id_point and coordinates).\n",
    "#     \"\"\"\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "#     las = laspy.read(src_input)\n",
    "#     x_min, x_max = las.x.min(), las.x.max()\n",
    "#     y_min, y_max = las.y.min(), las.y.max()\n",
    "\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "\n",
    "#     for ix in range(x_steps):\n",
    "#         for iy in range(y_steps):\n",
    "#             x0 = x_min + ix * tile_size - overlap\n",
    "#             x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "#             y0 = y_min + iy * tile_size - overlap\n",
    "#             y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "#             mask = (\n",
    "#                 (las.x >= x0) & (las.x <= x1) &\n",
    "#                 (las.y >= y0) & (las.y <= y1)\n",
    "#             )\n",
    "#             if not np.any(mask):\n",
    "#                 continue\n",
    "\n",
    "#             # ✅ Create new file with proper header scale/offset\n",
    "#             header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "#             header.offsets = las.header.offsets\n",
    "#             header.scales = las.header.scales\n",
    "\n",
    "#             # ✅ Copy CRS if any\n",
    "#             if hasattr(las.header, \"epsg\") and las.header.epsg is not None:\n",
    "#                 header.epsg = las.header.epsg\n",
    "\n",
    "#             tile = laspy.LasData(header)\n",
    "#             tile.points = las.points[mask]\n",
    "\n",
    "#             tile_filename = os.path.join(\n",
    "#                 src_target,\n",
    "#                 f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "#             )\n",
    "#             tile.write(tile_filename)\n",
    "\n",
    "#             if verbose:\n",
    "#                 print(f\"✅ Wrote {tile_filename} ({mask.sum()} points)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263eab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tilling_slow(src_with_id, src_folder_tiles_w_overlap, tile_size, overlap)\n",
    "# crop_laspy(src_with_id, src_folder_tiles_w_overlap, tile_size, overlap=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # utils\n",
    "# def tilling(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Tile the input LiDAR file into square tiles using PDAL and store them in the destination folder.\n",
    "\n",
    "#     Args:\n",
    "#         - verbose (bool): Whether to print verbose status updates.\n",
    "\n",
    "#     Returns:\n",
    "#         - None: Splits the input file into tiles and saves them in the destination folder.\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"Start tilling (with overlap = {overlap}m)...\")\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "    \n",
    "#     # compute the estimate number of tiles\n",
    "#     if verbose:\n",
    "#         print(\"Computing the estimated number of tiles...\")\n",
    "#     original_file = laspy.read(src_input)\n",
    "#     x_min = original_file.x.min()\n",
    "#     x_max = original_file.x.max()\n",
    "#     y_min = original_file.y.min()\n",
    "#     y_max = original_file.y.max() \n",
    "#     if verbose:\n",
    "#         print('Done!')\n",
    "\n",
    "\n",
    "    \n",
    "#     # output_pattern = \"tile_{i}_{j}.laz\"\n",
    "#     output_pattern = os.path.join(\n",
    "#         src_target, \n",
    "#         os.path.basename(src_input).split('.')[0] + \"_tile_{i}_{j}.laz\",\n",
    "#         )\n",
    "\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "#     combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "#     for _, (i,j) in tqdm(enumerate(combinations), total=len(combinations)):\n",
    "#         x0 = x_min + i * tile_size - overlap\n",
    "#         x1 = x_min + (i + 1) * tile_size + overlap\n",
    "#         y0 = y_min + j * tile_size - overlap\n",
    "#         y1 = y_min + (j + 1) * tile_size + overlap\n",
    "\n",
    "#         bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "#         pipeline_json = {\n",
    "#             \"pipeline\": [\n",
    "#                 # src_input,\n",
    "#                 {\n",
    "#                     \"type\": \"readers.las\",\n",
    "#                     \"filename\": src_input,\n",
    "#                     \"extra_dims\": \"id_point=uint32\"\n",
    "#                 },\n",
    "#                 {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "#                 {\n",
    "#                     \"type\": \"writers.las\",\n",
    "#                     \"filename\": output_pattern.format(i=i, j=j),\n",
    "#                     \"extra_dims\": \"id_point=uint32\"\n",
    "#                 }\n",
    "#             ]\n",
    "#         }\n",
    "\n",
    "#         pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#         pipeline.execute()\n",
    "\n",
    "\n",
    "# def stripes_file(src_input_file, src_output, dims, do_keep_existant=False):\n",
    "#     [tile_size_x, tile_size_y] = dims\n",
    "#     laz = laspy.read(src_input_file)\n",
    "#     # output_folder = os.path.join(src_output, f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripes_{tile_size_x}_{tile_size_y}\")\n",
    "#     os.makedirs(src_output, exist_ok=True)\n",
    "\n",
    "#     xmin, xmax, ymin, ymax = np.min(laz.x), np.max(laz.x), np.min(laz.y), np.max(laz.y)\n",
    "\n",
    "#     x_edges = np.arange(xmin, xmax, tile_size_x)\n",
    "#     y_edges = np.arange(ymin, ymax, tile_size_y)\n",
    "\n",
    "\n",
    "#     for i, x0 in enumerate(x_edges):\n",
    "#         print(f\"Column {i+1} / {len(x_edges)}:\")\n",
    "#         for j, y0 in tqdm(enumerate(y_edges), total=len(y_edges)):\n",
    "#             output = os.path.join(src_output,f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripe_{i}_{j}.laz\")\n",
    "#             if do_keep_existant and os.path.exists(output):\n",
    "#                 continue\n",
    "#             x1 = x0 + tile_size_x\n",
    "#             y1 = y0 + tile_size_y\n",
    "#             bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "#             pipeline_json = {\n",
    "#                 \"pipeline\": [\n",
    "#                     src_input_file,\n",
    "#                     {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "#                     {\"type\": \"writers.las\", \"filename\": output, 'extra_dims': 'all'}\n",
    "#                 ]\n",
    "#             }\n",
    "\n",
    "#             pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#             pipeline.execute()\n",
    "\n",
    "\n",
    "# def stripes_file(src_input_file, src_output, dims, do_keep_existant=False):\n",
    "#     [tile_size_x, tile_size_y] = dims\n",
    "#     laz = laspy.read(src_input_file)\n",
    "#     # output_folder = os.path.join(src_output, f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripes_{tile_size_x}_{tile_size_y}\")\n",
    "#     os.makedirs(src_output, exist_ok=True)\n",
    "\n",
    "#     xmin, xmax, ymin, ymax = np.min(laz.x), np.max(laz.x), np.min(laz.y), np.max(laz.y)\n",
    "\n",
    "#     x_edges = np.arange(xmin, xmax, tile_size_x)\n",
    "#     y_edges = np.arange(ymin, ymax, tile_size_y)\n",
    "\n",
    "\n",
    "#     for i, x0 in enumerate(x_edges):\n",
    "#         print(f\"Column {i+1} / {len(x_edges)}:\")\n",
    "#         for j, y0 in tqdm(enumerate(y_edges), total=len(y_edges)):\n",
    "#             output = os.path.join(src_output,f\"{os.path.basename(src_input_file).split('.laz')[0]}_stripe_{i}_{j}.laz\")\n",
    "#             if do_keep_existant and os.path.exists(output):\n",
    "#                 continue\n",
    "#             x1 = x0 + tile_size_x\n",
    "#             y1 = y0 + tile_size_y\n",
    "#             bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "#             pipeline_json = {\n",
    "#                 \"pipeline\": [\n",
    "#                     src_input_file,\n",
    "#                     {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "#                     {\"type\": \"writers.las\", \"filename\": output, 'extra_dims': 'all'}\n",
    "#                 ]\n",
    "#             }\n",
    "\n",
    "#             pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#             pipeline.execute()\n",
    "\n",
    "\n",
    "\n",
    "# def remove_duplicates(laz_file, decimals=2):\n",
    "#     \"\"\"\n",
    "#     Removes duplicate points from a LAS/LAZ file based on rounded 3D coordinates.\n",
    "\n",
    "#     Args:\n",
    "#         - laz_file (laspy.LasData): Input LAS/LAZ file as a laspy object.\n",
    "#         - decimals (int, optional): Number of decimals to round the coordinates for duplicate detection. Defaults to 2.\n",
    "\n",
    "#     Returns:\n",
    "#         - laspy.LasData: A new laspy object with duplicate points removed.\n",
    "#     \"\"\"\n",
    "        \n",
    "#     coords = np.round(np.vstack((laz_file.x, laz_file.y, laz_file.z)).T, decimals)\n",
    "#     _, unique_indices = np.unique(coords, axis=0, return_index=True)\n",
    "#     mask = np.zeros(len(coords), dtype=bool)\n",
    "#     mask[unique_indices] = True\n",
    "\n",
    "#     # Create new LAS object\n",
    "#     header = laspy.LasHeader(point_format=laz_file.header.point_format, version=laz_file.header.version)\n",
    "#     new_las = laspy.LasData(header)\n",
    "\n",
    "#     for dim in laz_file.point_format.dimension_names:\n",
    "#         setattr(new_las, dim, getattr(laz_file, dim)[mask])\n",
    "\n",
    "#     return new_las\n",
    "\n",
    "# def match_pointclouds(laz1, laz2):\n",
    "#     \"\"\"Sort laz2 to match the order of laz1 without changing laz1's order.\n",
    "\n",
    "#     Args:\n",
    "#         laz1: laspy.LasData object (reference order)\n",
    "#         laz2: laspy.LasData object (to be sorted)\n",
    "    \n",
    "#     Returns:\n",
    "#         laz2 sorted to match laz1\n",
    "#     \"\"\"\n",
    "#     # Retrieve and round coordinates for robust matching\n",
    "#     coords_1 = np.round(np.vstack((laz1.x, laz1.y, laz1.z)), 2).T\n",
    "#     coords_2 = np.round(np.vstack((laz2.x, laz2.y, laz2.z)), 2).T\n",
    "\n",
    "#     # Verify laz2 is of the same size as laz1\n",
    "#     assert len(coords_2) == len(coords_1), \"laz2 should be a subset of laz1\"\n",
    "\n",
    "#     # Create a dictionary mapping from coordinates to indices\n",
    "#     coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(coords_1)}\n",
    "\n",
    "#     # Find indices in laz1 that correspond to laz2\n",
    "#     matching_indices = []\n",
    "#     failed = 0\n",
    "#     for coord in coords_2:\n",
    "#         try:\n",
    "#             matching_indices.append(coord_to_idx[tuple(coord)])\n",
    "#         except Exception as e:\n",
    "#             failed += 1\n",
    "\n",
    "#     matching_indices = np.array([coord_to_idx[tuple(coord)] for coord in coords_2])\n",
    "\n",
    "#     # Sort laz2 to match laz1\n",
    "#     sorted_indices = np.argsort(matching_indices)\n",
    "\n",
    "#     # Apply sorting to all attributes of laz2\n",
    "#     laz2.points = laz2.points[sorted_indices]\n",
    "\n",
    "#     return laz2  # Now sorted to match laz1\n",
    "\n",
    "\n",
    "# def flattening_tile(tile_src, tile_new_original_src, grid_size=10, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Flattens a tile by interpolating the ground surface and subtracting it from the original elevation.\n",
    "\n",
    "#     Args:\n",
    "#         - tile_src (str): Path to the input tile in LAS/LAZ format.\n",
    "#         - tile_new_original_src (str): Path to save the resized original tile after filtering.\n",
    "#         - grid_size (int, optional): Size of the grid in meters for local interpolation. Defaults to 10.\n",
    "#         - verbose (bool, optional): Whether to display progress and debug information. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         - None: Saves the floor and flattened versions of the tile and updates the original file.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Load file\n",
    "#     laz = laspy.read(tile_src)\n",
    "#     init_len = len(laz)\n",
    "#     if init_len == 0:\n",
    "#         return\n",
    "#     # laz = remove_duplicates(laz)\n",
    "#     if verbose:\n",
    "#         print(f\"Removing duplicates: From {init_len} to {len(laz)}\")\n",
    "#     # laz.write(tile_new_original_src)\n",
    "    \n",
    "#     points = np.vstack((laz.x, laz.y, laz.z)).T\n",
    "#     points_flatten = copy.deepcopy(points)\n",
    "#     points_interpolated = copy.deepcopy(points)\n",
    "\n",
    "#     # Divide into tiles and find local minimums\n",
    "#     x_min, y_min = np.min(points[:, :2], axis=0)\n",
    "#     x_max, y_max = np.max(points[:, :2], axis=0)\n",
    "\n",
    "#     x_bins = np.append(np.arange(x_min, x_max, grid_size), x_max)\n",
    "#     y_bins = np.append(np.arange(y_min, y_max, grid_size), y_max)\n",
    "\n",
    "#     grid = {i:{j:[] for j in range(y_bins.size - 1)} for i in range(x_bins.size -1)}\n",
    "#     for _, (px, py, pz) in tqdm(enumerate(points), total=len(points), desc=\"Creating grid\", disable=verbose==False):\n",
    "#         xbin = np.clip(0, (px - x_min) // grid_size, x_bins.size - 2)\n",
    "#         ybin = np.clip(0, (py - y_min) // grid_size, y_bins.size - 2)\n",
    "#         try:\n",
    "#             grid[xbin][ybin].append((px, py, pz))\n",
    "#         except Exception as e:\n",
    "#             print(xbin)\n",
    "#             print(ybin)\n",
    "#             print(x_bins)\n",
    "#             print(y_bins)\n",
    "#             print(grid.keys())\n",
    "#             print(grid[0].keys())\n",
    "#             raise e\n",
    "\n",
    "\n",
    "#     # Create grid_min\n",
    "#     grid_used = np.zeros((x_bins.size - 1, y_bins.size - 1))\n",
    "#     lst_grid_min = []\n",
    "#     lst_grid_min_pos = []\n",
    "#     for x in grid.keys():\n",
    "#         for y in grid[x].keys():\n",
    "#             if np.array(grid[x][y]).shape[0] > 0:\n",
    "#                 grid_used[x, y] = 1\n",
    "#                 lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "#                 arg_min = np.argmin(np.array(grid[x][y])[:,2])\n",
    "#                 lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2])\n",
    "\n",
    "#                 # test if border\n",
    "#                 if x == list(grid.keys())[0]:\n",
    "#                     lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "#                     lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] - [5, 0])\n",
    "#                 if x == list(grid.keys())[-1]:\n",
    "#                     lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "#                     lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] + [5, 0])\n",
    "#                 if y == list(grid[x].keys())[0]:\n",
    "#                     lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "#                     lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] - [0, 5])\n",
    "#                 if y == list(grid[x].keys())[-1]:\n",
    "#                     lst_grid_min.append(np.min(np.array(grid[x][y])[:,2]))\n",
    "#                     lst_grid_min_pos.append(np.array(grid[x][y])[arg_min,0:2] + [0, 5])\n",
    "#             else:\n",
    "#                 grid_used[x, y] = 0\n",
    "#     arr_grid_min_pos = np.vstack(lst_grid_min_pos)\n",
    "#     if verbose:\n",
    "#         print(\"Resulting grid:\")\n",
    "#         print(arr_grid_min_pos.shape)\n",
    "#         print(grid_used)\n",
    "\n",
    "#     # Interpolate\n",
    "#     points_xy = np.array(points)[:,0:2]\n",
    "#     interpolated_min_z = scipy.interpolate.griddata(arr_grid_min_pos, np.array(lst_grid_min), points_xy, method=\"cubic\", fill_value=-1)\n",
    "\n",
    "#     # # Fill NaNs with nearest neighbor interpolation\n",
    "#     # nan_mask = np.isnan(interpolated_min_z)\n",
    "#     # x = np.array(points)[:,0]\n",
    "#     # y = np.array(points)[:,1]\n",
    "#     # z = np.array(points)[:,2]\n",
    "\n",
    "#     # if np.any(nan_mask):\n",
    "#     #     interpolated_min_z[nan_mask] = scipy.interpolate.griddata(arr_grid_min_pos, np.array(lst_grid_min), (x[nan_mask], y[nan_mask]), method='nearest')\n",
    "\n",
    "#     mask_valid = np.array([x != -1 for x in list(interpolated_min_z)])\n",
    "#     points_interpolated = points_interpolated[mask_valid]\n",
    "#     points_interpolated[:, 2] = interpolated_min_z[mask_valid]\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"Interpolation:\")\n",
    "#         print(f\"Original number of points: {points.shape[0]}\")\n",
    "#         print(f\"Interpollated number of points: {points_interpolated.shape[0]} ({int(points_interpolated.shape[0] / points.shape[0]*100)}%)\")\n",
    "\n",
    "#     # save floor\n",
    "#     filtered_points = {dim: getattr(laz, dim)[mask_valid] for dim in laz.point_format.dimension_names}\n",
    "#     header = laspy.LasHeader(point_format=laz.header.point_format, version=laz.header.version)\n",
    "#     new_las = laspy.LasData(header)\n",
    "\n",
    "#     #   _Assign filtered and modified data\n",
    "#     for dim, values in filtered_points.items():\n",
    "#         setattr(new_las, dim, values)\n",
    "#     setattr(new_las, 'x', points_interpolated[:,0])\n",
    "#     setattr(new_las, 'y', points_interpolated[:,1])\n",
    "#     setattr(new_las, 'z', points_interpolated[:,2])\n",
    "\n",
    "#     #   _Save new file\n",
    "#     # floor_dir = os.path.join(os.path.dirname(tile_src), 'floor')\n",
    "#     # os.makedirs(floor_dir, exist_ok=True)\n",
    "#     # new_las.write(os.path.join(floor_dir, os.path.basename(tile_src).split('.laz')[0] + f\"_floor_{grid_size}m.laz\"))\n",
    "#     # if verbose:\n",
    "#     #     print(\"Saved file: \", os.path.join(floor_dir, os.path.basename(tile_src).split('.laz')[0] + f\"_floor_{grid_size}m.laz\"))\n",
    "\n",
    "#     # Flatten\n",
    "#     points_flatten = points_flatten[mask_valid]\n",
    "#     points_flatten[:,2] = points_flatten[:,2] - points_interpolated[:,2]\n",
    "\n",
    "#     filtered_points = {dim: getattr(laz, dim)[mask_valid] for dim in laz.point_format.dimension_names}\n",
    "#     header = laspy.LasHeader(point_format=laz.header.point_format, version=laz.header.version)\n",
    "#     header.point_count = 0\n",
    "#     new_las = laspy.LasData(header)\n",
    "\n",
    "\n",
    "#     #   _Assign filtered and modified data\n",
    "#     for dim, values in filtered_points.items():\n",
    "#         setattr(new_las, dim, values)\n",
    "\n",
    "#     setattr(new_las, 'x', points_flatten[:,0])\n",
    "#     setattr(new_las, 'y', points_flatten[:,1])\n",
    "#     setattr(new_las, 'z', points_flatten[:,2])\n",
    "\n",
    "#     #   _Save new file\n",
    "#     # flatten_dir = os.path.join(os.path.dirname(tile_src), 'flatten')\n",
    "#     # os.makedirs(flatten_dir, exist_ok=True)\n",
    "#     # new_las.write(os.path.join(os.path.dirname(tile_src), os.path.basename(tile_src).split('.laz')[0] + f\"_flatten_{grid_size}m.laz\"))\n",
    "#     new_las.write(tile_new_original_src.split('.laz')[0] + \"_flatten.laz\")\n",
    "#     if verbose:\n",
    "#         print(\"Saved file: \", tile_new_original_src.split('.laz')[0] + \"_flatten.laz\")\n",
    "\n",
    "#     # Resize original file\n",
    "#     laz.points = laz.points[mask_valid]\n",
    "#     laz.write(tile_new_original_src)\n",
    "#     if verbose:\n",
    "#         print(\"Saved file: \", tile_new_original_src)\n",
    "\n",
    "\n",
    "# def flattening(src_tiles, src_new_tiles, grid_size=10, verbose=True, verbose_full=False):\n",
    "#     \"\"\"\n",
    "#     Applies the flattening process to all tiles in a directory using grid-based ground surface estimation.\n",
    "\n",
    "#     Args:\n",
    "#         - src_tiles (str): Path to the directory containing original tiles.\n",
    "#         - src_new_tiles (str): Path to the directory where resized tiles will be saved.\n",
    "#         - grid_size (int, optional): Size of the grid in meters for interpolation. Defaults to 10.\n",
    "#         - verbose (bool, optional): Whether to show a general progress bar. Defaults to True.\n",
    "#         - verbose_full (bool, optional): Whether to print detailed info per tile. Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#         - None: Processes and saves flattened tiles into their respective folders.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(\"Starting flattening:\")\n",
    "#     list_tiles = [x for x in os.listdir(src_tiles) if x.endswith('.laz')]\n",
    "#     for _, tile in tqdm(enumerate(list_tiles), total=len(list_tiles), desc=\"Processing\", disable=verbose==False):\n",
    "#         if verbose_full:\n",
    "#             print(\"Flattening tile: \", tile)\n",
    "#         flattening_tile(\n",
    "#             tile_src=os.path.join(src_tiles, tile), \n",
    "#             tile_new_original_src=os.path.join(src_new_tiles, tile),\n",
    "#             grid_size=grid_size,\n",
    "#             verbose=verbose_full,\n",
    "#             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc76dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tilling(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Tile the input LiDAR file into square tiles using PDAL and store them in the destination folder.\n",
    "\n",
    "#     Args:\n",
    "#         - verbose (bool): Whether to print verbose status updates.\n",
    "\n",
    "#     Returns:\n",
    "#         - None: Splits the input file into tiles and saves them in the destination folder.\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"Start tilling (with overlap = {overlap}m)...\")\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "#     # Compute the bounds from the input file\n",
    "#     if verbose:\n",
    "#         print(\"Computing the bounds...\")\n",
    "#     original_file = laspy.read(src_input)\n",
    "#     x_min = original_file.x.min()\n",
    "#     x_max = original_file.x.max()\n",
    "#     y_min = original_file.y.min()\n",
    "#     y_max = original_file.y.max()\n",
    "#     if verbose:\n",
    "#         print(f\"Done! Bounding box: x=[{x_min:.2f}, {x_max:.2f}], y=[{y_min:.2f}, {y_max:.2f}]\")\n",
    "\n",
    "#     # Compute tile steps\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "#     combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"Creating {len(combinations)} tiles...\")\n",
    "\n",
    "#     # Loop over each tile (avoid list of bounds)\n",
    "#     for i, (ix, iy) in enumerate(tqdm(combinations, total=len(combinations))):\n",
    "#         x0 = x_min + ix * tile_size - overlap\n",
    "#         x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "#         y0 = y_min + iy * tile_size - overlap\n",
    "#         y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "#         bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "\n",
    "#         tile_filename = os.path.join(\n",
    "#             src_target,\n",
    "#             f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "#         )\n",
    "\n",
    "#         pipeline_json = {\n",
    "#             \"pipeline\": [\n",
    "#                 {\n",
    "#                     \"type\": \"readers.las\",\n",
    "#                     \"filename\": src_input,\n",
    "#                     \"extra_dims\": \"id_point=uint32\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"filters.crop\",\n",
    "#                     \"bounds\": bounds\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"writers.las\",\n",
    "#                     \"filename\": tile_filename,\n",
    "#                     \"extra_dims\": \"id_point=uint32\"\n",
    "#                 }\n",
    "#             ]\n",
    "#         }\n",
    "\n",
    "#         pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#         pipeline.execute()\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"✅ All tiles successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import json\n",
    "# # import laspy\n",
    "# # import os\n",
    "# # import itertools\n",
    "# # import pdal\n",
    "\n",
    "# def tilling_batch(src_input, src_target, tile_size, overlap=0, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Tile a LAS/LAZ file into multiple tiles in one PDAL pipeline execution,\n",
    "#     avoiding reloading the dataset multiple times.\n",
    "#     \"\"\"\n",
    "\n",
    "#     os.makedirs(src_target, exist_ok=True)\n",
    "\n",
    "#     original_file = laspy.read(src_input)\n",
    "#     x_min, x_max = original_file.x.min(), original_file.x.max()\n",
    "#     y_min, y_max = original_file.y.min(), original_file.y.max()\n",
    "\n",
    "#     x_steps = int((x_max - x_min) / tile_size) + 1\n",
    "#     y_steps = int((y_max - y_min) / tile_size) + 1\n",
    "\n",
    "#     combinations = list(itertools.product(range(x_steps), range(y_steps)))\n",
    "\n",
    "#     # we’ll accumulate crop/writer pairs (no repeated readers)\n",
    "#     branches = []\n",
    "#     for ix, iy in combinations:\n",
    "#         x0 = x_min + ix * tile_size - overlap\n",
    "#         x1 = x_min + (ix + 1) * tile_size + overlap\n",
    "#         y0 = y_min + iy * tile_size - overlap\n",
    "#         y1 = y_min + (iy + 1) * tile_size + overlap\n",
    "\n",
    "#         bounds = f\"([{x0},{x1}],[{y0},{y1}])\"\n",
    "#         tile_filename = os.path.join(\n",
    "#             src_target,\n",
    "#             f\"{os.path.splitext(os.path.basename(src_input))[0]}_tile_{ix}_{iy}.laz\"\n",
    "#         )\n",
    "\n",
    "#         branches.extend([  # ✅ use extend to flatten directly\n",
    "#             {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "#             {\"type\": \"writers.las\", \"filename\": tile_filename, \"extra_dims\": \"id_point=uint32\"}\n",
    "#         ])\n",
    "\n",
    "#     # ✅ Only one reader at the top, then all crops/writers\n",
    "#     pipeline_json = {\n",
    "#         \"pipeline\": [\n",
    "#             {\"type\": \"readers.las\", \"filename\": src_input, \"extra_dims\": \"id_point=uint32\"},\n",
    "#             *branches  # expands list to flat sequence\n",
    "#         ]\n",
    "#     }\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"Running PDAL with {len(combinations)} tile branches...\")\n",
    "\n",
    "#     pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "#     pipeline.execute()\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"✅ Tiling complete.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
